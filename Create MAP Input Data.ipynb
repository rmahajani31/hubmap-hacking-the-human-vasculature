{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9b9a16e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset as BaseDataset\n",
    "import albumentations as albu\n",
    "import torch\n",
    "import segmentation_models_pytorch as smp\n",
    "import shutil\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9296b32f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9d9fe5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './'\n",
    "# x_test_dir = os.path.join(DATA_DIR, 'full_training_data/all_valid_imgs')\n",
    "# y_test_dir = os.path.join(DATA_DIR, 'full_training_data/all_valid_masks')\n",
    "x_test_dir = os.path.join(DATA_DIR, 'all_eval_imgs')\n",
    "y_test_dir = os.path.join(DATA_DIR, 'all_eval_masks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e1ff70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "681\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./tile_meta.csv')\n",
    "eval_ids = set(df.loc[(df['source_wsi']==3) | (df['source_wsi']==4)]['id'].values)\n",
    "print(len(eval_ids))\n",
    "if os.path.exists('./all_eval_imgs'):\n",
    "  shutil.rmtree('./all_eval_imgs')\n",
    "if os.path.exists('./all_eval_masks'):\n",
    "  shutil.rmtree('./all_eval_masks')\n",
    "os.mkdir('./all_eval_imgs')\n",
    "os.mkdir('./all_eval_masks')\n",
    "for f in os.listdir(x_test_dir):\n",
    "  if f.split('.')[0] in eval_ids:\n",
    "    shutil.copy(f'{x_test_dir}/{f}', f'./all_eval_imgs/{f}')\n",
    "    shutil.copy(f'{y_test_dir}/{f}', f'./all_eval_masks/{f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7d6b53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for data visualization\n",
    "def visualize(**images):\n",
    "    \"\"\"PLot images in one row.\"\"\"\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(' '.join(name.split('_')).title())\n",
    "        plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30be7793",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HubMapDataset(BaseDataset):\n",
    "    \"\"\"Read images, apply augmentation and preprocessing transformations.\n",
    "    \n",
    "    Args:\n",
    "        images_dir (str): path to images folder\n",
    "        masks_dir (str): path to segmentation masks folder\n",
    "        class_values (list): values of classes to extract from segmentation mask\n",
    "        augmentation (albumentations.Compose): data transfromation pipeline \n",
    "            (e.g. flip, scale, etc.)\n",
    "        preprocessing (albumentations.Compose): data preprocessing \n",
    "            (e.g. noralization, shape manipulation, etc.)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    CLASSES = ['unlabelled', 'blood_vessel']\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            images_dir, \n",
    "            masks_dir, \n",
    "            classes=None, \n",
    "            augmentation=None, \n",
    "            preprocessing=None,\n",
    "    ):\n",
    "        self.ids = os.listdir(images_dir)\n",
    "        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]\n",
    "        self.masks_fps = [os.path.join(masks_dir, image_id) for image_id in self.ids]\n",
    "        \n",
    "        # convert str names to class values on masks\n",
    "        self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]\n",
    "        \n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        # read data\n",
    "        image = cv2.imread(self.images_fps[i])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(self.masks_fps[i], 0)\n",
    "        \n",
    "        # extract certain classes from mask (e.g. cars)\n",
    "        masks = [(mask == v) for v in self.class_values]\n",
    "        mask = np.stack(masks, axis=-1).astype('float')\n",
    "        \n",
    "        # apply augmentations\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        \n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "            \n",
    "        return self.images_fps[i], image, mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1fe0fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_augmentation():\n",
    "    train_transform = [\n",
    "\n",
    "        albu.HorizontalFlip(p=0.5),\n",
    "\n",
    "        albu.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0.1, p=1, border_mode=0),\n",
    "\n",
    "        albu.PadIfNeeded(min_height=512, min_width=352, always_apply=True, border_mode=0),\n",
    "        albu.RandomCrop(height=512, width=352, always_apply=True),\n",
    "\n",
    "        albu.GaussNoise(p=0.2),\n",
    "        albu.Perspective(p=0.5),\n",
    "\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                albu.CLAHE(p=1),\n",
    "                albu.RandomBrightnessContrast(p=1),\n",
    "                albu.RandomGamma(p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                albu.Sharpen(p=1),\n",
    "                albu.Blur(blur_limit=3, p=1),\n",
    "                albu.MotionBlur(blur_limit=3, p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                albu.RandomBrightnessContrast(p=1),\n",
    "                albu.HueSaturationValue(p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "    ]\n",
    "    return albu.Compose(train_transform)\n",
    "\n",
    "\n",
    "def get_validation_augmentation():\n",
    "    \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n",
    "    test_transform = [\n",
    "        albu.PadIfNeeded(512, 512)\n",
    "    ]\n",
    "    return albu.Compose(test_transform)\n",
    "\n",
    "\n",
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "\n",
    "def get_preprocessing(preprocessing_fn):\n",
    "    \"\"\"Construct preprocessing transform\n",
    "    \n",
    "    Args:\n",
    "        preprocessing_fn (callbale): data normalization function \n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    _transform = [\n",
    "        albu.Lambda(image=preprocessing_fn),\n",
    "        albu.Lambda(image=to_tensor, mask=to_tensor),\n",
    "    ]\n",
    "    return albu.Compose(_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3547524f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = ['blood_vessel']\n",
    "ENCODER = 'efficientnet-b7'\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "ACTIVATION = 'sigmoid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e4c5ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ce00173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "DEVICE = 'cpu'\n",
    "best_model = torch.load('./models/best_model.pth', map_location=torch.device('cpu'))\n",
    "best_model = best_model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3aaed245",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = ['blood_vessel']\n",
    "test_dataset = HubMapDataset(\n",
    "    x_test_dir, \n",
    "    y_test_dir,\n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    "    classes=CLASSES,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3aa18ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(3):\n",
    "#   _, img, mask = test_dataset[i]\n",
    "#   visualize(image=img.transpose(1,2,0), mask=mask.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fde82f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_dataset = test_dataset\n",
    "# for i in range(3):\n",
    "#     _, image, gt_mask = target_dataset[i]\n",
    "#     print(image.shape, gt_mask.shape)\n",
    "    \n",
    "#     gt_mask = gt_mask.squeeze()\n",
    "    \n",
    "#     x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
    "#     print(x_tensor.shape)\n",
    "#     pr_mask = torch.sigmoid(best_model.predict(x_tensor))\n",
    "#     print(pr_mask.shape)\n",
    "#     pr_mask = (pr_mask.squeeze().cpu().numpy().round())\n",
    "#     print(pr_mask.shape)\n",
    "    \n",
    "#     visualize(\n",
    "#         image=image.transpose(2,1,0), \n",
    "#         ground_truth_mask=gt_mask, \n",
    "#         predicted_mask=pr_mask\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e130828",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import base64\n",
    "from pycocotools import _mask as coco_mask\n",
    "import typing as t\n",
    "import zlib\n",
    "\n",
    "def extract_polygon_masks(mask):\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    masks = []\n",
    "    \n",
    "    for contour in contours:\n",
    "        epsilon = 0.01 * cv2.arcLength(contour, True)\n",
    "        approx = cv2.approxPolyDP(contour, epsilon, True)\n",
    "        \n",
    "        if approx.shape[0] >= 3:\n",
    "            polygon_mask = np.zeros_like(mask)\n",
    "            cv2.drawContours(polygon_mask, [approx], 0, 1, -1)\n",
    "            masks.append(polygon_mask.astype('bool'))\n",
    "    \n",
    "    return masks\n",
    "\n",
    "def encode_binary_mask(mask: np.ndarray) -> t.Text:\n",
    "  \"\"\"Converts a binary mask into OID challenge encoding ascii text.\"\"\"\n",
    "\n",
    "  # check input mask --\n",
    "  if mask.dtype != np.bool_:\n",
    "    raise ValueError(\n",
    "        \"encode_binary_mask expects a binary mask, received dtype == %s\" %\n",
    "        mask.dtype)\n",
    "\n",
    "  mask = np.squeeze(mask)\n",
    "  if len(mask.shape) != 2:\n",
    "    raise ValueError(\n",
    "        \"encode_binary_mask expects a 2d mask, received shape == %s\" %\n",
    "        mask.shape)\n",
    "  \n",
    "  # convert input mask to expected COCO API input --\n",
    "  mask_to_encode = mask.reshape(mask.shape[0], mask.shape[1], 1)\n",
    "  mask_to_encode = mask_to_encode.astype(np.uint8)\n",
    "  mask_to_encode = np.asfortranarray(mask_to_encode)\n",
    "\n",
    "  # RLE encode mask --\n",
    "  encoded_mask = coco_mask.encode(mask_to_encode)[0][\"counts\"]\n",
    "\n",
    "  # compress and base64 encoding --\n",
    "  binary_str = zlib.compress(encoded_mask, zlib.Z_BEST_COMPRESSION)\n",
    "  base64_str = base64.b64encode(binary_str)\n",
    "  return base64_str\n",
    "\n",
    "def calculate_iou(target_polygon_masks, output_polygon_mask):\n",
    "  max_iou_score = 0\n",
    "  for target_polygon_mask in target_polygon_masks:\n",
    "    intersection = np.logical_and(target_polygon_mask, output_polygon_mask).sum()\n",
    "    union = np.logical_or(target_polygon_mask, output_polygon_mask).sum()\n",
    "    iou_score = float(intersection)/union\n",
    "    max_iou_score = max(max_iou_score, iou_score)\n",
    "  return max_iou_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6964e050",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def generate_map_score(model, device, dataloader, iou_thresh=0.6):\n",
    "    model.eval()\n",
    "    num_batches = len(dataloader)\n",
    "    print(f'Processing a total of {num_batches} images for map score')\n",
    "    map_score_info = []\n",
    "    start_time = time.time()\n",
    "    total_map_score_num = 0\n",
    "    total_map_score_denom = 0\n",
    "    # Disable gradient calculation\n",
    "    with torch.no_grad():\n",
    "        # Iterate over the validation dataset\n",
    "        for batch_idx, (img_file, inputs, targets) in enumerate(dataloader):\n",
    "            cur_dict = dict()\n",
    "            img_id = img_file[0].split('/')[-1].split('.')[0]\n",
    "            cur_dict['id'] = img_id\n",
    "            cur_dict['height'] = 512\n",
    "            cur_dict['width'] = 512\n",
    "            iou_values = []\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            targets = targets.squeeze().numpy().astype('uint8')\n",
    "            outputs = torch.sigmoid(model(inputs)).squeeze().numpy()\n",
    "            outputs_thresh = (outputs > 0.5).astype('uint8')\n",
    "            output_polygon_masks = extract_polygon_masks(outputs_thresh)\n",
    "            target_polygon_masks = extract_polygon_masks(targets)\n",
    "            map_score = 0\n",
    "            for output_polygon_mask in output_polygon_masks:\n",
    "              output_polygon_mask_conf = round(((output_polygon_mask * outputs).sum())/(output_polygon_mask.sum()), 2)\n",
    "              iou_val = calculate_iou(target_polygon_masks, output_polygon_mask)\n",
    "              match_val = 1 if iou_val > iou_thresh else 0\n",
    "              iou_values.append((match_val, output_polygon_mask_conf, iou_val))\n",
    "              map_score += match_val * output_polygon_mask_conf\n",
    "              total_map_score_num += match_val * output_polygon_mask_conf\n",
    "              total_map_score_denom += output_polygon_mask_conf\n",
    "            cur_dict['iou_values'] = iou_values\n",
    "            cur_dict['map_score'] = map_score\n",
    "            map_score_info.append(cur_dict)\n",
    "            if batch_idx % 50 == 0:\n",
    "              print(f'On batch {batch_idx} and finished in {(time.time()-start_time)} seconds')\n",
    "              start_time = time.time()\n",
    "    return map_score_info, total_map_score_num/total_map_score_denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ce6de95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing a total of 132 images for map score\n",
      "On batch 0 and finished in 1.6234540939331055 seconds\n",
      "On batch 50 and finished in 83.79900550842285 seconds\n",
      "On batch 100 and finished in 98.66336393356323 seconds\n"
     ]
    }
   ],
   "source": [
    "map_score_info, total_map_score = generate_map_score(best_model, DEVICE, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a1af192e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4354961541390574"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_map_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573da98d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de946e04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b1f849",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3baba1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c16b23c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed3b6d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ada993a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c783315",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cfc09b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79db33b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a23d94ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bbox file - ImageID,LabelName,XMin,XMax,YMin,YMax,IsGroupOf\n",
    "# labels file - ImageID,LabelName,Confidence\n",
    "# segmentations file - ImageID,LabelName,ImageWidth,ImageHeight,XMin,YMin,XMax,YMax,IsGroupOf,Mask\n",
    "# predictions file - ImageID,ImageWidth,ImageHeight,LabelName,Score,Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1792314c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import base64\n",
    "from pycocotools import _mask as coco_mask\n",
    "import typing as t\n",
    "import zlib\n",
    "\n",
    "def extract_polygon_masks(mask):\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    masks = []\n",
    "    \n",
    "    for contour in contours:\n",
    "        epsilon = 0.01 * cv2.arcLength(contour, True)\n",
    "        approx = cv2.approxPolyDP(contour, epsilon, True)\n",
    "        \n",
    "        if approx.shape[0] >= 3:\n",
    "            polygon_mask = np.zeros_like(mask)\n",
    "            cv2.drawContours(polygon_mask, [approx], 0, 1, -1)\n",
    "            masks.append(polygon_mask.astype('bool'))\n",
    "    \n",
    "    return masks\n",
    "\n",
    "def encode_binary_mask(mask: np.ndarray) -> t.Text:\n",
    "  \"\"\"Converts a binary mask into OID challenge encoding ascii text.\"\"\"\n",
    "\n",
    "  # check input mask --\n",
    "  if mask.dtype != np.bool_:\n",
    "    raise ValueError(\n",
    "        \"encode_binary_mask expects a binary mask, received dtype == %s\" %\n",
    "        mask.dtype)\n",
    "\n",
    "  mask = np.squeeze(mask)\n",
    "  if len(mask.shape) != 2:\n",
    "    raise ValueError(\n",
    "        \"encode_binary_mask expects a 2d mask, received shape == %s\" %\n",
    "        mask.shape)\n",
    "  \n",
    "  # convert input mask to expected COCO API input --\n",
    "  mask_to_encode = mask.reshape(mask.shape[0], mask.shape[1], 1)\n",
    "  mask_to_encode = mask_to_encode.astype(np.uint8)\n",
    "  mask_to_encode = np.asfortranarray(mask_to_encode)\n",
    "\n",
    "  # RLE encode mask --\n",
    "  encoded_mask = coco_mask.encode(mask_to_encode)[0][\"counts\"]\n",
    "\n",
    "  # compress and base64 encoding --\n",
    "  binary_str = zlib.compress(encoded_mask, zlib.Z_BEST_COMPRESSION)\n",
    "  base64_str = base64.b64encode(binary_str)\n",
    "  return base64_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c57f391d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./polygons.jsonl', 'r') as json_file:\n",
    "    json_list = list(json_file)\n",
    "    \n",
    "tiles_dicts = []\n",
    "for json_str in json_list:\n",
    "    tiles_dicts.append(json.loads(json_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "791fdc0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fd569e285d3d', 'fdc723893ce2', 'fdd133458d88']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_ids = [x.split('.')[0] for x in os.listdir('./sample_valid_imgs')]\n",
    "tgt_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fd8952ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles_dicts_new = [x for x in tiles_dicts if x['id'] in tgt_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "70cb55c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tiles_dicts_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fbaaee7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_dicts = []\n",
    "segfile_dicts = []\n",
    "labels_info = set()\n",
    "img_width = 512\n",
    "img_height = 512\n",
    "for tiles_dict in tiles_dicts_new:\n",
    "  img_id = tiles_dict['id']\n",
    "  base_image = cv2.imread(f'./sample_valid_imgs/{img_id}.png')\n",
    "  for annot in tiles_dict['annotations']:\n",
    "    if annot['type'] == 'blood_vessel':\n",
    "      blood_vessel_masked_image = np.zeros((512, 512))\n",
    "      cur_dict = dict()\n",
    "      cur_segfile_dict = dict()\n",
    "      cur_dict['ImageID'] = img_id\n",
    "      cur_dict['LabelName'] = annot['type']\n",
    "      coords = annot['coordinates'][0]\n",
    "      cv2.fillPoly(blood_vessel_masked_image, pts=[np.array(coords)], color=1)\n",
    "      encoded_mask = encode_binary_mask(blood_vessel_masked_image.astype('bool')).decode('utf-8')\n",
    "      x_vals = [x[0] for x in coords]\n",
    "      y_vals = [x[1] for x in coords]\n",
    "      x_min = float(min(x_vals))/img_width\n",
    "      x_max = float(max(x_vals))/img_width\n",
    "      y_min = float(min(y_vals))/img_height\n",
    "      y_max = float(max(y_vals))/img_height\n",
    "      cur_dict['XMin'] = x_min\n",
    "      cur_dict['XMax'] = x_max\n",
    "      cur_dict['YMin'] = y_min\n",
    "      cur_dict['YMax'] = y_max\n",
    "      cur_dict['IsGroupOf'] = 0\n",
    "      cur_segfile_dict['ImageID'] = img_id\n",
    "      cur_segfile_dict['LabelName'] = annot['type']\n",
    "      cur_segfile_dict['ImageWidth'] = img_width\n",
    "      cur_segfile_dict['ImageHeight'] = img_height\n",
    "      cur_segfile_dict['XMin'] = x_min\n",
    "      cur_segfile_dict['XMax'] = x_max\n",
    "      cur_segfile_dict['YMin'] = y_min\n",
    "      cur_segfile_dict['YMax'] = y_max\n",
    "      cur_segfile_dict['IsGroupOf'] = 0\n",
    "      cur_segfile_dict['Mask'] = encoded_mask\n",
    "      bbox_dicts.append(cur_dict)\n",
    "      segfile_dicts.append(cur_segfile_dict)\n",
    "      cur_labels_info = (img_id, annot['type'], 1)\n",
    "      if cur_labels_info not in labels_info:\n",
    "        labels_info.add(cur_labels_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "20f21d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_dicts_df = pd.DataFrame.from_dict(bbox_dicts)\n",
    "bbox_dicts_df.to_csv('./map_input_data/segmentation_bbox.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "641447ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_info_df = pd.DataFrame(list(labels_info), columns=['ImageID', 'LabelName', 'Confidence'])\n",
    "labels_info_df.to_csv('./map_input_data/segmentation_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "98b42b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "segfile_df = pd.DataFrame.from_dict(segfile_dicts)\n",
    "segfile_df.to_csv('./map_input_data/segmentation_masks.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "61580323",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HubMapDataset(BaseDataset):\n",
    "    \"\"\"Read images, apply augmentation and preprocessing transformations.\n",
    "    \n",
    "    Args:\n",
    "        images_dir (str): path to images folder\n",
    "        masks_dir (str): path to segmentation masks folder\n",
    "        class_values (list): values of classes to extract from segmentation mask\n",
    "        augmentation (albumentations.Compose): data transfromation pipeline \n",
    "            (e.g. flip, scale, etc.)\n",
    "        preprocessing (albumentations.Compose): data preprocessing \n",
    "            (e.g. noralization, shape manipulation, etc.)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    CLASSES = ['unlabelled', 'blood_vessel']\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            images_dir, \n",
    "            masks_dir, \n",
    "            classes=None, \n",
    "            augmentation=None, \n",
    "            preprocessing=None,\n",
    "    ):\n",
    "        self.ids = os.listdir(images_dir)\n",
    "        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]\n",
    "        self.masks_fps = [os.path.join(masks_dir, image_id) for image_id in self.ids]\n",
    "        \n",
    "        # convert str names to class values on masks\n",
    "        self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]\n",
    "        \n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        # read data\n",
    "        image = cv2.imread(self.images_fps[i])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(self.masks_fps[i], 0)\n",
    "        \n",
    "        # extract certain classes from mask (e.g. cars)\n",
    "        masks = [(mask == v) for v in self.class_values]\n",
    "        mask = np.stack(masks, axis=-1).astype('float')\n",
    "        \n",
    "        # apply augmentations\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        \n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "            \n",
    "        return self.images_fps[i], image, mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b4d4dcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_augmentation():\n",
    "    train_transform = [\n",
    "\n",
    "        albu.HorizontalFlip(p=0.5),\n",
    "\n",
    "        albu.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0.1, p=1, border_mode=0),\n",
    "\n",
    "        albu.PadIfNeeded(min_height=512, min_width=352, always_apply=True, border_mode=0),\n",
    "        albu.RandomCrop(height=512, width=352, always_apply=True),\n",
    "\n",
    "        albu.GaussNoise(p=0.2),\n",
    "        albu.Perspective(p=0.5),\n",
    "\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                albu.CLAHE(p=1),\n",
    "                albu.RandomBrightnessContrast(p=1),\n",
    "                albu.RandomGamma(p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                albu.Sharpen(p=1),\n",
    "                albu.Blur(blur_limit=3, p=1),\n",
    "                albu.MotionBlur(blur_limit=3, p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                albu.RandomBrightnessContrast(p=1),\n",
    "                albu.HueSaturationValue(p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "    ]\n",
    "    return albu.Compose(train_transform)\n",
    "\n",
    "\n",
    "def get_validation_augmentation():\n",
    "    \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n",
    "    test_transform = [\n",
    "        albu.PadIfNeeded(512, 512)\n",
    "    ]\n",
    "    return albu.Compose(test_transform)\n",
    "\n",
    "\n",
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "\n",
    "def get_preprocessing(preprocessing_fn):\n",
    "    \"\"\"Construct preprocessing transform\n",
    "    \n",
    "    Args:\n",
    "        preprocessing_fn (callbale): data normalization function \n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    _transform = [\n",
    "        albu.Lambda(image=preprocessing_fn),\n",
    "        albu.Lambda(image=to_tensor, mask=to_tensor),\n",
    "    ]\n",
    "    return albu.Compose(_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2950c6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cpu'\n",
    "best_model = torch.load('./models/best_model_no_preproc_30_epochs.pth', map_location=torch.device('cpu'))\n",
    "best_model = best_model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "63f9f9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = ['blood_vessel']\n",
    "test_dataset = HubMapDataset(\n",
    "    './sample_valid_imgs', \n",
    "    './sample_valid_masks', \n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    "    classes=CLASSES,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6ae947eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing a total of 3 images for submission\n",
      "On batch 0 and finished in 1.4885179996490479 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "model = best_model\n",
    "dataloader = test_loader\n",
    "device = 'cpu'\n",
    "model.eval()\n",
    "num_batches = len(dataloader)\n",
    "print(f'Processing a total of {num_batches} images for submission')\n",
    "prediction_dicts = []\n",
    "start_time = time.time()\n",
    "# Disable gradient calculation\n",
    "with torch.no_grad():\n",
    "    # Iterate over the validation dataset\n",
    "    for batch_idx, (img_file, inputs, targets) in enumerate(dataloader):\n",
    "        cur_dict = dict()\n",
    "        img_id = img_file[0].split('/')[-1].split('.')[0]\n",
    "        cur_dict['ImageID'] = img_id\n",
    "        cur_dict['ImageWidth'] = 512\n",
    "        cur_dict['ImageHeight'] = 512\n",
    "        cur_dict['LabelName'] = 'blood_vessel'\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = torch.sigmoid(model(inputs)).squeeze().numpy()\n",
    "        outputs_thresh = (outputs > 0.5).astype('uint8')\n",
    "        polygon_masks = extract_polygon_masks(outputs_thresh)\n",
    "        for polygon_mask in polygon_masks:\n",
    "          cur_dict = dict()\n",
    "          cur_dict['ImageID'] = img_id\n",
    "          cur_dict['ImageWidth'] = 512\n",
    "          cur_dict['ImageHeight'] = 512\n",
    "          cur_dict['LabelName'] = 'blood_vessel'\n",
    "          polygon_mask_conf = round(((polygon_mask * outputs).sum())/(polygon_mask.sum()), 2)\n",
    "          polygon_mask_string = encode_binary_mask(polygon_mask).decode('utf-8')\n",
    "          cur_dict['Score'] = polygon_mask_conf\n",
    "          cur_dict['Mask'] = polygon_mask_string\n",
    "          prediction_dicts.append(cur_dict)\n",
    "        if batch_idx % 50 == 0:\n",
    "          print(f'On batch {batch_idx} and finished in {(time.time()-start_time)} seconds')\n",
    "          start_time = time.time()\n",
    "    predictions_df = pd.DataFrame.from_dict(prediction_dicts)\n",
    "    predictions_df.to_csv('./map_input_data/seg_preds.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8532a199",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HubMapEnv",
   "language": "python",
   "name": "hubmapenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
