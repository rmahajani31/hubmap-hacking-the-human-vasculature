{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bdba8260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset as BaseDataset\n",
    "from networks.vit_seg_modeling import VisionTransformer as ViT_seg\n",
    "from networks.vit_seg_modeling import CONFIGS as CONFIGS_ViT_seg\n",
    "import albumentations as albu\n",
    "import torch\n",
    "import base64\n",
    "from pycocotools import _mask as coco_mask\n",
    "import typing as t\n",
    "import zlib\n",
    "import json\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from datasets.dataset_synapse import Synapse_dataset\n",
    "from utils import test_single_volume\n",
    "from networks.vit_seg_modeling import VisionTransformer as ViT_seg\n",
    "from networks.vit_seg_modeling import CONFIGS as CONFIGS_ViT_seg\n",
    "from scipy.ndimage import zoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5947d34c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f2a1aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded snapshot path is: ../model/TU_HubMap512/TU_pretrain_R50-ViT-B_16_skip3_epo50_bs4_lr0.002_512/epoch_39.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 1234\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "dataset_config = {\n",
    "        'HubMap': {\n",
    "            'Dataset': Synapse_dataset,\n",
    "            'volume_path': '../data/HubMap/test_h5',\n",
    "            'list_dir': './lists/lists_HubMap',\n",
    "            'num_classes': 2,\n",
    "            'z_spacing': 1,\n",
    "        },\n",
    "    }\n",
    "dataset_name = 'HubMap'\n",
    "num_classes = dataset_config[dataset_name]['num_classes']\n",
    "volume_path = dataset_config[dataset_name]['volume_path']\n",
    "Dataset = dataset_config[dataset_name]['Dataset']\n",
    "list_dir = dataset_config[dataset_name]['list_dir']\n",
    "z_spacing = dataset_config[dataset_name]['z_spacing']\n",
    "\n",
    "img_size = 512\n",
    "is_pretrain = True\n",
    "vit_name = 'R50-ViT-B_16'\n",
    "n_skip = 3\n",
    "vit_patches_size = 16\n",
    "max_epochs = 50\n",
    "batch_size = 4\n",
    "base_lr = 0.002\n",
    "tgt_model_epoch = 39\n",
    "exp = 'TU_' + dataset_name + str(img_size)\n",
    "snapshot_path = \"../model/{}/{}\".format(exp, 'TU')\n",
    "snapshot_path = snapshot_path + '_pretrain' if is_pretrain else snapshot_path\n",
    "snapshot_path += '_' + vit_name\n",
    "snapshot_path = snapshot_path + '_skip' + str(n_skip)\n",
    "snapshot_path = snapshot_path + '_vitpatch' + str(vit_patches_size) if vit_patches_size!=16 else snapshot_path\n",
    "snapshot_path = snapshot_path + '_epo' + str(max_epochs) if max_epochs != 30 else snapshot_path\n",
    "if dataset_name == 'ACDC':  # using max_epoch instead of iteration to control training duration\n",
    "    snapshot_path = snapshot_path + '_' + str(max_iterations)[0:2] + 'k' if max_iterations != 30000 else snapshot_path\n",
    "snapshot_path = snapshot_path+'_bs'+str(batch_size)\n",
    "snapshot_path = snapshot_path + '_lr' + str(base_lr) if base_lr != 0.01 else snapshot_path\n",
    "snapshot_path = snapshot_path + '_'+str(img_size)\n",
    "snapshot_path = snapshot_path + '_s'+str(seed) if seed!=1234 else snapshot_path\n",
    "\n",
    "config_vit = CONFIGS_ViT_seg[vit_name]\n",
    "config_vit.n_classes = num_classes\n",
    "config_vit.n_skip = n_skip\n",
    "config_vit.patches.size = (vit_patches_size, vit_patches_size)\n",
    "if vit_name.find('R50') !=-1:\n",
    "    config_vit.patches.grid = (int(img_size/vit_patches_size), int(img_size/vit_patches_size))\n",
    "net = ViT_seg(config_vit, img_size=img_size, num_classes=config_vit.n_classes)\n",
    "\n",
    "snapshot = os.path.join(snapshot_path, 'best_model.pth')\n",
    "if not os.path.exists(snapshot): snapshot = snapshot.replace('best_model', 'epoch_'+str(tgt_model_epoch))\n",
    "#if not os.path.exists(snapshot): snapshot = snapshot.replace('best_model', 'epoch_'+str(14))\n",
    "print(f'Loaded snapshot path is: {snapshot}')\n",
    "\n",
    "net.load_state_dict(torch.load(snapshot, map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05f5d14e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (transformer): Transformer(\n",
       "    (embeddings): Embeddings(\n",
       "      (hybrid_model): ResNetV2(\n",
       "        (root): Sequential(\n",
       "          (conv): StdConv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "          (gn): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (body): Sequential(\n",
       "          (block1): Sequential(\n",
       "            (unit1): PreActBottleneck(\n",
       "              (gn1): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "              (conv1): StdConv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (gn2): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "              (conv2): StdConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (gn3): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv3): StdConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (downsample): StdConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (gn_proj): GroupNorm(256, 256, eps=1e-05, affine=True)\n",
       "            )\n",
       "            (unit2): PreActBottleneck(\n",
       "              (gn1): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "              (conv1): StdConv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (gn2): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "              (conv2): StdConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (gn3): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv3): StdConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (relu): ReLU(inplace=True)\n",
       "            )\n",
       "            (unit3): PreActBottleneck(\n",
       "              (gn1): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "              (conv1): StdConv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (gn2): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "              (conv2): StdConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (gn3): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv3): StdConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (relu): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (block2): Sequential(\n",
       "            (unit1): PreActBottleneck(\n",
       "              (gn1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (conv1): StdConv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (gn2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (conv2): StdConv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (gn3): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (conv3): StdConv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (downsample): StdConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (gn_proj): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "            )\n",
       "            (unit2): PreActBottleneck(\n",
       "              (gn1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (conv1): StdConv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (gn2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (conv2): StdConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (gn3): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (conv3): StdConv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (relu): ReLU(inplace=True)\n",
       "            )\n",
       "            (unit3): PreActBottleneck(\n",
       "              (gn1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (conv1): StdConv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (gn2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (conv2): StdConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (gn3): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (conv3): StdConv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (relu): ReLU(inplace=True)\n",
       "            )\n",
       "            (unit4): PreActBottleneck(\n",
       "              (gn1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (conv1): StdConv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (gn2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (conv2): StdConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (gn3): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (conv3): StdConv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (relu): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (block3): Sequential(\n",
       "            (unit1): PreActBottleneck(\n",
       "              (gn1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv1): StdConv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (gn2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (gn3): GroupNorm(32, 1024, eps=1e-06, affine=True)\n",
       "              (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (downsample): StdConv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (gn_proj): GroupNorm(1024, 1024, eps=1e-05, affine=True)\n",
       "            )\n",
       "            (unit2): PreActBottleneck(\n",
       "              (gn1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv1): StdConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (gn2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (gn3): GroupNorm(32, 1024, eps=1e-06, affine=True)\n",
       "              (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (relu): ReLU(inplace=True)\n",
       "            )\n",
       "            (unit3): PreActBottleneck(\n",
       "              (gn1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv1): StdConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (gn2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (gn3): GroupNorm(32, 1024, eps=1e-06, affine=True)\n",
       "              (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (relu): ReLU(inplace=True)\n",
       "            )\n",
       "            (unit4): PreActBottleneck(\n",
       "              (gn1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv1): StdConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (gn2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (gn3): GroupNorm(32, 1024, eps=1e-06, affine=True)\n",
       "              (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (relu): ReLU(inplace=True)\n",
       "            )\n",
       "            (unit5): PreActBottleneck(\n",
       "              (gn1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv1): StdConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (gn2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (gn3): GroupNorm(32, 1024, eps=1e-06, affine=True)\n",
       "              (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (relu): ReLU(inplace=True)\n",
       "            )\n",
       "            (unit6): PreActBottleneck(\n",
       "              (gn1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv1): StdConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (gn2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (gn3): GroupNorm(32, 1024, eps=1e-06, affine=True)\n",
       "              (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (relu): ReLU(inplace=True)\n",
       "            )\n",
       "            (unit7): PreActBottleneck(\n",
       "              (gn1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv1): StdConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (gn2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (gn3): GroupNorm(32, 1024, eps=1e-06, affine=True)\n",
       "              (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (relu): ReLU(inplace=True)\n",
       "            )\n",
       "            (unit8): PreActBottleneck(\n",
       "              (gn1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv1): StdConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (gn2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (gn3): GroupNorm(32, 1024, eps=1e-06, affine=True)\n",
       "              (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (relu): ReLU(inplace=True)\n",
       "            )\n",
       "            (unit9): PreActBottleneck(\n",
       "              (gn1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv1): StdConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (gn2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (gn3): GroupNorm(32, 1024, eps=1e-06, affine=True)\n",
       "              (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (relu): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (patch_embeddings): Conv2d(1024, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): Block(\n",
       "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (1): Block(\n",
       "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (2): Block(\n",
       "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (3): Block(\n",
       "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (4): Block(\n",
       "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (5): Block(\n",
       "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (6): Block(\n",
       "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (7): Block(\n",
       "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (8): Block(\n",
       "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (9): Block(\n",
       "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (10): Block(\n",
       "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (11): Block(\n",
       "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): DecoderCup(\n",
       "    (conv_more): Conv2dReLU(\n",
       "      (0): Conv2d(768, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (up): UpsamplingBilinear2d(scale_factor=2.0, mode=bilinear)\n",
       "      )\n",
       "      (1): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (up): UpsamplingBilinear2d(scale_factor=2.0, mode=bilinear)\n",
       "      )\n",
       "      (2): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (up): UpsamplingBilinear2d(scale_factor=2.0, mode=bilinear)\n",
       "      )\n",
       "      (3): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (up): UpsamplingBilinear2d(scale_factor=2.0, mode=bilinear)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (segmentation_head): SegmentationHead(\n",
       "    (0): Conv2d(16, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29ad2d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_test_preproc_trans_unet_img_size_512_tgt_epoch_39\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = './'\n",
    "x_test_dir = os.path.join(DATA_DIR, '../../test')\n",
    "y_test_dir = os.path.join(DATA_DIR, '../../test')\n",
    "best_model = net\n",
    "best_model = best_model.to(DEVICE)\n",
    "suffix = f'all_test_preproc_trans_unet_img_size_{img_size}_tgt_epoch_{tgt_model_epoch}'\n",
    "print(suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d098c6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for data visualization\n",
    "def visualize(**images):\n",
    "    \"\"\"PLot images in one row.\"\"\"\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(' '.join(name.split('_')).title())\n",
    "        plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aa94c0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HubMapDataset(BaseDataset):\n",
    "    \"\"\"Read images, apply augmentation and preprocessing transformations.\n",
    "    \n",
    "    Args:\n",
    "        images_dir (str): path to images folder\n",
    "        masks_dir (str): path to segmentation masks folder\n",
    "        class_values (list): values of classes to extract from segmentation mask\n",
    "        augmentation (albumentations.Compose): data transfromation pipeline \n",
    "            (e.g. flip, scale, etc.)\n",
    "        preprocessing (albumentations.Compose): data preprocessing \n",
    "            (e.g. noralization, shape manipulation, etc.)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    CLASSES = ['unlabelled', 'blood_vessel']\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            images_dir, \n",
    "            masks_dir, \n",
    "            classes=None, \n",
    "            augmentation=None, \n",
    "            preprocessing=None,\n",
    "    ):\n",
    "        self.ids = os.listdir(images_dir)\n",
    "        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]\n",
    "        self.masks_fps = [os.path.join(masks_dir, image_id) for image_id in self.ids]\n",
    "        \n",
    "        # convert str names to class values on masks\n",
    "        self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]\n",
    "        \n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        # read data\n",
    "        image = cv2.imread(self.images_fps[i])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(self.masks_fps[i], 0)\n",
    "        \n",
    "        # extract certain classes from mask (e.g. cars)\n",
    "        masks = [(mask == v) for v in self.class_values]\n",
    "        mask = np.stack(masks, axis=-1).astype('float')\n",
    "        \n",
    "        # apply augmentations\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        \n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "            \n",
    "        return self.images_fps[i], image, mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "591ec961",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_augmentation():\n",
    "    train_transform = [\n",
    "\n",
    "        albu.HorizontalFlip(p=0.5),\n",
    "\n",
    "        albu.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0.1, p=1, border_mode=0),\n",
    "\n",
    "        albu.PadIfNeeded(min_height=512, min_width=352, always_apply=True, border_mode=0),\n",
    "        albu.RandomCrop(height=512, width=352, always_apply=True),\n",
    "\n",
    "        albu.GaussNoise(p=0.2),\n",
    "        albu.Perspective(p=0.5),\n",
    "\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                albu.CLAHE(p=1),\n",
    "                albu.RandomBrightnessContrast(p=1),\n",
    "                albu.RandomGamma(p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                albu.Sharpen(p=1),\n",
    "                albu.Blur(blur_limit=3, p=1),\n",
    "                albu.MotionBlur(blur_limit=3, p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                albu.RandomBrightnessContrast(p=1),\n",
    "                albu.HueSaturationValue(p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "    ]\n",
    "    return albu.Compose(train_transform)\n",
    "\n",
    "\n",
    "def get_validation_augmentation():\n",
    "    \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n",
    "    test_transform = [\n",
    "        albu.PadIfNeeded(512, 512)\n",
    "    ]\n",
    "    return albu.Compose(test_transform)\n",
    "\n",
    "def to_tensor(image, **kwargs):\n",
    "  return torch.tensor(image, dtype=torch.float32)\n",
    "\n",
    "def convert_to_grayscale(image):\n",
    "  grayscale = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "  max_grayscale_num = grayscale.max()\n",
    "  min_grayscale_num = grayscale.min()\n",
    "  grayscale = (grayscale-min_grayscale_num) / float(max_grayscale_num-min_grayscale_num)\n",
    "  return grayscale\n",
    "\n",
    "def preprocess_validation_imgs(image, img_size, **kwargs):\n",
    "  grayscale = convert_to_grayscale(image)\n",
    "  height, width = grayscale.shape\n",
    "  if img_size < height:\n",
    "    grayscale = zoom(grayscale, (img_size / height, img_size / width), order=3)\n",
    "  return grayscale\n",
    "  \n",
    "from functools import partial\n",
    "def get_preprocessing(preprocessing_fn, img_size):\n",
    "    \"\"\"Construct preprocessing transform\n",
    "    \n",
    "    Args:\n",
    "        preprocessing_fn (callbale): data normalization function \n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "    \n",
    "    \"\"\"\n",
    "    partial_func = partial(preprocessing_fn, img_size=img_size)\n",
    "    _transform = [\n",
    "        albu.Lambda(image=partial_func),\n",
    "        albu.Lambda(image=to_tensor, mask=to_tensor),\n",
    "    ]\n",
    "    return albu.Compose(_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "69218f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_without_preproc = HubMapDataset(\n",
    "    x_test_dir, \n",
    "    y_test_dir, \n",
    "    classes=['unlabelled', 'blood_vessel'],\n",
    ")\n",
    "\n",
    "test_dataset = HubMapDataset(\n",
    "    x_test_dir, \n",
    "    y_test_dir, \n",
    "    preprocessing=get_preprocessing(preprocess_validation_imgs, img_size),\n",
    "    classes=['unlabelled', 'blood_vessel'],\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aad3713b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import base64\n",
    "from pycocotools import _mask as coco_mask\n",
    "import typing as t\n",
    "import zlib\n",
    "\n",
    "def extract_polygon_masks(mask):\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    masks = []\n",
    "    \n",
    "    for contour in contours:\n",
    "        epsilon = 0.01 * cv2.arcLength(contour, True)\n",
    "        approx = cv2.approxPolyDP(contour, epsilon, True)\n",
    "        \n",
    "        if approx.shape[0] >= 3:\n",
    "            polygon_mask = np.zeros_like(mask)\n",
    "            cv2.drawContours(polygon_mask, [approx], 0, 1, -1)\n",
    "            masks.append(polygon_mask.astype('bool'))\n",
    "    \n",
    "    return masks\n",
    "\n",
    "def encode_binary_mask(mask: np.ndarray) -> t.Text:\n",
    "  \"\"\"Converts a binary mask into OID challenge encoding ascii text.\"\"\"\n",
    "\n",
    "  # check input mask --\n",
    "  if mask.dtype != np.bool_:\n",
    "    raise ValueError(\n",
    "        \"encode_binary_mask expects a binary mask, received dtype == %s\" %\n",
    "        mask.dtype)\n",
    "\n",
    "  mask = np.squeeze(mask)\n",
    "  if len(mask.shape) != 2:\n",
    "    raise ValueError(\n",
    "        \"encode_binary_mask expects a 2d mask, received shape == %s\" %\n",
    "        mask.shape)\n",
    "  \n",
    "  # convert input mask to expected COCO API input --\n",
    "  mask_to_encode = mask.reshape(mask.shape[0], mask.shape[1], 1)\n",
    "  mask_to_encode = mask_to_encode.astype(np.uint8)\n",
    "  mask_to_encode = np.asfortranarray(mask_to_encode)\n",
    "\n",
    "  # RLE encode mask --\n",
    "  encoded_mask = coco_mask.encode(mask_to_encode)[0][\"counts\"]\n",
    "\n",
    "  # compress and base64 encoding --\n",
    "  binary_str = zlib.compress(encoded_mask, zlib.Z_BEST_COMPRESSION)\n",
    "  base64_str = base64.b64encode(binary_str)\n",
    "  return base64_str\n",
    "\n",
    "def decode_binary_mask(encoded_mask):\n",
    "    # Decode base64 and decompress the binary string\n",
    "    binary_str = base64.b64decode(encoded_mask)\n",
    "    decompressed_str = zlib.decompress(binary_str)\n",
    "\n",
    "    # Decode RLE-encoded mask\n",
    "    encoded_mask = np.frombuffer(decompressed_str, dtype=np.uint8)\n",
    "    decoded_mask = coco_mask.decode({\"counts\": encoded_mask})\n",
    "\n",
    "    # Convert COCO API format to binary mask\n",
    "    mask = np.squeeze(decoded_mask)\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3095abe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_mask(image, mask):\n",
    "    # Make a copy of the original image to avoid modifying it\n",
    "    overlay = np.copy(image)\n",
    "    \n",
    "    # Convert the binary mask to a boolean mask\n",
    "    mask = mask.astype(bool)\n",
    "    \n",
    "    # Set the red channel of the overlay where the mask is True to 255\n",
    "    overlay[mask, 0] = 255\n",
    "    \n",
    "    # Set the green and blue channels of the overlay where the mask is True to 0\n",
    "    overlay[mask, 1:] = 0\n",
    "    \n",
    "    return overlay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e311cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "\n",
    "best_model.eval()\n",
    "for i in range(20):\n",
    "  image_vis = test_dataset_without_preproc[i][1].astype('uint8')\n",
    "  _, image, gt_mask = test_dataset[i]\n",
    "\n",
    "  gt_mask = gt_mask.squeeze()\n",
    "  \n",
    "  pr_mask = best_model(image.unsqueeze(0).unsqueeze(0))\n",
    "  pr_mask = torch.softmax(pr_mask, dim=1)[:,1,:,:].squeeze().detach().numpy()\n",
    "  pr_mask_height, pr_mask_width = pr_mask.shape\n",
    "  if pr_mask_height != image_vis.shape[0]:\n",
    "    pr_mask_zoomed = zoom(pr_mask, (float(image_vis.shape[0]/pr_mask_height), float(image_vis.shape[1]/pr_mask_width)), order=3)\n",
    "  else:\n",
    "    pr_mask_zoomed = pr_mask\n",
    "  pr_mask_zoomed = (pr_mask_zoomed>0.5).astype('uint8')\n",
    "  print(np.unique(pr_mask_zoomed), pr_mask_zoomed.shape)\n",
    "  print(image_vis.shape, gt_mask.shape)\n",
    "  masked_image_gt = overlay_mask(image_vis, gt_mask[:,:,1].numpy())\n",
    "  print(pr_mask_zoomed.shape)\n",
    "  masked_image_pr = overlay_mask(image_vis, pr_mask_zoomed)\n",
    "\n",
    "  visualize(\n",
    "      image_orig = image_vis,\n",
    "      image=image, \n",
    "      ground_truth_mask=masked_image_gt, \n",
    "      predicted_mask=pr_mask_zoomed\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "14bff0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def generate_submission(model, device, dataloader, suffix):\n",
    "    model.eval()\n",
    "    num_batches = len(dataloader)\n",
    "    print(f'Processing a total of {num_batches} images for submission')\n",
    "    submission_dicts = []\n",
    "    start_time = time.time()\n",
    "    # Disable gradient calculation\n",
    "    with torch.no_grad():\n",
    "        # Iterate over the validation dataset\n",
    "        for batch_idx, (img_file, inputs, targets) in enumerate(dataloader):\n",
    "            model_inference_start_time = time.time()\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs.unsqueeze(1))\n",
    "            print(f'Takes {float(time.time()-model_inference_start_time)} seconds to inference for 8 samples')\n",
    "            post_proc_start_time = time.time()\n",
    "            outputs = torch.softmax(outputs, dim=1)[:,1,:,:].detach().numpy()\n",
    "            output_batch_size, outputs_height, outputs_width = outputs.shape\n",
    "            if outputs_height != 512:\n",
    "              outputs_zoomed = zoom(outputs, (1, float(512/outputs_height), float(512/outputs_width)), order=3)\n",
    "            else:\n",
    "              outputs_zoomed = outputs\n",
    "            outputs_zoomed_thresh = (outputs_zoomed>0.5).astype('uint8')\n",
    "            print(outputs.shape, outputs_zoomed.shape, outputs_zoomed_thresh.shape)\n",
    "            for i in range(len(outputs_zoomed_thresh)):\n",
    "              cur_dict = dict()\n",
    "              img_id = img_file[i].split('/')[-1].split('.')[0]\n",
    "              cur_dict['id'] = img_id\n",
    "              cur_dict['height'] = 512\n",
    "              cur_dict['width'] = 512\n",
    "              prediction_string = ''\n",
    "              polygon_masks = extract_polygon_masks(outputs_zoomed_thresh[i,:,:])\n",
    "              for polygon_mask in polygon_masks:\n",
    "                polygon_mask_conf = round(((polygon_mask * outputs_zoomed[i,:,:]).sum())/(polygon_mask.sum()), 2)\n",
    "                polygon_mask_string = encode_binary_mask(polygon_mask).decode('utf-8')\n",
    "                prediction_string += f'0 {polygon_mask_conf} {polygon_mask_string} '\n",
    "              cur_dict['prediction_string'] = prediction_string.strip()\n",
    "              submission_dicts.append(cur_dict)\n",
    "            print(f'Takes {float(time.time()-post_proc_start_time)} seconds for post processing 8 samples')\n",
    "            if (batch_idx+1) % 10 == 0:\n",
    "              print(f'On batch {batch_idx} and finished in {float(time.time()-start_time)/60} minutes')\n",
    "        submission_df = pd.DataFrame.from_dict(submission_dicts)\n",
    "        submission_df.to_csv(f'./submissions/submission_{suffix}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "af55f48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./submissions'):\n",
    "  os.mkdir('./submissions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b3d440d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing a total of 1 images for submission\n",
      "Takes 2.8165123462677 seconds to inference for 8 samples\n",
      "(1, 512, 512) (1, 512, 512) (1, 512, 512)\n",
      "Takes 0.015571832656860352 seconds for post processing 8 samples\n"
     ]
    }
   ],
   "source": [
    "generate_submission(best_model, DEVICE, test_loader, suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0542b161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ground_truth_map_files(tiles_dicts_new, x_test_dir, suffix):\n",
    "  bbox_dicts = []\n",
    "  segfile_dicts = []\n",
    "  labels_info = set()\n",
    "  img_width = 512\n",
    "  img_height = 512\n",
    "  print(f'Processing a total of {len(tiles_dicts_new)} tiles')\n",
    "  start_time = time.time()\n",
    "  for idx, tiles_dict in enumerate(tiles_dicts_new):\n",
    "    img_id = tiles_dict['id']\n",
    "    base_image = cv2.imread(f'{x_test_dir}/{img_id}.png')\n",
    "    for annot in tiles_dict['annotations']:\n",
    "      if annot['type'] == 'blood_vessel':\n",
    "        blood_vessel_masked_image = np.zeros((512, 512))\n",
    "        cur_dict = dict()\n",
    "        cur_segfile_dict = dict()\n",
    "        cur_dict['ImageID'] = img_id\n",
    "        cur_dict['LabelName'] = annot['type']\n",
    "        coords = annot['coordinates'][0]\n",
    "        cv2.fillPoly(blood_vessel_masked_image, pts=[np.array(coords)], color=1)\n",
    "        encoded_mask = encode_binary_mask(blood_vessel_masked_image.astype('bool')).decode('utf-8')\n",
    "        x_vals = [x[0] for x in coords]\n",
    "        y_vals = [x[1] for x in coords]\n",
    "        x_min = float(min(x_vals))/img_width\n",
    "        x_max = float(max(x_vals))/img_width\n",
    "        y_min = float(min(y_vals))/img_height\n",
    "        y_max = float(max(y_vals))/img_height\n",
    "        cur_dict['XMin'] = x_min\n",
    "        cur_dict['XMax'] = x_max\n",
    "        cur_dict['YMin'] = y_min\n",
    "        cur_dict['YMax'] = y_max\n",
    "        cur_dict['IsGroupOf'] = 0\n",
    "        cur_segfile_dict['ImageID'] = img_id\n",
    "        cur_segfile_dict['LabelName'] = annot['type']\n",
    "        cur_segfile_dict['ImageWidth'] = img_width\n",
    "        cur_segfile_dict['ImageHeight'] = img_height\n",
    "        cur_segfile_dict['XMin'] = x_min\n",
    "        cur_segfile_dict['XMax'] = x_max\n",
    "        cur_segfile_dict['YMin'] = y_min\n",
    "        cur_segfile_dict['YMax'] = y_max\n",
    "        cur_segfile_dict['IsGroupOf'] = 0\n",
    "        cur_segfile_dict['Mask'] = encoded_mask\n",
    "        bbox_dicts.append(cur_dict)\n",
    "        segfile_dicts.append(cur_segfile_dict)\n",
    "        cur_labels_info = (img_id, annot['type'], 1)\n",
    "        if cur_labels_info not in labels_info:\n",
    "          labels_info.add(cur_labels_info)\n",
    "    if idx % 50 == 0:\n",
    "      print(f'Finished {idx} tiles in {float(time.time()-start_time)/60} minutes')\n",
    "  bbox_dicts_df = pd.DataFrame.from_dict(bbox_dicts)\n",
    "  bbox_dicts_df.to_csv(f'./map_input_data/segmentation_bbox_{suffix}.csv', index=False)\n",
    "  labels_info_df = pd.DataFrame(list(labels_info), columns=['ImageID', 'LabelName', 'Confidence'])\n",
    "  labels_info_df.to_csv(f'./map_input_data/segmentation_labels_{suffix}.csv', index=False)\n",
    "  segfile_df = pd.DataFrame.from_dict(segfile_dicts)\n",
    "  segfile_df.to_csv(f'./map_input_data/segmentation_masks_{suffix}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a01cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./map_input_data'):\n",
    "  os.mkdir('./map_input_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed3172b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./polygons.jsonl', 'r') as json_file:\n",
    "    json_list = list(json_file)\n",
    "    \n",
    "tiles_dicts = []\n",
    "for json_str in json_list:\n",
    "    tiles_dicts.append(json.loads(json_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a64f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efeaf532",
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_ids = [x.split('.')[0] for x in os.listdir(x_test_dir)]\n",
    "tiles_dicts_new = [x for x in tiles_dicts if x['id'] in tgt_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c72d808",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_ground_truth_map_files(tiles_dicts_new, x_test_dir, suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08285800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pred_parts(pred_string):\n",
    "  pred_parts = pred_string.split()\n",
    "  pred_parts_arr = []\n",
    "  for i in range(0, len(pred_parts), 3):\n",
    "    pred_parts_arr.append(pred_parts[i:i+3])\n",
    "  return pred_parts_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565285af",
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34e8efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prediction_map_file(submission_df, suffix):\n",
    "  non_empty_pred_mask = submission_df.apply(lambda x: x['prediction_string']!='', axis=1)\n",
    "  submission_df = submission_df[non_empty_pred_mask].dropna(subset=['prediction_string'], axis=0)\n",
    "  submission_df = submission_df.rename(columns={'id': 'ImageID', 'height': 'ImageHeight', 'width': 'ImageWidth'})\n",
    "  submission_df['prediction_string'] = submission_df['prediction_string'].apply(create_pred_parts)\n",
    "  submission_df = submission_df.explode('prediction_string')\n",
    "  submission_df[['LabelName', 'Score', 'Mask']] = submission_df['prediction_string'].apply(pd.Series)\n",
    "  submission_df['LabelName'] = submission_df['LabelName'].astype(str).replace('0', 'blood_vessel')\n",
    "  submission_df = submission_df.drop('prediction_string', axis=1)\n",
    "  submission_df.to_csv(f'./map_input_data/seg_preds_{suffix}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ebc76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.read_csv(f'./submissions/submission_{suffix}.csv')\n",
    "generate_prediction_map_file(submission_df, suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be78a219",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_preds_df = pd.read_csv(f'./map_input_data/seg_preds_{suffix}.csv')\n",
    "seg_preds_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5ab776",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_preds_df = seg_preds_df.loc[seg_preds_df['Score']>=0.7]\n",
    "seg_preds_df.to_csv(f'./map_input_data/seg_preds_{suffix}_test.csv', index=False)\n",
    "seg_preds_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d0b83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "print(seg_preds_df.shape, seg_preds_df.loc[seg_preds_df['Score']>0.7].shape, seg_preds_df.loc[seg_preds_df['Score']>0.75].shape)\n",
    "sns.boxplot(x=seg_preds_df[\"Score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "44b210c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycocotools\n",
    "def encode_binary_mask(mask: np.ndarray) -> t.Text:\n",
    "  \"\"\"Converts a binary mask into OID challenge encoding ascii text.\"\"\"\n",
    "\n",
    "  # check input mask --\n",
    "  if mask.dtype != np.bool_:\n",
    "    raise ValueError(\n",
    "        \"encode_binary_mask expects a binary mask, received dtype == %s\" %\n",
    "        mask.dtype)\n",
    "\n",
    "  mask = np.squeeze(mask)\n",
    "  if len(mask.shape) != 2:\n",
    "    raise ValueError(\n",
    "        \"encode_binary_mask expects a 2d mask, received shape == %s\" %\n",
    "        mask.shape)\n",
    "  \n",
    "  # convert input mask to expected COCO API input --\n",
    "  mask_to_encode = mask.reshape(mask.shape[0], mask.shape[1], 1)\n",
    "  mask_to_encode = mask_to_encode.astype(np.uint8)\n",
    "  mask_to_encode = np.asfortranarray(mask_to_encode)\n",
    "\n",
    "  # RLE encode mask --\n",
    "  encoded_mask = coco_mask.encode(mask_to_encode)[0][\"counts\"]\n",
    "\n",
    "  # compress and base64 encoding --\n",
    "  binary_str = zlib.compress(encoded_mask, zlib.Z_BEST_COMPRESSION)\n",
    "  base64_str = base64.b64encode(binary_str)\n",
    "  return base64_str\n",
    "\n",
    "def decode_binary_mask(encoded_mask):\n",
    "    # Decode base64 and decompress the binary string\n",
    "    binary_str = base64.b64decode(encoded_mask)\n",
    "    print(binary_str)\n",
    "    decompressed_str = zlib.decompress(binary_str)\n",
    "    print(decompressed_str)\n",
    "    \n",
    "    # Decode RLE-encoded mask\n",
    "    encoded_mask = np.frombuffer(decompressed_str, dtype=np.uint8)\n",
    "    print(encoded_mask)\n",
    "    decoded_mask = coco_mask.decode(decompressed_str)\n",
    "    print(decoded_mask)\n",
    "\n",
    "    # Convert COCO API format to binary mask\n",
    "    mask = np.squeeze(decoded_mask)\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "55f6af67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'x\\xda\\xcb\\x0e\\x8f13\\xcc\\xb37\\xf6304\\xf070\\xf4\\x0f\\xc9H0\\x04\\x00;\\xc2\\x05\\x91'\n",
      "b'kW\\\\61n?3N010O01OTh`1'\n",
      "[107  87  92  54  49 110  63  51  78  48  49  48  79  48  49  79  84 104\n",
      "  96  49]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_28589/3116710107.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minput_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'eNrLDo8xM8yzN/YzMDTwNzD0D8lIMAQAO8IFkQ=='\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_binary_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_28589/3976150679.py\u001b[0m in \u001b[0;36mdecode_binary_mask\u001b[0;34m(encoded_mask)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mencoded_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecompressed_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mdecoded_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoco_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecompressed_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpycocotools/_mask.pyx\u001b[0m in \u001b[0;36mpycocotools._mask.decode\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpycocotools/_mask.pyx\u001b[0m in \u001b[0;36mpycocotools._mask._frString\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "input_str = 'eNrLDo8xM8yzN/YzMDTwNzD0D8lIMAQAO8IFkQ=='\n",
    "mask = decode_binary_mask(input_str.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8979f2ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a774c69c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15513b35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4547134f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb477fbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5878dbe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deff13d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528fcf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from multiprocessing import Pool\n",
    "def generate_submission_multiproc(model, device, dataloader, suffix):\n",
    "  model.eval()\n",
    "  num_batches = len(dataloader)\n",
    "  print(f'Processing a total of {num_batches} images for submission')\n",
    "  submission_dicts = []\n",
    "  start_time = time.time()\n",
    "  cur_batch = []\n",
    "  batch_size = 8\n",
    "  with torch.no_grad():\n",
    "    for batch_idx, (img_file, inputs, targets) in enumerate(dataloader):\n",
    "      if (batch_idx+1) % batch_size == 0:\n",
    "        with Pool(8) as pool:\n",
    "          result = pool.starmap(run_inference_multiproc, cur_batch)\n",
    "          return result\n",
    "      else:\n",
    "        cur_batch.append((model, device, img_file, inputs))\n",
    "\n",
    "\n",
    "def run_inference_multiproc(model, device, img_file, inputs):\n",
    "    with torch.no_grad():\n",
    "      cur_dict = dict()\n",
    "      img_id = img_file[0].split('/')[-1].split('.')[0]\n",
    "      cur_dict['id'] = img_id\n",
    "      cur_dict['height'] = 512\n",
    "      cur_dict['width'] = 512\n",
    "      prediction_string = ''\n",
    "      inputs = inputs.to(device)\n",
    "      outputs = model(inputs.unsqueeze(1))\n",
    "      outputs = torch.softmax(outputs, dim=1)[:,1,:,:].squeeze().detach().numpy()\n",
    "      outputs_height, outputs_width = outputs.shape\n",
    "      if outputs_height != cur_dict['height']:\n",
    "        outputs_zoomed = zoom(outputs, (float(cur_dict['height']/outputs_height), float(cur_dict['width']/outputs_width)), order=3)\n",
    "      else:\n",
    "        outputs_zoomed = outputs\n",
    "      outputs_zoomed_thresh = (outputs_zoomed>0.5).astype('uint8')\n",
    "      polygon_masks = extract_polygon_masks(outputs_zoomed_thresh)\n",
    "      for polygon_mask in polygon_masks:\n",
    "        polygon_mask_conf = round(((polygon_mask * outputs_zoomed).sum())/(polygon_mask.sum()), 2)\n",
    "        polygon_mask_string = encode_binary_mask(polygon_mask).decode('utf-8')\n",
    "        prediction_string += f'0 {polygon_mask_conf} {polygon_mask_string} '\n",
    "      cur_dict['prediction_string'] = prediction_string.strip()\n",
    "    return cur_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d903b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = generate_submission_multiproc(best_model, DEVICE, test_loader, suffix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HubMapEnv",
   "language": "python",
   "name": "hubmapenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
