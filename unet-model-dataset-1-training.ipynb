{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip3 install segmentation_models_pytorch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset as BaseDataset\nimport torch.nn as nn\nimport albumentations as albu\nimport torch\nimport segmentation_models_pytorch as smp","metadata":{"execution":{"iopub.status.busy":"2023-06-07T03:57:15.167985Z","iopub.execute_input":"2023-06-07T03:57:15.168956Z","iopub.status.idle":"2023-06-07T03:57:20.037282Z","shell.execute_reply.started":"2023-06-07T03:57:15.168918Z","shell.execute_reply":"2023-06-07T03:57:20.036264Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nDEVICE","metadata":{"execution":{"iopub.status.busy":"2023-06-07T03:57:20.038648Z","iopub.execute_input":"2023-06-07T03:57:20.039109Z","iopub.status.idle":"2023-06-07T03:57:20.136628Z","shell.execute_reply.started":"2023-06-07T03:57:20.039077Z","shell.execute_reply":"2023-06-07T03:57:20.135569Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}]},{"cell_type":"code","source":"DATA_DIR = './'\nx_train_dir = '/kaggle/input/all-dataset-files/all_dataset_files/all_dataset_files/all_dataset_imgs'\ny_train_dir = '/kaggle/input/all-dataset-files/all_dataset_files/all_dataset_files/all_dataset_masks'\n\nx_valid_dir = '/kaggle/input/all-dataset-files/sample_valid_imgs'\ny_valid_dir = '/kaggle/input/all-dataset-files/sample_valid_masks'","metadata":{"execution":{"iopub.status.busy":"2023-06-07T03:57:20.138573Z","iopub.execute_input":"2023-06-07T03:57:20.139089Z","iopub.status.idle":"2023-06-07T03:57:20.143910Z","shell.execute_reply.started":"2023-06-07T03:57:20.139055Z","shell.execute_reply":"2023-06-07T03:57:20.142782Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"len(os.listdir(x_train_dir)), len(os.listdir(y_train_dir)), len(os.listdir(x_valid_dir)), len(os.listdir(y_valid_dir))","metadata":{"execution":{"iopub.status.busy":"2023-06-07T03:57:20.147738Z","iopub.execute_input":"2023-06-07T03:57:20.148146Z","iopub.status.idle":"2023-06-07T03:57:20.161932Z","shell.execute_reply.started":"2023-06-07T03:57:20.148069Z","shell.execute_reply":"2023-06-07T03:57:20.161116Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"(1633, 1633, 3, 3)"},"metadata":{}}]},{"cell_type":"code","source":"# helper function for data visualization\ndef visualize(**images):\n    \"\"\"PLot images in one row.\"\"\"\n    n = len(images)\n    plt.figure(figsize=(16, 5))\n    for i, (name, image) in enumerate(images.items()):\n        plt.subplot(1, n, i + 1)\n        plt.xticks([])\n        plt.yticks([])\n        plt.title(' '.join(name.split('_')).title())\n        plt.imshow(image)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-07T03:57:20.163316Z","iopub.execute_input":"2023-06-07T03:57:20.163643Z","iopub.status.idle":"2023-06-07T03:57:20.171336Z","shell.execute_reply.started":"2023-06-07T03:57:20.163613Z","shell.execute_reply":"2023-06-07T03:57:20.169410Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class HubMapDataset(BaseDataset):\n    \"\"\"Read images, apply augmentation and preprocessing transformations.\n    \n    Args:\n        images_dir (str): path to images folder\n        masks_dir (str): path to segmentation masks folder\n        class_values (list): values of classes to extract from segmentation mask\n        augmentation (albumentations.Compose): data transfromation pipeline \n            (e.g. flip, scale, etc.)\n        preprocessing (albumentations.Compose): data preprocessing \n            (e.g. noralization, shape manipulation, etc.)\n    \n    \"\"\"\n    \n    CLASSES = ['unlabelled', 'blood_vessel']\n    \n    def __init__(\n            self, \n            images_dir, \n            masks_dir, \n            classes=None, \n            augmentation=None, \n            preprocessing=None,\n    ):\n        self.ids = os.listdir(images_dir)\n        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]\n        self.masks_fps = [os.path.join(masks_dir, image_id) for image_id in self.ids]\n        \n        # convert str names to class values on masks\n        self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]\n        \n        self.augmentation = augmentation\n        self.preprocessing = preprocessing\n    \n    def __getitem__(self, i):\n        \n        # read data\n        image = cv2.imread(self.images_fps[i])\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        mask = cv2.imread(self.masks_fps[i], 0)\n        \n        # extract certain classes from mask (e.g. cars)\n        masks = [(mask == v) for v in self.class_values]\n        mask = np.stack(masks, axis=-1).astype('float')\n        \n        # apply augmentations\n        if self.augmentation:\n            sample = self.augmentation(image=image, mask=mask)\n            image, mask = sample['image'], sample['mask']\n        \n        # apply preprocessing\n        if self.preprocessing:\n            sample = self.preprocessing(image=image, mask=mask)\n            image, mask = sample['image'], sample['mask']\n            \n        return self.masks_fps[i], image, mask\n        \n    def __len__(self):\n        return len(self.ids)","metadata":{"execution":{"iopub.status.busy":"2023-06-07T03:57:20.172759Z","iopub.execute_input":"2023-06-07T03:57:20.173020Z","iopub.status.idle":"2023-06-07T03:57:20.186633Z","shell.execute_reply.started":"2023-06-07T03:57:20.172999Z","shell.execute_reply":"2023-06-07T03:57:20.185450Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def get_training_augmentation():\n  train_transform = [\n    albu.ShiftScaleRotate(shift_limit=0, scale_limit=0, rotate_limit=90),\n    albu.ShiftScaleRotate(shift_limit=0.2, scale_limit=0, rotate_limit=0),\n    albu.ShiftScaleRotate(shift_limit=0, scale_limit=0.2, rotate_limit=0),\n    albu.Flip(),\n    albu.RandomBrightnessContrast(),\n    albu.RandomResizedCrop(height=512, width=512, scale=(0.8, 1.0), p=1)\n  ]\n  return albu.Compose(train_transform)\n\ndef to_tensor(x, **kwargs):\n    return x.transpose(2, 0, 1).astype('float32')\n\ndef get_preprocessing(preprocessing_fn):\n    \"\"\"Construct preprocessing transform\n    \n    Args:\n        preprocessing_fn (callbale): data normalization function \n            (can be specific for each pretrained neural network)\n    Return:\n        transform: albumentations.Compose\n    \n    \"\"\"\n    \n    _transform = [\n        albu.Lambda(image=preprocessing_fn),\n        albu.Lambda(image=to_tensor, mask=to_tensor),\n    ]\n    return albu.Compose(_transform)","metadata":{"execution":{"iopub.status.busy":"2023-06-07T03:57:20.189056Z","iopub.execute_input":"2023-06-07T03:57:20.189306Z","iopub.status.idle":"2023-06-07T03:57:20.199276Z","shell.execute_reply.started":"2023-06-07T03:57:20.189284Z","shell.execute_reply":"2023-06-07T03:57:20.198453Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"CLASSES = ['unlabelled', 'blood_vessel']\nENCODER = 'efficientnet-b7'\nENCODER_WEIGHTS = 'imagenet'","metadata":{"execution":{"iopub.status.busy":"2023-06-07T03:57:20.201701Z","iopub.execute_input":"2023-06-07T03:57:20.202137Z","iopub.status.idle":"2023-06-07T03:57:20.212712Z","shell.execute_reply.started":"2023-06-07T03:57:20.202106Z","shell.execute_reply":"2023-06-07T03:57:20.212138Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"model = smp.Unet(\n    encoder_name=ENCODER,        \n    encoder_weights=ENCODER_WEIGHTS,     \n    in_channels=3,                  \n    classes=len(CLASSES)\n)\nmodel = model.to(DEVICE)\nmodel = nn.DataParallel(model)","metadata":{"execution":{"iopub.status.busy":"2023-06-07T03:57:20.213834Z","iopub.execute_input":"2023-06-07T03:57:20.214981Z","iopub.status.idle":"2023-06-07T03:57:22.861085Z","shell.execute_reply.started":"2023-06-07T03:57:20.214949Z","shell.execute_reply":"2023-06-07T03:57:22.860125Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)","metadata":{"execution":{"iopub.status.busy":"2023-06-07T03:57:22.862437Z","iopub.execute_input":"2023-06-07T03:57:22.862921Z","iopub.status.idle":"2023-06-07T03:57:22.868984Z","shell.execute_reply.started":"2023-06-07T03:57:22.862889Z","shell.execute_reply":"2023-06-07T03:57:22.868048Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train_dataset = HubMapDataset(\n    x_train_dir, \n    y_train_dir, \n    augmentation=get_training_augmentation(),\n    preprocessing=get_preprocessing(preprocessing_fn),\n    classes=CLASSES,\n)\n\nvalid_dataset = HubMapDataset(\n    x_valid_dir, \n    y_valid_dir, \n    preprocessing=get_preprocessing(preprocessing_fn),\n    classes=CLASSES,\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)\nvalid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=False, num_workers=2)","metadata":{"execution":{"iopub.status.busy":"2023-06-07T03:57:22.870385Z","iopub.execute_input":"2023-06-07T03:57:22.871025Z","iopub.status.idle":"2023-06-07T03:57:22.887537Z","shell.execute_reply.started":"2023-06-07T03:57:22.870990Z","shell.execute_reply":"2023-06-07T03:57:22.886666Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"_, image, mask = train_dataset[0]\nprint(image.shape, mask.shape)","metadata":{"execution":{"iopub.status.busy":"2023-06-07T03:57:22.889712Z","iopub.execute_input":"2023-06-07T03:57:22.890654Z","iopub.status.idle":"2023-06-07T03:57:22.941275Z","shell.execute_reply.started":"2023-06-07T03:57:22.890622Z","shell.execute_reply":"2023-06-07T03:57:22.940239Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"(3, 512, 512) (2, 512, 512)\n","output_type":"stream"}]},{"cell_type":"code","source":"from torchmetrics import Metric\nclass IoUScore(Metric):\n    def __init__(self, threshold=0.5, dist_sync_on_step=False):\n        super().__init__(dist_sync_on_step=dist_sync_on_step)\n        self.threshold = threshold\n        self.add_state(\"intersection_back\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n        self.add_state(\"union_back\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n        self.add_state(\"intersection_fore\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n        self.add_state(\"union_fore\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n        self.add_state(\"num_images\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n\n    def update(self, preds: torch.Tensor, target: torch.Tensor):\n        preds = (preds > self.threshold).int()\n        intersection_back = torch.logical_and(preds[:,0,:,:], target[:,0,:,:]).sum()\n        union_back = torch.logical_or(preds[:,0,:,:], target[:,0,:,:]).sum()\n        intersection_fore = torch.logical_and(preds[:,1,:,:], target[:,1,:,:]).sum()\n        union_fore = torch.logical_or(preds[:,1,:,:], target[:,1,:,:]).sum()\n        num_images = preds.shape[0]\n\n        self.intersection_back += intersection_back\n        self.union_back += union_back\n        self.intersection_fore += intersection_fore\n        self.union_fore += union_fore\n        self.num_images += num_images\n\n    def compute(self):\n        print(f'num images is: {self.num_images}')\n        iou_back = (self.intersection_back.float() / self.union_back.float())\n        iou_fore = (self.intersection_fore.float() / self.union_fore.float())\n        self.intersection_back = 0\n        self.union_back = 0\n        self.intersection_fore = 0\n        self.union_fore = 0\n        self.num_images = 0\n        return iou_back,iou_fore","metadata":{"execution":{"iopub.status.busy":"2023-06-07T03:57:22.945282Z","iopub.execute_input":"2023-06-07T03:57:22.945580Z","iopub.status.idle":"2023-06-07T03:57:26.268133Z","shell.execute_reply.started":"2023-06-07T03:57:22.945555Z","shell.execute_reply":"2023-06-07T03:57:26.267164Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"import torchmetrics\nloss = smp.losses.DiceLoss(mode='binary', from_logits=True)\nmetrics = [\n    IoUScore(threshold=0.5).to(DEVICE),\n]\n\noptimizer = torch.optim.Adam([ \n    dict(params=model.parameters(), lr=0.0001),\n])","metadata":{"execution":{"iopub.status.busy":"2023-06-07T03:57:26.269595Z","iopub.execute_input":"2023-06-07T03:57:26.270748Z","iopub.status.idle":"2023-06-07T03:57:27.247770Z","shell.execute_reply.started":"2023-06-07T03:57:26.270713Z","shell.execute_reply":"2023-06-07T03:57:27.246768Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\n# Training loop\ndef train_epoch(model, loss_fn, metrics, optimizer, device, dataloader):\n    model.train()\n    num_batches = len(dataloader)\n    total_loss = 0\n    print(f'Processing a total of {num_batches} batches in training')\n    # Iterate over the training dataset\n    for batch_idx, (f, inputs, targets) in tqdm(enumerate(dataloader)):        \n        inputs = inputs.to(device)\n        targets = targets.to(device)\n\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = model(inputs)\n#         print(f'input and output shapes: {inputs.shape}, {outputs.shape}, {targets.shape}')\n#         print(f'Outputs min: {torch.min(outputs)}, Outputs max: {torch.max(outputs)}')\n        loss = loss_fn(outputs, targets)\n\n        # Backward pass\n        loss.backward()\n        optimizer.step()\n\n        # Compute metrics\n        for metric in metrics:\n            metric.update(torch.softmax(outputs, dim=1), targets)\n        total_loss += loss\n\n    # Get the metric values\n    metric_values = [float(total_loss)/num_batches] + [metric.compute() for metric in metrics]\n    return metric_values\n\n# Validation loop\ndef valid_epoch(model, loss_fn, metrics, device, dataloader):\n    model.eval()\n    num_batches = len(dataloader)\n    total_loss = 0\n    print(f'Processing a total of {num_batches} batches in validation')\n    # Disable gradient calculation\n    with torch.no_grad():\n        # Iterate over the validation dataset\n        for batch_idx, (f, inputs, targets) in tqdm(enumerate(dataloader)):\n            inputs = inputs.to(device)\n            targets = targets.to(device)\n\n            # Forward pass\n            outputs = model(inputs)\n            loss = loss_fn(outputs, targets)\n\n            # Compute metrics\n            for metric in metrics:\n                metric.update(torch.softmax(outputs, dim=1), targets)\n            total_loss += loss\n\n    # Get the metric values\n    metric_values = [float(total_loss)/num_batches] + [metric.compute() for metric in metrics]\n    return metric_values","metadata":{"execution":{"iopub.status.busy":"2023-06-07T03:57:27.249364Z","iopub.execute_input":"2023-06-07T03:57:27.249961Z","iopub.status.idle":"2023-06-07T03:57:27.262769Z","shell.execute_reply.started":"2023-06-07T03:57:27.249925Z","shell.execute_reply":"2023-06-07T03:57:27.261720Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"import time\nmax_iou = 0\nnum_epochs = 50\nif os.path.exists('/kaggle/working/model_stats.txt'):\n  os.remove('/kaggle/working/model_stats.txt')\nfp = open('/kaggle/working/model_stats.txt', 'a')\nfor epoch in range(num_epochs):\n    # Training\n    start_time = time.time()\n    train_metrics = train_epoch(model, loss, metrics, optimizer, DEVICE, train_loader)\n    print(f'=========Finished Training Epoch {epoch} in {float(time.time()-start_time)/60}==========')\n    # Validation\n    start_time = time.time()\n    valid_metrics = valid_epoch(model, loss, metrics, DEVICE, valid_loader)\n    print(f'=========Finished Validation Epoch {epoch} {float(time.time()-start_time)/60}in =========')\n    \n    save_interval = 10\n    if (epoch+1) % 10 == 0:\n        torch.save(model, f'/kaggle/working/model_{epoch}.pth')\n    \n    cur_validation_iou = 0.5*valid_metrics[1][0] + 0.5*valid_metrics[1][1]\n    if cur_validation_iou > max_iou:\n      print(f'Saving model with IoU: {cur_validation_iou}...')\n      torch.save(model, '/kaggle/working/best_model.pth')\n      with open('/kaggle/working/best_model.txt', 'w') as f:\n        f.write(f\"Epoch {epoch}: Train Loss={train_metrics[0]}, Validation Loss={valid_metrics[0]}, Train IoU Back={train_metrics[1][0]}, Train IoU Fore={train_metrics[1][1]}, Validation IoU Back={valid_metrics[1][0]}, Validation IoU Fore={valid_metrics[1][1]}\")\n      max_iou = cur_validation_iou\n    # Print or log the metrics for each epoch\n    print(f\"Epoch {epoch}: Train Loss={train_metrics[0]}, Validation Loss={valid_metrics[0]}, Train IoU Back={train_metrics[1][0]}, Train IoU Fore={train_metrics[1][1]}, Validation IoU Back={valid_metrics[1][0]}, Validation IoU Fore={valid_metrics[1][1]}\")\n    fp.write(f\"Epoch {epoch}: Train Loss={train_metrics[0]}, Validation Loss={valid_metrics[0]}, Train IoU Back={train_metrics[1][0]}, Train IoU Fore={train_metrics[1][1]}, Validation IoU Back={valid_metrics[1][0]}, Validation IoU Fore={valid_metrics[1][1]}\\n\")\n    fp.flush()\nfp.close()","metadata":{"execution":{"iopub.status.busy":"2023-06-07T03:57:27.264623Z","iopub.execute_input":"2023-06-07T03:57:27.265083Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Processing a total of 205 batches in training\n","output_type":"stream"},{"name":"stderr","text":"205it [04:10,  1.22s/it]\n","output_type":"stream"},{"name":"stdout","text":"num images is: 1633\n=========Finished Training Epoch 0 in 4.182070712248485==========\nProcessing a total of 1 batches in validation\n","output_type":"stream"},{"name":"stderr","text":"1it [00:00,  1.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"num images is: 3\n=========Finished Validation Epoch 0 0.01372600793838501in =========\nSaving model with IoU: 0.49571308493614197...\nEpoch 0: Train Loss=0.4098246504620808, Validation Loss=0.2467450499534607, Train IoU Back=0.7560134530067444, Train IoU Fore=0.04336860030889511, Validation IoU Back=0.9910023808479309, Validation IoU Fore=0.00042378867510706186\nProcessing a total of 205 batches in training\n","output_type":"stream"},{"name":"stderr","text":"205it [04:04,  1.19s/it]\n","output_type":"stream"},{"name":"stdout","text":"num images is: 1633\n=========Finished Training Epoch 1 in 4.090193243821462==========\nProcessing a total of 1 batches in validation\n","output_type":"stream"},{"name":"stderr","text":"1it [00:00,  1.29it/s]","output_type":"stream"},{"name":"stdout","text":"num images is: 3\n=========Finished Validation Epoch 1 0.018679459889729817in =========\nEpoch 1: Train Loss=0.2143523239507908, Validation Loss=0.12928783893585205, Train IoU Back=0.9538811445236206, Train IoU Fore=0.0122360959649086, Validation IoU Back=0.9908599853515625, Validation IoU Fore=0.00027816410874947906\nProcessing a total of 205 batches in training\n","output_type":"stream"},{"name":"stderr","text":"\n32it [00:38,  1.18s/it]","output_type":"stream"}]},{"cell_type":"code","source":"# Inference\nDEVICE = 'cpu'\nbest_model = torch.load('./models/best_model.pth', map_location=torch.device('cpu'))\nbest_model = best_model.to(DEVICE)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CLASSES = ['blood_vessel']\ntest_dataset = HubMapDataset(\n    x_test_dir, \n    y_test_dir, \n    augmentation=get_validation_augmentation(), \n    preprocessing=get_preprocessing(preprocessing_fn),\n    classes=CLASSES,\n)\n\ntest_loader = DataLoader(test_dataset)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_metrics = valid_epoch(best_model, loss, metrics, DEVICE, test_loader)\nprint(f'Test Loss: {valid_metrics[0]}, Test IoU: {valid_metrics[1]}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset_without_aug = HubMapDataset(\n    x_test_dir, y_test_dir, \n    classes=CLASSES,\n)\ntrain_dataset_without_aug = HubMapDataset(\n    x_train_dir, y_train_dir, \n    classes=CLASSES,\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_dataset = test_dataset\ntarget_dataset_without_aug = test_dataset_without_aug\nfor i in range(20):\n    n = np.random.choice(len(target_dataset))\n    \n    image_vis = target_dataset_without_aug[n][0].astype('uint8')\n    image, gt_mask = target_dataset[n]\n    image_trans = image.transpose(1,2,0)\n    print(f'image tran shape: {image_trans.shape}')\n    print(image.shape, gt_mask.shape)\n    \n    gt_mask = gt_mask.squeeze()\n    \n    x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n    print(x_tensor.shape)\n    pr_mask = torch.sigmoid(best_model.predict(x_tensor))\n    print(pr_mask.shape)\n    pr_mask = (pr_mask.squeeze().cpu().numpy().round())\n    print(pr_mask.shape)\n    \n    visualize(\n        image=image_trans, \n        ground_truth_mask=gt_mask, \n        predicted_mask=pr_mask\n    )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize(\n        image=image_trans, \n        ground_truth_mask=gt_mask, \n        predicted_mask=pr_mask\n    )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(pr_mask.shape)\nprint(pr_mask)\nplt.imshow(pr_mask)\n# im2, contours, hierarchy = cv2.findContours(pr_mask.astype('uint8'),cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}