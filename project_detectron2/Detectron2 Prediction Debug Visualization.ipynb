{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "505672e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import shutil\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from sklearn.model_selection import KFold\n",
    "import detectron2.utils.comm as comm\n",
    "from detectron2.checkpoint import DetectionCheckpointer, PeriodicCheckpointer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import default_argument_parser, default_setup, default_writers, launch\n",
    "from detectron2.evaluation import (\n",
    "    CityscapesInstanceEvaluator,\n",
    "    CityscapesSemSegEvaluator,\n",
    "    COCOEvaluator,\n",
    "    COCOPanopticEvaluator,\n",
    "    DatasetEvaluators,\n",
    "    LVISEvaluator,\n",
    "    PascalVOCDetectionEvaluator,\n",
    "    SemSegEvaluator,\n",
    "    inference_on_dataset,\n",
    "    print_csv_format,\n",
    ")\n",
    "from detectron2.data import (\n",
    "    DatasetCatalog,\n",
    "    MetadataCatalog,\n",
    "    build_detection_test_loader,\n",
    "    build_detection_train_loader,\n",
    ")\n",
    "from detectron2.modeling import build_model\n",
    "from detectron2.solver import build_lr_scheduler, build_optimizer\n",
    "from detectron2.utils.events import EventStorage\n",
    "import cv2\n",
    "import pickle\n",
    "from detectron2.structures import BoxMode\n",
    "from detectron2.data import DatasetMapper\n",
    "import detectron2.data.transforms as T\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "import numpy as np\n",
    "from detectron2.utils.visualizer import Visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d0cea9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_border(image, border_size, border_color):\n",
    "    height, width = image.shape[:2]\n",
    "    new_height = height + 2 * border_size\n",
    "    new_width = width + 2 * border_size\n",
    "    bordered_image = np.zeros((new_height, new_width, 3), dtype=np.uint8)\n",
    "    bordered_image[border_size:height + border_size, border_size:width + border_size] = image\n",
    "    cv2.rectangle(bordered_image, (0, 0), (new_width - 1, new_height - 1), border_color, border_size)\n",
    "    return bordered_image\n",
    "def get_evaluator(cfg, dataset_name, output_folder=None):\n",
    "    \"\"\"\n",
    "    Create evaluator(s) for a given dataset.\n",
    "    This uses the special metadata \"evaluator_type\" associated with each builtin dataset.\n",
    "    For your own dataset, you can simply create an evaluator manually in your\n",
    "    script and do not have to worry about the hacky if-else logic here.\n",
    "    \"\"\"\n",
    "    if output_folder is None:\n",
    "        output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n",
    "    evaluator_list = []\n",
    "    evaluator_type = MetadataCatalog.get(dataset_name).evaluator_type\n",
    "    if evaluator_type in [\"sem_seg\", \"coco_panoptic_seg\"]:\n",
    "        evaluator_list.append(\n",
    "            SemSegEvaluator(\n",
    "                dataset_name,\n",
    "                distributed=True,\n",
    "                output_dir=output_folder,\n",
    "            )\n",
    "        )\n",
    "    if evaluator_type in [\"coco\", \"coco_panoptic_seg\"]:\n",
    "        evaluator_list.append(COCOEvaluator(dataset_name, output_dir=output_folder))\n",
    "    if evaluator_type == \"coco_panoptic_seg\":\n",
    "        evaluator_list.append(COCOPanopticEvaluator(dataset_name, output_folder))\n",
    "    if evaluator_type == \"cityscapes_instance\":\n",
    "        return CityscapesInstanceEvaluator(dataset_name)\n",
    "    if evaluator_type == \"cityscapes_sem_seg\":\n",
    "        return CityscapesSemSegEvaluator(dataset_name)\n",
    "    if evaluator_type == \"pascal_voc\":\n",
    "        return PascalVOCDetectionEvaluator(dataset_name)\n",
    "    if evaluator_type == \"lvis\":\n",
    "        return LVISEvaluator(dataset_name, cfg, True, output_folder)\n",
    "    if len(evaluator_list) == 0:\n",
    "        raise NotImplementedError(\n",
    "            \"no Evaluator for the dataset {} with the type {}\".format(dataset_name, evaluator_type)\n",
    "        )\n",
    "    if len(evaluator_list) == 1:\n",
    "        return evaluator_list[0]\n",
    "    return DatasetEvaluators(evaluator_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db613d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HubMapDataset:\n",
    "    def __init__(self, image_dir, annotation_dir):\n",
    "        self.image_dir = image_dir\n",
    "        self.annotation_dir = annotation_dir\n",
    "        self.image_file_list = sorted(os.listdir(self.image_dir))\n",
    "        self.annotation_file_list = os.listdir(self.annotation_dir)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_file_list)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_dir, self.image_file_list[idx])\n",
    "        img_id = self.image_file_list[idx].split('.png')[0]\n",
    "        annotation_path = os.path.join(self.annotation_dir, f'{img_id}.pkl')\n",
    "        \n",
    "        image = cv2.imread(image_path)\n",
    "        height, width = image.shape[:2]\n",
    "        \n",
    "        record = {\n",
    "            'file_name': image_path,\n",
    "            'image_id': idx,\n",
    "            'height': height,\n",
    "            'width': width,\n",
    "        }\n",
    "        \n",
    "        with open(annotation_path, 'rb') as f:\n",
    "            orig_annots = pickle.load(f)\n",
    "        \n",
    "        objs = []\n",
    "        for orig_annot in orig_annots:\n",
    "            bbox = orig_annot['bbox']\n",
    "            orig_annot['bbox'] = [bbox[0], bbox[1], bbox[0]+bbox[2], bbox[1]+bbox[3]]\n",
    "            orig_annot['bbox_mode'] = BoxMode.XYXY_ABS\n",
    "            objs.append(orig_annot)\n",
    "            \n",
    "        record['annotations'] = objs\n",
    "        \n",
    "        return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d15c7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = ['blood_vessel']\n",
    "def register_custom_dataset(dataset_name, image_dir, annotation_dir):\n",
    "    DatasetCatalog.register(dataset_name, lambda: HubMapDataset(image_dir, annotation_dir))\n",
    "    MetadataCatalog.get(dataset_name).set(thing_classes=CLASSES, evaluator_type=\"coco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca0738b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 5\n",
    "config_file = '/home/ec2-user/hubmap-hacking-the-human-vasculature/detectron2/configs/COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml'\n",
    "base_model_path = '/home/ec2-user/hubmap-hacking-the-human-vasculature/project_detectron2/output/inference'\n",
    "base_dataset_path = '/home/ec2-user/hubmap-hacking-the-human-vasculature/dataset1_files'\n",
    "base_dataset_name = 'hubmap-dataset1'\n",
    "num_machines = 1\n",
    "num_gpus = 1\n",
    "machine_rank = 0\n",
    "port = 2**15 + 2**14 + hash(os.getuid() if sys.platform != \"win32\" else 1) % 2**14\n",
    "dist_url = \"tcp://127.0.0.1:{}\".format(port)\n",
    "opts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14566d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "register_custom_dataset(f'{base_dataset_name}-train-fold-0', f'{base_dataset_path}/all_dataset1_imgs_merged_train_0', f'{base_dataset_path}/all_dataset1_annotations_merged_train_0')\n",
    "register_custom_dataset(f'{base_dataset_name}-validation-fold-0', f'{base_dataset_path}/all_dataset1_imgs_merged_validation_0', f'{base_dataset_path}/all_dataset1_annotations_merged_validation_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "badd13fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file('COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml'))\n",
    "cfg.MODEL.WEIGHTS = f'{base_model_path}/best_model_fold_0_current_best_2.pth'\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
    "cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST = 0.5\n",
    "cfg.MODEL.DEVICE = 'cpu'\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b51e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========Stats for score thresh: 0.05==============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/detectron_env/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# score_thresholds = [x for x in np.arange(0, 1, 0.1)]\n",
    "# for score_thresh in score_thresholds:\n",
    "start_time = time.time()\n",
    "# cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = float(score_thresh) if score_thresh>0 else 0.001\n",
    "predictor = DefaultPredictor(cfg)\n",
    "evaluator = get_evaluator(\n",
    "    cfg, f'{base_dataset_name}-validation-fold-0', os.path.join(cfg.OUTPUT_DIR, \"inference\", f'{base_dataset_name}-validation-fold-0')\n",
    ")\n",
    "validation_data_loader = build_detection_test_loader(cfg, f'{base_dataset_name}-validation-fold-0')\n",
    "print(f'=========Stats for score thresh: {cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST}==============')\n",
    "print(inference_on_dataset(predictor.model, validation_data_loader, evaluator))\n",
    "print(f'Time Taken: {(time.time()-start_time)/60} minutes')\n",
    "print(f'=========Stats for score thresh: {cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST}==============')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376e877f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the metadata for your dataset\n",
    "metadata = MetadataCatalog.get(f'{base_dataset_name}-validation-fold-0')\n",
    "\n",
    "# Get the dataset dictionary\n",
    "dataset_dicts = DatasetCatalog.get(f'{base_dataset_name}-validation-fold-0')\n",
    "\n",
    "print(f'Dataset Dicsts length is: {len(dataset_dicts)}')\n",
    "print(dataset_dicts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3b2f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "tgt_coco_dicts = [dataset_dicts[i] for i in range(50)]\n",
    "# tgt_dict = None\n",
    "# for d in dataset_dicts:\n",
    "#     if 'e6d9bd78b840' in d['file_name']:\n",
    "#         tgt_coco_dicts.append(d)\n",
    "border_size = 2\n",
    "border_color = (255, 0, 0)\n",
    "score_thresholds = [x for x in np.arange(0, 1, 0.1)]\n",
    "\n",
    "for i, d in enumerate(tgt_coco_dicts):\n",
    "    # read the image with cv2\n",
    "    img = cv2.imread(d[\"file_name\"])\n",
    "    test_data_loader = build_detection_test_loader(d, mapper=DatasetMapper(cfg, is_train=False))\n",
    "#     print(d.keys())\n",
    "#     print(d['annotations'])\n",
    "    outputs = predictor(img)\n",
    "    pred_masks = outputs['instances'].pred_masks\n",
    "    pred_scores = outputs['instances'].scores\n",
    "    print(d['file_name'])\n",
    "    print(pred_scores.shape)\n",
    "    print(pred_masks.shape)\n",
    "    all_images = []\n",
    "    visualizer = Visualizer(img[:, :, ::-1], metadata=metadata, scale=1.0)\n",
    "    vis = visualizer.draw_dataset_dict(d)\n",
    "    img_with_ground_truth_overlay = vis.get_image()[:, :, ::-1]\n",
    "    img_with_ground_truth_overlay = add_border(img_with_ground_truth_overlay, border_size, border_color)\n",
    "    all_images.append(img_with_ground_truth_overlay)\n",
    "    for score_thresh in score_thresholds:\n",
    "        visualizer_pred = Visualizer(img[:, :, ::-1], metadata=metadata, scale=1.0)\n",
    "        visualizer_pred.overlay_instances(\n",
    "            masks=pred_masks[pred_scores>score_thresh, :, :],\n",
    "            alpha=0.5  # Adjust transparency if needed\n",
    "            )\n",
    "        img_with_mask_overlay = visualizer_pred.get_output().get_image()[:, :, ::-1]\n",
    "        img_with_mask_overlay = add_border(img_with_mask_overlay, border_size, border_color)\n",
    "        all_images.append(img_with_mask_overlay)\n",
    "    num_imgs_in_slide = 3\n",
    "    show_images_arr = []\n",
    "    for i in range(len(all_images)):\n",
    "        show_images_arr.append(all_images[i])\n",
    "        if (i+1) % num_imgs_in_slide == 0:\n",
    "            final_image = cv2.hconcat(show_images_arr)\n",
    "            plt.figure(figsize=(50, 50))\n",
    "            plt.imshow(final_image)\n",
    "            plt.show()\n",
    "            show_images_arr = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f980a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the metadata for your dataset\n",
    "metadata = MetadataCatalog.get(f'{base_dataset_name}-train-fold-0')\n",
    "\n",
    "# Get the dataset dictionary\n",
    "dataset_dicts = DatasetCatalog.get(f'{base_dataset_name}-train-fold-0')\n",
    "\n",
    "print(f'Dataset Dicsts length is: {len(dataset_dicts)}')\n",
    "print(dataset_dicts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b69a970",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "tgt_coco_dicts = [dataset_dicts[i] for i in range(20)]\n",
    "# tgt_dict = None\n",
    "# for d in dataset_dicts:\n",
    "#     if 'e6d9bd78b840' in d['file_name']:\n",
    "#         tgt_coco_dicts.append(d)\n",
    "border_size = 2\n",
    "border_color = (255, 0, 0)\n",
    "score_thresholds = [x for x in np.arange(0, 1, 0.1)]\n",
    "\n",
    "for i, d in enumerate(tgt_coco_dicts):\n",
    "    # read the image with cv2\n",
    "    img = cv2.imread(d[\"file_name\"])\n",
    "#     print(d.keys())\n",
    "#     print(d['annotations'])\n",
    "    outputs = predictor(img)\n",
    "    pred_masks = outputs['instances'].pred_masks\n",
    "    pred_scores = outputs['instances'].scores\n",
    "    print(d['file_name'])\n",
    "    print(pred_scores.shape)\n",
    "    print(pred_masks.shape)\n",
    "    all_images = []\n",
    "    visualizer = Visualizer(img[:, :, ::-1], metadata=metadata, scale=1.0)\n",
    "    vis = visualizer.draw_dataset_dict(d)\n",
    "    img_with_ground_truth_overlay = vis.get_image()[:, :, ::-1]\n",
    "    img_with_ground_truth_overlay = add_border(img_with_ground_truth_overlay, border_size, border_color)\n",
    "    all_images.append(img_with_ground_truth_overlay)\n",
    "    for score_thresh in score_thresholds:\n",
    "        visualizer_pred = Visualizer(img[:, :, ::-1], metadata=metadata, scale=1.0)\n",
    "        visualizer_pred.overlay_instances(\n",
    "            masks=pred_masks[pred_scores>score_thresh, :, :],\n",
    "            alpha=0.5  # Adjust transparency if needed\n",
    "            )\n",
    "        img_with_mask_overlay = visualizer_pred.get_output().get_image()[:, :, ::-1]\n",
    "        img_with_mask_overlay = add_border(img_with_mask_overlay, border_size, border_color)\n",
    "        all_images.append(img_with_mask_overlay)\n",
    "    num_imgs_in_slide = 3\n",
    "    show_images_arr = []\n",
    "    for i in range(len(all_images)):\n",
    "        show_images_arr.append(all_images[i])\n",
    "        if (i+1) % num_imgs_in_slide == 0:\n",
    "            final_image = cv2.hconcat(show_images_arr)\n",
    "            plt.figure(figsize=(50, 50))\n",
    "            plt.imshow(final_image)\n",
    "            plt.show()\n",
    "            show_images_arr = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23acd433",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detectron_env",
   "language": "python",
   "name": "detectron_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
