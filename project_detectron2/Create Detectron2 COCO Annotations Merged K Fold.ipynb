{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c82b133",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import json\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "684dd9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '..'\n",
    "generate_all_datset_annots = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93399e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '..'\n",
    "num_folds = 5\n",
    "generate_masks = True\n",
    "should_dilate = False\n",
    "use_merged_dataset = False\n",
    "generate_all_datset_annots = True\n",
    "base_data_dir = 'dataset1_files' if not generate_all_datset_annots else 'all_dataset_files'\n",
    "base_data_name = 'all_dataset1' if not generate_all_datset_annots else 'all_dataset'\n",
    "get_whole_dataset_into_fold = False\n",
    "pick_validation_from_file = False\n",
    "input_imgs_dir = f'{base_path}/{base_data_dir}/{base_data_name}_imgs_merged' if use_merged_dataset else f'{base_path}/{base_data_dir}/{base_data_name}_imgs'\n",
    "input_annots_dir = f'{base_path}/{base_data_dir}/{base_data_name}_annotations_merged_cleaned' if use_merged_dataset else f'{base_path}/{base_data_dir}/{base_data_name}_annotations'\n",
    "vis_output_imgs_dir = f'{base_path}/{base_data_dir}/{base_data_name}_vis'\n",
    "if not os.path.exists(vis_output_imgs_dir):\n",
    "    os.mkdir(vis_output_imgs_dir)\n",
    "\n",
    "for i in range(num_folds):\n",
    "    train_modes = ['train', 'validation']\n",
    "    for train_mode in train_modes:\n",
    "        output_imgs_dir = f'{base_path}/{base_data_dir}/{base_data_name}_imgs_merged_{train_mode}_{i}' if use_merged_dataset else f'{base_path}/{base_data_dir}/{base_data_name}_imgs_{train_mode}_{i}'\n",
    "        output_annots_dir = f'{base_path}/{base_data_dir}/{base_data_name}_annotations_merged_{train_mode}_{i}' if use_merged_dataset else f'{base_path}/{base_data_dir}/{base_data_name}_annotations_{train_mode}_{i}'\n",
    "        output_masks_dir = f'{base_path}/{base_data_dir}/{base_data_name}_masks_merged_{train_mode}_{i}' if use_merged_dataset else f'{base_path}/{base_data_dir}/{base_data_name}_masks_{train_mode}_{i}'\n",
    "        if not os.path.exists(output_imgs_dir):\n",
    "            os.mkdir(output_imgs_dir)\n",
    "        if not os.path.exists(output_annots_dir):\n",
    "            os.mkdir(output_annots_dir)\n",
    "        if generate_masks and not os.path.exists(output_masks_dir):\n",
    "          os.mkdir(output_masks_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02cfe4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count_type(tiles_dict):\n",
    "  ids_with_info = []\n",
    "  for tile in tiles_dict:\n",
    "    cur_dict = {'id': tile['id'], 'blood_vessel': 0, 'glomerulus': 0, 'unsure': 0}\n",
    "    for annot in tile['annotations']:\n",
    "      cur_dict[annot['type']] += 1\n",
    "    ids_with_info.append(cur_dict)\n",
    "  return ids_with_info\n",
    "def calculate_area(coordinates):\n",
    "    num_points = len(coordinates)\n",
    "    if num_points < 3:\n",
    "        return 0\n",
    "\n",
    "    area = 0\n",
    "    for i in range(num_points - 1):\n",
    "        x_i, y_i = coordinates[i]\n",
    "        x_iplus1, y_iplus1 = coordinates[i + 1]\n",
    "        area += (x_i * y_iplus1) - (x_iplus1 * y_i)\n",
    "\n",
    "    x_n, y_n = coordinates[-1]\n",
    "    x_0, y_0 = coordinates[0]\n",
    "    area += (x_n * y_0) - (x_0 * y_n)\n",
    "\n",
    "    area = abs(area / 2)\n",
    "    return area\n",
    "# helper function for data visualization\n",
    "def visualize(**images):\n",
    "    \"\"\"PLot images in one row.\"\"\"\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(30, 30))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(' '.join(name.split('_')).title())\n",
    "        plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0746f13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{base_path}/polygons.jsonl', 'r') as json_file:\n",
    "    json_list = list(json_file)\n",
    "    \n",
    "tiles_dicts = []\n",
    "for json_str in json_list:\n",
    "    tiles_dicts.append(json.loads(json_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "772e29f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source_wsi</th>\n",
       "      <th>dataset</th>\n",
       "      <th>i</th>\n",
       "      <th>j</th>\n",
       "      <th>blood_vessel</th>\n",
       "      <th>glomerulus</th>\n",
       "      <th>unsure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0006ff2aa7cd</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>16896</td>\n",
       "      <td>16420</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00168d1b7522</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>14848</td>\n",
       "      <td>14884</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0033bbc76b6b</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10240</td>\n",
       "      <td>43008</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>003504460b3a</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>8192</td>\n",
       "      <td>11776</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>004daf1cbe75</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>6144</td>\n",
       "      <td>11264</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  source_wsi  dataset      i      j  blood_vessel  glomerulus  \\\n",
       "0  0006ff2aa7cd           2        2  16896  16420             8           1   \n",
       "1  00168d1b7522           2        2  14848  14884             1           1   \n",
       "2  0033bbc76b6b           1        1  10240  43008             3           0   \n",
       "3  003504460b3a           3        2   8192  11776             7           0   \n",
       "4  004daf1cbe75           3        2   6144  11264            10           1   \n",
       "\n",
       "   unsure  \n",
       "0       0  \n",
       "1       0  \n",
       "2       1  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tile_df = pd.read_csv(f'{base_path}/tile_meta.csv')\n",
    "# Find the same stats as the above cell for annotated images\n",
    "annotated_ids_with_info = get_count_type(tiles_dicts)\n",
    "annotated_ids_with_info_df = pd.DataFrame.from_dict(annotated_ids_with_info)\n",
    "tile_df_annotated = pd.merge(tile_df, annotated_ids_with_info_df, on='id', how='inner')\n",
    "tile_df_annotated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bf90c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "## Below is the code to create variability by merging adjacent tiles\n",
    "def get_relevant_annots(tgt_tile_dicts):\n",
    "  final_tile_dicts = dict()\n",
    "  for tile_dict in tgt_tile_dicts:\n",
    "    img_id = tile_dict['id']\n",
    "    final_annots = []\n",
    "    for annot in tile_dict['annotations']:\n",
    "      if annot['type'] == 'blood_vessel':\n",
    "        final_annots.append(annot['coordinates'][0])\n",
    "    final_tile_dicts[img_id] = final_annots\n",
    "  return final_tile_dicts\n",
    "def get_adjacent_tiles(i, j, img_id, adjacent_tile_dict, dataset_1_pos_dict):\n",
    "  adjacent_tile_dict[img_id] = dict()\n",
    "  keys = ['B', 'L', 'R', 'BL', 'BR']\n",
    "  for k in keys:\n",
    "    tgt_img_id = None\n",
    "    if k == 'B':\n",
    "      pos_to_check = (i, j+512)\n",
    "    elif k == 'L':\n",
    "      pos_to_check = (i-512, j)\n",
    "    elif k == 'R':\n",
    "      pos_to_check = (i+512, j)\n",
    "    elif k == 'BL':\n",
    "      pos_to_check = (i-512, j+512)\n",
    "    elif k == 'BR':\n",
    "      pos_to_check = (i+512, j+512)\n",
    "    if pos_to_check in dataset_1_pos_dict:\n",
    "      tgt_img_id = dataset_1_pos_dict[pos_to_check]\n",
    "    adjacent_tile_dict[img_id][k] = tgt_img_id\n",
    "  return adjacent_tile_dict\n",
    "def single_shift_polygon(seg_array_1, seg_array_2, shifting_threshold, shift_type, border_eps=1):\n",
    "    polygon_regions = []\n",
    "    num_polygons_1 = len(seg_array_1)\n",
    "    num_polygons_2 = len(seg_array_2)\n",
    "    \n",
    "    combined_array_1 = []\n",
    "    combined_array_2 = []\n",
    "    for i in range(num_polygons_1):\n",
    "        polygon_1 = seg_array_1[i]\n",
    "        new_polygon = []\n",
    "        for point in polygon_1:\n",
    "            x, y = point\n",
    "            if shift_type == 'R':\n",
    "              if x >= math.ceil(shifting_threshold*512):\n",
    "                  new_x = x - int(math.ceil(shifting_threshold*512))\n",
    "                  new_y = y\n",
    "                  new_polygon.append([new_x, new_y])\n",
    "            elif shift_type == 'B':\n",
    "              if y >= math.ceil(shifting_threshold*512):\n",
    "                new_x = x\n",
    "                new_y = y - int(math.ceil(shifting_threshold*512))\n",
    "                new_polygon.append([new_x, new_y])\n",
    "            elif shift_type == 'L':\n",
    "              if x <= math.floor((1-shifting_threshold)*512):\n",
    "                new_x = x + int(math.ceil(shifting_threshold*512))\n",
    "                new_y = y\n",
    "                new_polygon.append([new_x, new_y])\n",
    "        if len(new_polygon) > 0:\n",
    "          combined_array_1.append(new_polygon)\n",
    "    \n",
    "    polygon_regions.append(combined_array_1)\n",
    "    \n",
    "    for i in range(num_polygons_2):\n",
    "        polygon_2 = seg_array_2[i]\n",
    "        new_polygon = []\n",
    "        for point in polygon_2:\n",
    "            x, y = point\n",
    "            if shift_type == 'R':\n",
    "              if x <= math.ceil(shifting_threshold*512):\n",
    "                new_x = x + int(math.floor((1-shifting_threshold)*512))\n",
    "                new_y = y\n",
    "                new_polygon.append([new_x, new_y])\n",
    "            elif shift_type == 'B':\n",
    "              if y <= math.ceil(shifting_threshold*512):\n",
    "                new_x = x\n",
    "                new_y = y + int(math.floor((1-shifting_threshold)*512))\n",
    "                new_polygon.append([new_x, new_y])\n",
    "            elif shift_type == 'L':\n",
    "              if x >= math.floor((1-shifting_threshold)*512):\n",
    "                  new_x = x - int(math.floor((1-shifting_threshold)*512))\n",
    "                  new_y = y\n",
    "                  new_polygon.append([new_x, new_y])\n",
    "        if len(new_polygon) > 0:\n",
    "          combined_array_2.append(new_polygon)\n",
    "    \n",
    "    polygon_regions.append(combined_array_2)\n",
    "    \n",
    "    return polygon_regions\n",
    "\n",
    "def double_shift_polygon(seg_array_1, seg_array_2, seg_array_3, seg_array_4, shifting_threshold, shift_type, border_eps=1):\n",
    "  polygon_regions = []\n",
    "  num_polygons_1 = len(seg_array_1)\n",
    "  num_polygons_2 = len(seg_array_2)\n",
    "  num_polygons_3 = len(seg_array_3)\n",
    "  num_polygons_4 = len(seg_array_4)\n",
    "  \n",
    "  combined_array = []\n",
    "  for i in range(num_polygons_1):\n",
    "    polygon_1 = seg_array_1[i]\n",
    "    new_polygon = []\n",
    "    for point in polygon_1:\n",
    "        x, y = point\n",
    "        if shift_type == 'BR':\n",
    "          if x >= math.ceil(shifting_threshold*512) and y >= math.ceil(shifting_threshold*512):\n",
    "              new_x = x - int(math.ceil(shifting_threshold*512))\n",
    "              new_y = y - int(math.ceil(shifting_threshold*512))\n",
    "              new_polygon.append([new_x, new_y])\n",
    "        elif shift_type == 'BL':\n",
    "          if x <= math.floor((1-shifting_threshold)*512) and y >= math.ceil(shifting_threshold*512):\n",
    "            new_x = x + int(math.ceil(shifting_threshold*512))\n",
    "            new_y = y - int(math.ceil(shifting_threshold*512))\n",
    "            new_polygon.append([new_x, new_y])\n",
    "    if len(new_polygon) > 0:\n",
    "      combined_array.append(new_polygon)\n",
    "  \n",
    "  polygon_regions.append(combined_array)\n",
    "  \n",
    "  combined_array = []\n",
    "  for i in range(num_polygons_2):\n",
    "    polygon_2 = seg_array_2[i]\n",
    "    new_polygon = []\n",
    "    for point in polygon_2:\n",
    "        x, y = point\n",
    "        if shift_type == 'BR':\n",
    "          if x <= math.ceil(shifting_threshold*512) and y >= math.ceil(shifting_threshold*512):\n",
    "              new_x = x + int(math.floor((1-shifting_threshold)*512))\n",
    "              new_y = y - int(math.ceil(shifting_threshold*512))\n",
    "              new_polygon.append([new_x, new_y])\n",
    "        elif shift_type == 'BL':\n",
    "          if x >= math.floor((1-shifting_threshold)*512) and y >= math.ceil(shifting_threshold*512):\n",
    "            new_x = x - int(math.floor((1-shifting_threshold)*512))\n",
    "            new_y = y - int(math.ceil(shifting_threshold*512))\n",
    "            new_polygon.append([new_x, new_y])\n",
    "    if len(new_polygon) > 0:\n",
    "      combined_array.append(new_polygon)\n",
    "    \n",
    "  polygon_regions.append(combined_array)\n",
    "  \n",
    "  combined_array = []\n",
    "  for i in range(num_polygons_3):\n",
    "    polygon_3 = seg_array_3[i]\n",
    "    new_polygon = []\n",
    "    for point in polygon_3:\n",
    "        x, y = point\n",
    "        if shift_type == 'BR':\n",
    "          if x >= math.ceil(shifting_threshold*512) and y <= math.ceil(shifting_threshold*512):\n",
    "            new_x = x - int(math.ceil(shifting_threshold*512))\n",
    "            new_y = y + int(math.floor((1-shifting_threshold)*512))\n",
    "            new_polygon.append([new_x, new_y])\n",
    "        elif shift_type == 'BL':\n",
    "          if x <= math.floor((1-shifting_threshold)*512) and y <= math.ceil(shifting_threshold*512):\n",
    "            new_x = x + int(math.ceil(shifting_threshold*512))\n",
    "            new_y = y + int(math.floor((1-shifting_threshold)*512))\n",
    "            new_polygon.append([new_x, new_y])\n",
    "    if len(new_polygon) > 0:\n",
    "      combined_array.append(new_polygon)\n",
    "  \n",
    "  polygon_regions.append(combined_array)\n",
    "  \n",
    "  combined_array = []\n",
    "  for i in range(num_polygons_4):\n",
    "    polygon_4 = seg_array_4[i]\n",
    "    new_polygon = []\n",
    "    for point in polygon_4:\n",
    "        x, y = point\n",
    "        if shift_type == 'BR':\n",
    "          if x <= math.ceil(shifting_threshold*512) and y <= math.ceil(shifting_threshold*512):\n",
    "            new_x = x + int(math.floor((1-shifting_threshold)*512))\n",
    "            new_y = y + int(math.floor((1-shifting_threshold)*512))\n",
    "            new_polygon.append([new_x, new_y])\n",
    "        elif shift_type == 'BL':\n",
    "          if x >= math.floor((1-shifting_threshold)*512) and y <= math.ceil(shifting_threshold*512):\n",
    "            new_x = x - int(math.floor((1-shifting_threshold)*512))\n",
    "            new_y = y + int(math.floor((1-shifting_threshold)*512))\n",
    "            new_polygon.append([new_x, new_y])\n",
    "    if len(new_polygon) > 0:\n",
    "      combined_array.append(new_polygon)\n",
    "  \n",
    "  polygon_regions.append(combined_array)\n",
    "  \n",
    "  return polygon_regions\n",
    "\n",
    "def check_polygon_border(pt, border_eps=1):\n",
    "  return pt[0] <= border_eps or pt[0] >= 512-border_eps or pt[1] <= border_eps or pt[1] >= 512-border_eps\n",
    "\n",
    "def single_shift_img(img_1, img_2, shifting_threshold, shift_type):\n",
    "    height, width, _ = img_1.shape\n",
    "    \n",
    "    if shift_type == 'R':\n",
    "      img_1_part = img_1[:, int(math.ceil(shifting_threshold*width)):, :]\n",
    "      img_2_part = img_2[:, :int(math.ceil(shifting_threshold*width)), :]\n",
    "      combined_image = cv2.hconcat([img_1_part, img_2_part])\n",
    "    elif shift_type == 'B':\n",
    "      img_1_part = img_1[int(math.ceil(shifting_threshold*height)):, :, :]\n",
    "      img_2_part = img_2[:int(math.ceil(shifting_threshold*height)), :, :]\n",
    "      combined_image = cv2.vconcat([img_1_part, img_2_part])\n",
    "    else:\n",
    "      img_1_part = img_1[:, :int(math.floor((1-shifting_threshold)*width)), :]\n",
    "      img_2_part = img_2[:, int(math.floor((1-shifting_threshold)*width)):, :]\n",
    "      combined_image = cv2.hconcat([img_2_part, img_1_part])\n",
    "    return combined_image\n",
    "\n",
    "def double_shift_img(img_1, img_2, img_3, img_4, shifting_threshold, shift_type):\n",
    "  height, width, _ = img_1.shape\n",
    "  if shift_type == 'BR':\n",
    "    img_1_part = img_1[int(math.ceil(shifting_threshold*height)):, int(math.ceil(shifting_threshold*width)):, :]\n",
    "    img_2_part = img_2[int(math.ceil(shifting_threshold*height)):, :int(math.ceil(shifting_threshold*width)), :]\n",
    "    img_3_part = img_3[:int(math.ceil(shifting_threshold*height)), int(math.ceil(shifting_threshold*width)):, :]\n",
    "    img_4_part = img_4[:int(math.ceil(shifting_threshold*height)), :int(math.ceil(shifting_threshold*width)), :]\n",
    "    image_top = cv2.hconcat([img_1_part, img_2_part])\n",
    "    image_bottom = cv2.hconcat([img_3_part, img_4_part])\n",
    "    combined_image = cv2.vconcat([image_top, image_bottom])\n",
    "  else:\n",
    "    img_1_part = img_1[int(math.ceil(shifting_threshold*height)):, :int(math.floor((1-shifting_threshold)*width)), :]\n",
    "    img_2_part = img_2[int(math.ceil(shifting_threshold*height)):, int(math.floor((1-shifting_threshold)*width)):, :]\n",
    "    img_3_part = img_3[:int(math.ceil(shifting_threshold*height)), :int(math.floor((1-shifting_threshold)*width)), :]\n",
    "    img_4_part = img_4[:int(math.ceil(shifting_threshold*height)), int(math.floor((1-shifting_threshold)*width)):, :]\n",
    "    image_top = cv2.hconcat([img_2_part, img_1_part])\n",
    "    image_bottom = cv2.hconcat([img_4_part, img_3_part])\n",
    "    combined_image = cv2.vconcat([image_top, image_bottom])\n",
    "  return combined_image\n",
    "\n",
    "def load_img(img_dir, img_id):\n",
    "  img = cv2.imread(f\"{img_dir}/{img_id}.png\")\n",
    "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "  return img\n",
    "\n",
    "def add_border(image, border_size, border_color):\n",
    "    height, width = image.shape[:2]\n",
    "    new_height = height + 2 * border_size\n",
    "    new_width = width + 2 * border_size\n",
    "    bordered_image = np.zeros((new_height, new_width, 3), dtype=np.uint8)\n",
    "    bordered_image[border_size:height + border_size, border_size:width + border_size] = image\n",
    "    cv2.rectangle(bordered_image, (0, 0), (new_width - 1, new_height - 1), border_color, border_size)\n",
    "    return bordered_image\n",
    "\n",
    "def create_coco_annots(all_coords, pair_format=True):\n",
    "  cur_tile_coco_annots = []\n",
    "  for coords in all_coords:\n",
    "#     print('======================')\n",
    "#     print(coords)\n",
    "#     print('======================')\n",
    "    if not pair_format:\n",
    "      coords = [[pt[0], pt[1]] for pt in zip(coords[::2], coords[1::2])]\n",
    "    segmentations = [[pt for pair in coords for pt in pair]]\n",
    "    segmentation_area = calculate_area(coords)\n",
    "    min_x = min(coords, key=lambda x: x[0])[0]\n",
    "    max_x = max(coords, key=lambda x: x[0])[0]\n",
    "    min_y = min(coords, key=lambda x: x[1])[1]\n",
    "    max_y = max(coords, key=lambda x: x[1])[1]\n",
    "    segmentation_bbox = [min_x, min_y, max_x-min_x, max_y-min_y]\n",
    "    category_id = 0\n",
    "    cur_tile_coco_annots.append({\n",
    "      'segmentation': segmentations,\n",
    "      'area': segmentation_area,\n",
    "      'bbox': segmentation_bbox,\n",
    "      'category_id': category_id\n",
    "    })\n",
    "  return cur_tile_coco_annots\n",
    "\n",
    "def save_merged_imgs(input_imgs_dir, output_imgs_dir, base_img_id, base_img_dict, shifting_thresholds):\n",
    "  img_arr_dict = {'base_img': load_img(input_imgs_dir, base_img_id)}\n",
    "  cv2.imwrite(f'{output_imgs_dir}/{base_img_id}.png', img_arr_dict['base_img'])\n",
    "  for shift_type in base_img_dict.keys():\n",
    "    if base_img_dict[shift_type] is not None:\n",
    "      img_arr_dict[shift_type] = load_img(input_imgs_dir, base_img_dict[shift_type])\n",
    "    else:\n",
    "      img_arr_dict[shift_type] = None\n",
    "  for shifting_threshold in shifting_thresholds:\n",
    "    for shift_type in img_arr_dict:\n",
    "      if img_arr_dict[shift_type] is not None:\n",
    "        if shift_type in ['R', 'B']:\n",
    "          shifted_img = single_shift_img(img_arr_dict['base_img'], img_arr_dict[shift_type], shifting_threshold, shift_type)\n",
    "          if shifted_img.shape[0] != 512 or shifted_img.shape[1] != 512 or shifted_img.shape[2] != 3:\n",
    "            print(f'Error!!! Got an odd combined shape: {shifted_img.shape} for image id: {base_img_id} and shift type: {shift_type} and shifting threshold: {shifting_threshold}')\n",
    "          cv2.imwrite(f'{output_imgs_dir}/{base_img_id}_{shift_type}_{shifting_threshold}.png', shifted_img)\n",
    "        elif shift_type == 'BR' and img_arr_dict['B'] is not None and img_arr_dict['R'] is not None:\n",
    "          shifted_img = double_shift_img(img_arr_dict['base_img'], img_arr_dict['R'], img_arr_dict['B'], img_arr_dict[shift_type], shifting_threshold, shift_type)\n",
    "          if shifted_img.shape[0] != 512 or shifted_img.shape[1] != 512 or shifted_img.shape[2] != 3:\n",
    "            print(f'Error!!! Got an odd combined shape: {shifted_img.shape} for image id: {base_img_id} and shift type: {shift_type}')\n",
    "          cv2.imwrite(f'{output_imgs_dir}/{base_img_id}_{shift_type}_{shifting_threshold}.png', shifted_img)\n",
    "        elif shift_type == 'BL' and img_arr_dict['B'] is not None and img_arr_dict['L'] is not None:\n",
    "          shifted_img = double_shift_img(img_arr_dict['base_img'], img_arr_dict['L'], img_arr_dict['B'], img_arr_dict[shift_type], shifting_threshold, shift_type)\n",
    "          if shifted_img.shape[0] != 512 or shifted_img.shape[1] != 512 or shifted_img.shape[2] != 3:\n",
    "            print(f'Error!!! Got an odd combined shape: {shifted_img.shape} for image id: {base_img_id} and shift type: {shift_type}')\n",
    "          cv2.imwrite(f'{output_imgs_dir}/{base_img_id}_{shift_type}_{shifting_threshold}.png', shifted_img)\n",
    "\n",
    "def save_merged_annotations(output_annots_dir, base_img_id, base_img_dict, shifting_thresholds, tgt_tile_dicts):\n",
    "  print(f'base image id: {base_img_id}')\n",
    "  cur_tile_annots = create_coco_annots(tgt_tile_dicts[base_img_id])\n",
    "  with open(f'{output_annots_dir}/{base_img_id}.pkl', 'wb') as f:\n",
    "    pickle.dump(cur_tile_annots, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "  for shifting_threshold in shifting_thresholds:\n",
    "    for shift_type in base_img_dict:\n",
    "      if base_img_dict[shift_type] is not None:\n",
    "        if shift_type in ['R', 'B']:\n",
    "          polygon_regions = single_shift_polygon(tgt_tile_dicts[base_img_id], tgt_tile_dicts[base_img_dict[shift_type]], shifting_threshold, shift_type)\n",
    "#           print(f'===========after single shift - {len(polygon_regions)}============')\n",
    "#           print(polygon_regions)\n",
    "#           print('===========after single shift============')\n",
    "          shifted_coords = get_polygon_merged_coords(polygon_regions)\n",
    "#           print(f'===========after polyon merge - {len(shifted_coords)}============')\n",
    "#           print(shifted_coords)\n",
    "#           print('===========after polyon merge============')\n",
    "#           print(f'===========merged coords shift type R B===========')\n",
    "#           print(shifted_coords)\n",
    "#           print(f'===========merged coords shift type R B===========')\n",
    "          shifted_annots = create_coco_annots(shifted_coords, pair_format=False)\n",
    "#           print(f'===========after coco - {len(shifted_coords)}============')\n",
    "#           print(shifted_annots)\n",
    "#           print('===========after coco============')\n",
    "          with open(f'{output_annots_dir}/{base_img_id}_{shift_type}_{shifting_threshold}.pkl', 'wb') as f:\n",
    "            pickle.dump(shifted_annots, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        elif shift_type == 'BR' and base_img_dict['B'] is not None and base_img_dict['R'] is not None:\n",
    "          polygon_regions = double_shift_polygon(tgt_tile_dicts[base_img_id], tgt_tile_dicts[base_img_dict['R']], tgt_tile_dicts[base_img_dict['B']], tgt_tile_dicts[base_img_dict[shift_type]], shifting_threshold, shift_type)\n",
    "          shifted_coords = get_polygon_merged_coords(polygon_regions)\n",
    "#           print(f'===========merged coords shift type BR===========')\n",
    "#           print(shifted_coords)\n",
    "#           print(f'===========merged coords shift type BR===========')\n",
    "          shifted_annots = create_coco_annots(shifted_coords, pair_format=False)\n",
    "          with open(f'{output_annots_dir}/{base_img_id}_{shift_type}_{shifting_threshold}.pkl', 'wb') as f:\n",
    "            pickle.dump(shifted_annots, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        elif shift_type == 'BL' and base_img_dict['B'] is not None and base_img_dict['L'] is not None:\n",
    "          polygon_regions = double_shift_polygon(tgt_tile_dicts[base_img_id], tgt_tile_dicts[base_img_dict['L']], tgt_tile_dicts[base_img_dict['B']], tgt_tile_dicts[base_img_dict[shift_type]], shifting_threshold, shift_type)\n",
    "          shifted_coords = get_polygon_merged_coords(polygon_regions)\n",
    "#           print(f'===========merged coords shift type BL===========')\n",
    "#           print(shifted_coords)\n",
    "#           print(f'===========merged coords shift type BL===========')\n",
    "          shifted_annots = create_coco_annots(shifted_coords, pair_format=False)\n",
    "          with open(f'{output_annots_dir}/{base_img_id}_{shift_type}_{shifting_threshold}.pkl', 'wb') as f:\n",
    "            pickle.dump(shifted_annots, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def get_annotated_img(imgs_dir, annots_dir, base_img_id):\n",
    "  eps = 1\n",
    "  annotated_image_coco = load_img(imgs_dir, base_img_id)\n",
    "  img_height, img_width, _ = annotated_image_coco.shape\n",
    "  with open(f'{annots_dir}/{base_img_id}.pkl', 'rb') as f:\n",
    "    tgt_annots = pickle.load(f)\n",
    "  for tgt_annot in tgt_annots:\n",
    "    coords = [[x, y] for x, y in zip(tgt_annot['segmentation'][0][::2], tgt_annot['segmentation'][0][1::2])]\n",
    "    min_x, min_y, width, height = tgt_annot['bbox']\n",
    "    cv2.fillPoly(annotated_image_coco, pts=[np.array(coords)], color=(0,255,0))\n",
    "    top_left = (int(tgt_annot['bbox'][0]), int(tgt_annot['bbox'][1]))\n",
    "    min_x = min(coords, key=lambda x: x[0])[0]\n",
    "    min_y = min(coords, key=lambda x: x[1])[1]\n",
    "    max_x = max(coords, key=lambda x: x[0])[0]\n",
    "    max_y = max(coords, key=lambda x: x[1])[1]\n",
    "    bottom_right = (int(tgt_annot['bbox'][0]+tgt_annot['bbox'][2]), int(tgt_annot['bbox'][1]+tgt_annot['bbox'][3]))\n",
    "    print(max_x-min_x, max_y-min_y, tgt_annot['bbox'])\n",
    "#     if min_x <= eps or max_x >= img_width-eps or min_y <= eps or max_y >= img_height-eps:\n",
    "#       annotated_image_coco = cv2.rectangle(annotated_image_coco, top_left, bottom_right, (255,0,0), 2)\n",
    "    annotated_image_coco = cv2.rectangle(annotated_image_coco, top_left, bottom_right, (255,0,0), 2)\n",
    "  annotated_image_coco = add_border(annotated_image_coco, 5, (0,0,255))\n",
    "  return annotated_image_coco\n",
    "\n",
    "def get_polygon_merged_annotated_img(imgs_dir, annots_dir, base_img_id):\n",
    "  eps = 1\n",
    "  annotated_image_coco = load_img(imgs_dir, base_img_id)\n",
    "  img_height, img_width, _ = annotated_image_coco.shape\n",
    "  annotated_image_mask = np.zeros((img_height,img_width))\n",
    "  with open(f'{annots_dir}/{base_img_id}.pkl', 'rb') as f:\n",
    "    tgt_annots = pickle.load(f)\n",
    "  all_polygons = [x for tgt_annot in tgt_annots for x in tgt_annot['segmentation']]\n",
    "#   all_polygons = merge_connected_polygons(all_polygons, 1)\n",
    "  for polygon in all_polygons:\n",
    "    coords = [[x, y] for x, y in zip(polygon[::2], polygon[1::2])]\n",
    "    cv2.fillPoly(annotated_image_mask, pts=[np.array(coords)], color=255)\n",
    "  merged_polygons = get_segmentation_coordinates(annotated_image_mask)\n",
    "  for merged_polygon in merged_polygons:\n",
    "    coords = [[x, y] for x, y in zip(merged_polygon[::2], merged_polygon[1::2])]\n",
    "    cv2.fillPoly(annotated_image_coco, pts=[np.array(coords)], color=(0,255,0))\n",
    "    min_x = min(coords, key=lambda x: x[0])[0]\n",
    "    min_y = min(coords, key=lambda x: x[1])[1]\n",
    "    max_x = max(coords, key=lambda x: x[0])[0]\n",
    "    max_y = max(coords, key=lambda x: x[1])[1]\n",
    "    top_left = (int(min_x), int(min_y))\n",
    "    bottom_right = (int(max_x), int(max_y))\n",
    "#     if min_x <= eps or max_x >= img_width-eps or min_y <= eps or max_y >= img_height-eps:\n",
    "#       annotated_image_coco = cv2.rectangle(annotated_image_coco, top_left, bottom_right, (255,0,0), 2)\n",
    "    annotated_image_coco = cv2.rectangle(annotated_image_coco, top_left, bottom_right, (255,0,0), 2)\n",
    "  annotated_image_coco = add_border(annotated_image_coco, 5, (0,0,255))\n",
    "  return annotated_image_coco\n",
    "\n",
    "def get_polygon_merged_coords(polygons_regions, epsilon=1):\n",
    "  merged_polygons = group_merged_polygons(polygons_regions, epsilon=epsilon)\n",
    "  final_segmentation_coords = []\n",
    "  \n",
    "  for merged_polygon in merged_polygons:\n",
    "    annotated_image_mask = np.zeros((512,512))\n",
    "    for coords in merged_polygon:\n",
    "      cv2.fillPoly(annotated_image_mask, pts=[np.array(coords)], color=255)\n",
    "#     plt.imshow(annotated_image_mask)\n",
    "#     plt.show()\n",
    "    final_segmentation_coords += get_segmentation_coordinates(annotated_image_mask)\n",
    "  \n",
    "  return final_segmentation_coords\n",
    "  \n",
    "import math\n",
    "\n",
    "def group_merged_polygons(regions, epsilon):\n",
    "    merged_groups = []\n",
    "    \n",
    "    def merge_polygons(poly1, poly2):\n",
    "        return poly1 + poly2\n",
    "    \n",
    "    def distance(point1, point2):\n",
    "        return math.sqrt((point2[0] - point1[0]) ** 2 + (point2[1] - point1[1]) ** 2)\n",
    "    \n",
    "    def are_polygons_close(poly1, poly2):\n",
    "        for p1 in poly1:\n",
    "            for p2 in poly2:\n",
    "                if distance(p1, p2) <= epsilon:\n",
    "                    return True\n",
    "        return False\n",
    "    \n",
    "    for region_index, region in enumerate(regions):\n",
    "        for polygon in region:\n",
    "            merged = False\n",
    "            for group in merged_groups:\n",
    "                if (not any(region_index == group_region_idx for group_region_idx, _ in group)) and any(are_polygons_close(polygon, poly) for _, poly in group):\n",
    "                    group.append([region_index, polygon])\n",
    "                    merged = True\n",
    "                    break\n",
    "            if not merged:\n",
    "                merged_groups.append([[region_index, polygon]])\n",
    "    \n",
    "    merged_polygons = []\n",
    "    for group in merged_groups:\n",
    "        polygon_group = []\n",
    "        for _, polygon in group:\n",
    "          polygon_group.append(polygon)\n",
    "        merged_polygons.append(polygon_group)\n",
    "    return merged_polygons\n",
    "\n",
    "\n",
    "def get_segmentation_coordinates(mask):\n",
    "    mask = np.uint8(mask)\n",
    "    \n",
    "    # Find contours in the binary mask\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Extract segmentation coordinates from the contours\n",
    "    segmentation_coords = []\n",
    "    for contour in contours:\n",
    "        contour = np.squeeze(contour)\n",
    "        coords = contour.flatten().tolist()\n",
    "        segmentation_coords.append(coords)\n",
    "\n",
    "    return segmentation_coords\n",
    "\n",
    "def save_merged_img_id_with_annot(output_imgs_dir, output_annots_dir, vis_output_imgs_dir, base_img_id, base_img_dict, shifting_thresholds):\n",
    "  img_arr_dict = {'base_image': get_annotated_img(output_imgs_dir, output_annots_dir, base_img_id)}\n",
    "  for shift_type in base_img_dict.keys():\n",
    "    if base_img_dict[shift_type] is not None:\n",
    "      img_arr_dict[shift_type] = get_annotated_img(output_imgs_dir, output_annots_dir, base_img_dict[shift_type])\n",
    "    else:\n",
    "      img_arr_dict[shift_type] = add_border((np.zeros((512, 512, 3), dtype=\"uint8\")), 5, (0,0,255))\n",
    "  image_top = cv2.hconcat([img_arr_dict['L'], img_arr_dict['base_image'], img_arr_dict['R']])\n",
    "  print(img_arr_dict['BL'].shape, img_arr_dict['B'].shape, img_arr_dict['BR'].shape)\n",
    "  image_bottom = cv2.hconcat([img_arr_dict['BL'], img_arr_dict['B'], img_arr_dict['BR']])\n",
    "  image_full = cv2.vconcat([image_top, image_bottom])\n",
    "  cv2.imwrite(f'{vis_output_imgs_dir}/{base_img_id}_full_annotated.png', image_full)\n",
    "  all_merged_img_ids = [x.split('.png')[0] for x in os.listdir(output_imgs_dir) if base_img_id in x and x != f'{base_img_id}.png']\n",
    "  for merged_img_id in all_merged_img_ids:\n",
    "    merged_annotated_img = get_annotated_img(output_imgs_dir, output_annots_dir, merged_img_id)\n",
    "    cv2.imwrite(f'{vis_output_imgs_dir}/{merged_img_id}_full_annotated.png', merged_annotated_img)\n",
    "#     polygon_merged_annotated_img = get_polygon_merged_annotated_img(output_imgs_dir, output_annots_dir, merged_img_id)\n",
    "#     cv2.imwrite(f'{vis_output_imgs_dir}/{merged_img_id}_full_annotated_polygon_merged.png', polygon_merged_annotated_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77980b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# random.seed(42)\n",
    "# all_dataset_tile_ids = sorted(list(tile_df_annotated.loc[:, 'id'].values))\n",
    "# dataset_1 = tile_df_annotated.loc[tile_df_annotated['dataset']==1]\n",
    "# dataset_1_tile_ids = sorted(list(dataset_1.loc[:, 'id'].values))\n",
    "# wsi_1_ids = list(dataset_1.loc[dataset_1['source_wsi']==1, 'id'])\n",
    "# wsi_2_ids = list(dataset_1.loc[dataset_1['source_wsi']==2, 'id'])\n",
    "# tgt_tile_dicts = [x for x in tiles_dicts if x['id'] in dataset_1_tile_ids]\n",
    "# tgt_tile_dicts = get_relevant_annots(tgt_tile_dicts)\n",
    "# print(len(all_dataset_tile_ids), len(dataset_1_tile_ids))\n",
    "# random.shuffle(dataset_1_tile_ids)\n",
    "# random.shuffle(wsi_1_ids)\n",
    "# random.shuffle(wsi_2_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db2eff8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "dataset_1_tile_ids = sorted(list(tile_df_annotated.loc[tile_df_annotated['dataset']==1, 'id'].values))\n",
    "all_dataset_tile_ids = sorted(list(tile_df_annotated.loc[:, 'id'].values))\n",
    "tgt_wsis = [1,2,3,4]\n",
    "wsi_tile_ids = []\n",
    "for tgt_wsi in tgt_wsis:\n",
    "    if generate_all_datset_annots:\n",
    "        wsi_ids = list(tile_df_annotated.loc[tile_df_annotated['source_wsi']==tgt_wsi, 'id'].values)\n",
    "    else:\n",
    "        wsi_ids = list(tile_df_annotated.loc[(tile_df_annotated['dataset']==1) & (tile_df_annotated['source_wsi']==tgt_wsi), 'id'].values)\n",
    "    random.shuffle(wsi_ids)\n",
    "    wsi_tile_ids.append(wsi_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c4f674b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cfb0bd617652',\n",
       " '678c51bde237',\n",
       " '2519a0f1b867',\n",
       " '523a3912a7ee',\n",
       " '9a70c2d9e4f4',\n",
       " 'ca081412bad8',\n",
       " 'd9547a630fab',\n",
       " '5384b927f5b3',\n",
       " '2d2093e38e39',\n",
       " '3e248be7dd4c',\n",
       " '39b8aafd630b',\n",
       " '578c8b32057e',\n",
       " '4c058755d597',\n",
       " '23c30a541c7a',\n",
       " '9f999c74b67e',\n",
       " '0e8aed930dc6',\n",
       " '03d335057db3',\n",
       " '328a651a4ca7',\n",
       " '55b07829e888',\n",
       " '4cf2563142f4',\n",
       " '6d1315ab9ef7',\n",
       " '7483bb77d145',\n",
       " 'e506a133c873',\n",
       " '18be061202ea',\n",
       " 'f1af97caff9d',\n",
       " 'db8adbc51c82',\n",
       " '13aa34ced90d',\n",
       " '8f6f3e18eda5',\n",
       " '033a656390b2',\n",
       " '5ed62676da45',\n",
       " '76e5b8a1caa1',\n",
       " '415a900f7a89',\n",
       " '9ed941a933d0',\n",
       " 'ac68833641fe',\n",
       " '1c0295c1d01d',\n",
       " '4a1b99c7ab16',\n",
       " 'f6b0c55c34c7',\n",
       " '15ed954c0cdf',\n",
       " '0a993633aa5e',\n",
       " 'd6529e6e8602',\n",
       " '242db333c452',\n",
       " '8e90e6189c6b',\n",
       " '783e8f735476',\n",
       " '4b197545ea0e',\n",
       " '45d6f53ececf',\n",
       " '5a89c7f7ad05',\n",
       " '9b95807e6b23',\n",
       " '30ad648e36a2',\n",
       " 'cb96f7b681f8',\n",
       " '4e8b6849684d',\n",
       " '303b72a38e29',\n",
       " '41225437e33e',\n",
       " 'ac8f685801d0',\n",
       " 'abbacf024cb5',\n",
       " '95c077870f91',\n",
       " 'dcea4e9d6726',\n",
       " '7392e21e0fe2',\n",
       " 'e1647f94f1f9',\n",
       " '1a54cda8f32d',\n",
       " '1f0d3cd4b621',\n",
       " 'f40804ea84e6',\n",
       " '664fdc3e5dca',\n",
       " '88d349bebe86',\n",
       " 'da2863316247',\n",
       " 'e5ac1caaadd8',\n",
       " '56b724dc963a',\n",
       " 'aad30f930d6d',\n",
       " '8ebea8a23178',\n",
       " '01a7fca6263b',\n",
       " '43b4654b4c3d',\n",
       " 'dfcea0f4ef63',\n",
       " '8047c43b1cb2',\n",
       " '733296a59c9b',\n",
       " '951d7a241936',\n",
       " 'b454fe1b98f8',\n",
       " '62bc683d8c8a',\n",
       " '6443bced36cd',\n",
       " 'bf610d0dd510',\n",
       " '31195284c86e',\n",
       " '8a100f8a780f',\n",
       " '611aee20c207',\n",
       " 'afe3a6d7f7ba',\n",
       " 'ac6d05ce6dd1',\n",
       " 'db20b95a3ab9',\n",
       " 'aa5761ee8eda',\n",
       " '2fd7649afbc1',\n",
       " '06034408218a',\n",
       " '02f563532696',\n",
       " 'bd4bbe103c24',\n",
       " '3c72922e02f4',\n",
       " '9596b376c210',\n",
       " 'bd090bb1b654',\n",
       " 'd6f0f1f4a7dc',\n",
       " 'cd6b90e3f4b3',\n",
       " '60649906e443',\n",
       " '99ddcf7fb9a3',\n",
       " 'a7a03dcb5485',\n",
       " '0ce440ad7080',\n",
       " '26b7c381eb73',\n",
       " 'cc103e610139',\n",
       " '8a2b71c9531a',\n",
       " '6e74e333e5e4',\n",
       " 'f4c6166d1039',\n",
       " '30eb9abf4a94',\n",
       " '6637388750f9',\n",
       " '59c99a7cd635',\n",
       " '836f728b5c18',\n",
       " '7cb715d5413c',\n",
       " '7d0038b7076a',\n",
       " '94ad7d9e5497',\n",
       " '7f41aff561f7',\n",
       " 'd92dc34cd8f2',\n",
       " 'c51d8cf96223',\n",
       " '2bf7751456cd',\n",
       " 'bf96591278cd',\n",
       " '3d5f53402137',\n",
       " 'cbc4758c8c53',\n",
       " 'c2569a219a0b',\n",
       " 'a44370ab145d',\n",
       " '13e70093a1f8',\n",
       " '81d7080ee1f1',\n",
       " '23c98917da69',\n",
       " '33d56ea8889f',\n",
       " 'b50d9b7c01c4',\n",
       " '4a8a791a23cb',\n",
       " '94dd41f92b1d',\n",
       " '6932d1e4a62b',\n",
       " '209426451a36',\n",
       " '4eb2a2b392f6',\n",
       " '2d882504c2ea',\n",
       " 'd171145b9fb7',\n",
       " '495169bbaa46',\n",
       " '3ca3b1b0db63',\n",
       " 'bffe164b7f65',\n",
       " '56102e85ff9f',\n",
       " 'ef7f5d12b1ff',\n",
       " '6b1e4cea2071',\n",
       " '00d75ad65de3',\n",
       " '093a52e9bc1d',\n",
       " '458a92eda7b4',\n",
       " '8a60272c70aa',\n",
       " '25b98e7aea78',\n",
       " 'c4fd3d068713',\n",
       " 'a4b58c23d4c6',\n",
       " 'fa10ae8f9fec',\n",
       " 'f0c2afc5fb05',\n",
       " '4b94404d2e6c',\n",
       " 'f993befe3cb4',\n",
       " '228da07bf8d9',\n",
       " '55f4ccb3f710',\n",
       " '3f4f881abe88',\n",
       " '44b8b0eaa3b2',\n",
       " 'c15c8d3980fb',\n",
       " 'dd690b7d9a47',\n",
       " 'adadaeaa3635',\n",
       " 'd873758275c6',\n",
       " '4eef9950c417',\n",
       " '5631a47d5b0c',\n",
       " 'bb488d9b931e',\n",
       " '454cf72a9ed9',\n",
       " '8a6e6fb756da',\n",
       " 'dc0b64e2a3a6',\n",
       " '65fcbb5ef4c7',\n",
       " '7c0c2bdb3873',\n",
       " 'a5ab2872a910',\n",
       " '089a9e6be240',\n",
       " 'e2924beb5567',\n",
       " '0a10b8716b30',\n",
       " '3cf35a0393b8',\n",
       " 'e0ea1978d5dc',\n",
       " 'eda294658a0a',\n",
       " '936f9bfb3966',\n",
       " 'ace43628b108',\n",
       " 'ae4641d3fb5f',\n",
       " '481234519879',\n",
       " '6a4d868ab872',\n",
       " 'eba1f93e5a11',\n",
       " 'cf5f9ffd9957',\n",
       " '6230a948334b',\n",
       " '68c4d1b29b13',\n",
       " 'd6d79e965902',\n",
       " '0788fc3be62e',\n",
       " '1c8af4861691',\n",
       " '257c077f001a',\n",
       " '740afe5498ac',\n",
       " '15262223b967',\n",
       " 'a9de6e1a6c4d',\n",
       " 'e176191e4f83',\n",
       " '892146ef3207',\n",
       " '88c95fb9fb14',\n",
       " '59642acc100e',\n",
       " '5608b200acb6',\n",
       " '720c0fa5ce8e',\n",
       " 'cd39806070ab',\n",
       " 'd850250778f2',\n",
       " 'f45a29109ff5',\n",
       " '326c9663d95d',\n",
       " 'a0fed17a5bc6',\n",
       " '314a25a7f92d',\n",
       " '92f12671d129',\n",
       " 'cbbdc5d83ec9',\n",
       " '91858313a40e',\n",
       " 'a6526eeecee5',\n",
       " '9a4d59b6c3ec',\n",
       " 'ca57af0413ac',\n",
       " '8533ba9bc160',\n",
       " '16aff0aaa514',\n",
       " '1fc1c6cc515a',\n",
       " '5c48013d4c6d',\n",
       " 'd8f0d57073c8',\n",
       " 'a1e821ead370',\n",
       " 'cb0d9bad1bd4',\n",
       " 'c96f8a2737a2',\n",
       " 'fce9ad75428d',\n",
       " '7b7327ccac8d',\n",
       " 'fc6def641612',\n",
       " '8565f71a4149',\n",
       " '359bb86fa14f',\n",
       " '09e1916da449',\n",
       " 'eef5565a7e5a',\n",
       " '3901bf683fc0',\n",
       " 'be9250fc8ec5',\n",
       " 'e7dd45905a5f',\n",
       " '8182a03edd17',\n",
       " 'bb1587775541',\n",
       " 'fb833bb39c55',\n",
       " 'c50f59592766',\n",
       " 'b6395d5c113f',\n",
       " 'e1f6c8a7873e',\n",
       " 'dbf5ec194977',\n",
       " '0d6c0cfee2db',\n",
       " '70d79d2694c0',\n",
       " '91101158fefd',\n",
       " '2c369bf03bd4',\n",
       " 'fab2054e45f5',\n",
       " '333c1d618ae3',\n",
       " 'f53f64d220e3',\n",
       " '0c54942878fa',\n",
       " 'a60200ee318c',\n",
       " '50c80988e3ff',\n",
       " 'e4c6313913c3',\n",
       " 'f406fdba08f4',\n",
       " 'd1d485660263',\n",
       " '71bf4443c163',\n",
       " 'd68324eb7980',\n",
       " '3398c337fe7a',\n",
       " '92450a58e849',\n",
       " '2dbb4700f7be',\n",
       " 'e52522470fea',\n",
       " '51551e6feeb1',\n",
       " '8491370e454b',\n",
       " 'b5f0453b4c6d',\n",
       " '19c8864ac6a0',\n",
       " '3382edde0a6c',\n",
       " '65a235166914',\n",
       " '8758e0b50301',\n",
       " '3c18fbcff68b',\n",
       " '7db4a2106f14',\n",
       " '2321d0fb53f7',\n",
       " '34bfda589476',\n",
       " 'f217dcd37517',\n",
       " 'd4e99e162f37',\n",
       " '3b523d3117e2',\n",
       " '88e8fbbc2b4e',\n",
       " 'f6afc2903670',\n",
       " '51bcce4e7c55',\n",
       " 'eaa096d85a52',\n",
       " '1222b4306c01',\n",
       " 'cd4b698ac682',\n",
       " '0e0836cf1824',\n",
       " 'cee9e898e32c',\n",
       " 'eaa85fa2b857',\n",
       " 'b987af6e6106',\n",
       " 'c8e71ace0b11',\n",
       " '0596bfb19322',\n",
       " '5566406c59ee',\n",
       " 'b29ab50ca7aa',\n",
       " '37cac808f375',\n",
       " 'bda6c8ca9d47',\n",
       " '20113c729fe4',\n",
       " '77c7c3714f08',\n",
       " '891613413e69',\n",
       " '7292fe2d34d6',\n",
       " 'a3ebac2db19d',\n",
       " 'ba5cffc61dce',\n",
       " '4c1361e4c2d8',\n",
       " '32d912178fdd',\n",
       " 'a29f889679f4',\n",
       " 'f9faf12fc07b',\n",
       " 'c9d0ac176add',\n",
       " 'e95c2881d1dc',\n",
       " 'bef001de468a',\n",
       " '2af27d65509c',\n",
       " '90481ae2a0c9',\n",
       " '7e72cf2b43bf',\n",
       " '21acb908cf67',\n",
       " '214866956ea3',\n",
       " '8b33e8e0f0a1',\n",
       " 'd8f61858d47a',\n",
       " '81e49a2e8c6b',\n",
       " '189064c6a137',\n",
       " 'da9507215057',\n",
       " 'c3e34170ff28',\n",
       " '3b8d1919d08e',\n",
       " '0dce2b8d2c25',\n",
       " '3b83d7668181',\n",
       " '4ba838cffa29',\n",
       " 'ba276097772d',\n",
       " 'aa89285a0dbd',\n",
       " '04c08080b2ae',\n",
       " '83accb438714',\n",
       " '501398697fc1',\n",
       " '9b9349a10d8d',\n",
       " '0033bbc76b6b',\n",
       " '8b29a4cb940e',\n",
       " '8e61e30e0090',\n",
       " '2a577425c773',\n",
       " '5ac25a1e40dd',\n",
       " 'b4a153ea1fc8',\n",
       " 'f21ab98d3572',\n",
       " 'cee03f90d9c9',\n",
       " 'ce27b0613895',\n",
       " '4995f7836ec4',\n",
       " 'a4b63d91f173',\n",
       " '1c6c39a22324',\n",
       " '881186005e82',\n",
       " '2e0c92f0c9df',\n",
       " '861ffbc05b45',\n",
       " 'c2731b6c0c17',\n",
       " '00656c6f2690',\n",
       " 'd4a6a9e8d85e',\n",
       " '27ac6c438f3b',\n",
       " '71cd41c93924',\n",
       " 'db03a1487798',\n",
       " 'fd2437954fd8',\n",
       " 'e6d9bd78b840',\n",
       " 'a9653f16e0b6',\n",
       " 'cfbf75947d43',\n",
       " 'dc03d69afd95',\n",
       " '1e14b9acc9b9',\n",
       " '025ec20b8b73',\n",
       " '910b93e1cf28',\n",
       " '3c9609fefe07',\n",
       " '8abfc630ccdd',\n",
       " '7c58b92324f7',\n",
       " 'cf921187e2bc',\n",
       " '5ed19b1b6f88',\n",
       " '5f9afd2ee26d',\n",
       " '0f6e1cb07f74',\n",
       " '9afac4fc65ef',\n",
       " 'e72945522a29',\n",
       " 'b12239b2762f',\n",
       " 'e8a183610034',\n",
       " 'da876b636c6c',\n",
       " '270f0c7b0901',\n",
       " '1d8aa548c370',\n",
       " '0bc3e1a729e4',\n",
       " '192846d46db7',\n",
       " '84a3f37bf808',\n",
       " '88614cbaa57c',\n",
       " '23d7fcc83332',\n",
       " 'c9dc3cfc0567',\n",
       " 'da1c6d6657e9',\n",
       " '644a62eaaf2b',\n",
       " '97551fbdf2bf',\n",
       " '6f6d94a82c86',\n",
       " '98afebe7cca1',\n",
       " '3fa3016e2bc5',\n",
       " '8ca6628424da',\n",
       " 'f24e15fe294b',\n",
       " '3c3796ee0923',\n",
       " 'faba1bf818ae',\n",
       " 'deb8c17483c0',\n",
       " '244711bad1bd',\n",
       " '7aa96f6832b9',\n",
       " 'a977353c25d6',\n",
       " '62cb0165db74',\n",
       " '85f5c8b8e408',\n",
       " 'aa989f83a794',\n",
       " 'c4dbb5659b69',\n",
       " 'c79b1b1886d4',\n",
       " 'bc9d7d15a9a9',\n",
       " '932c4fa02fee',\n",
       " '349a6a8fc66e',\n",
       " '10162104cbaf',\n",
       " '40ac7a9fc203',\n",
       " '64a7e1b01c32',\n",
       " '4e455f0cb054',\n",
       " 'f592b552a1af',\n",
       " '38ff92c9ae72',\n",
       " '0d9d65340ef8',\n",
       " '5047c7523e09',\n",
       " 'b20773efe051',\n",
       " 'ef5523d50b5b',\n",
       " '91170b600f70',\n",
       " 'b36f28986045',\n",
       " 'fe248458ea89',\n",
       " '41146970649b',\n",
       " 'd59914131ffa',\n",
       " '7b521f53df5b',\n",
       " 'd17335683f96',\n",
       " 'fbe845f13c20',\n",
       " 'eb7564c1ca68',\n",
       " '8be88b402878',\n",
       " '2d0c1b760913',\n",
       " 'd430a5d229e3',\n",
       " '9f62eba82f40',\n",
       " '130d4d323ce4',\n",
       " 'a9f8aa3cccf0',\n",
       " 'b1374d055915',\n",
       " 'f10a389fd4b0',\n",
       " '409a7a2dafd0',\n",
       " 'ada5d6fa80aa',\n",
       " '340bbab914db',\n",
       " '564283be4af8',\n",
       " '5a22624135ab',\n",
       " '2ac4cfbb7405',\n",
       " '588bd0b6ac0c',\n",
       " 'a661cbfc8464',\n",
       " '780362d8c87e',\n",
       " 'e63d156bc72f',\n",
       " 'e283ea6e3f7b',\n",
       " '1a785cc0d167',\n",
       " 'f48f6580655c',\n",
       " 'db8431a57b16',\n",
       " '462d5da043a5',\n",
       " 'ca178713fdb2',\n",
       " '3857131da21c',\n",
       " 'abe1088b3890',\n",
       " '0ba172f33ea6',\n",
       " '12ab9ac0fc55',\n",
       " 'b815cc48bb76',\n",
       " '31672f6050b5',\n",
       " '9546e3a4b2a9',\n",
       " '5871b8dc3e88',\n",
       " 'a34d3fde892c',\n",
       " 'a4f027cdc65e',\n",
       " 'd6bd77533b2b',\n",
       " '484301f61207',\n",
       " '9050af994abf',\n",
       " '15e5df255b86',\n",
       " '5d0731972927',\n",
       " '2136df4a5aeb',\n",
       " '8c66c89b3e78',\n",
       " '7ae0eb4bda2c',\n",
       " 'bf0f8e9bf257',\n",
       " '0b8029db1fb4',\n",
       " 'd078cb53bbd5',\n",
       " '4021872b194c',\n",
       " '9d8e026859d7',\n",
       " '55eef661b175',\n",
       " 'd9808586aef2',\n",
       " '573aeb182976',\n",
       " '1a0daccec1db',\n",
       " '5e72a0265b2f',\n",
       " 'f5140e2c6baf',\n",
       " 'fa6a66f01888',\n",
       " '54b61f30d4ec',\n",
       " 'c9455a06321d',\n",
       " '352777709276',\n",
       " '2798a3698885',\n",
       " 'ed6a92a9410c',\n",
       " '557efd4b1180',\n",
       " 'f6906101ee0e',\n",
       " 'b63650c923a7',\n",
       " '29d2f472e46d',\n",
       " 'cfe02239d1c7',\n",
       " 'c7d49f3f1ae3',\n",
       " '00da70813521',\n",
       " 'dcea2cfbc429',\n",
       " 'd169174a769b',\n",
       " '4399be208888',\n",
       " '9a2004e16035',\n",
       " '758c0b61f9db',\n",
       " '3576fc5288a3',\n",
       " '6b0fd03dfafe',\n",
       " 'f8cff6556e61',\n",
       " 'b73efafa7477',\n",
       " 'aa05d1efb6bc',\n",
       " 'bc995681dd54',\n",
       " '329d9bc6f0f8',\n",
       " '9227f6ede08f',\n",
       " '072f5307f243',\n",
       " '9cbdd3eb5929',\n",
       " '87a07ec41751',\n",
       " '391e9699357a',\n",
       " '352c7fdfb717',\n",
       " '19551df15a1e',\n",
       " '07b809228ef4',\n",
       " '0828cc1c22be',\n",
       " '6bfe9903a1cf',\n",
       " '9a4ac92edd79',\n",
       " '1893f913d64b',\n",
       " '8f256d18b5e4',\n",
       " 'e2c1ebfddb4a',\n",
       " 'fcbe5ce12d5f',\n",
       " 'af587261a215',\n",
       " '1bd726055ad8',\n",
       " 'c12e5e644392',\n",
       " '2405b58d5b61',\n",
       " '36be7dacbabf',\n",
       " '3c232223b2d0',\n",
       " '41493100d434',\n",
       " 'c1a16cb3923b',\n",
       " '0672b96aaccc',\n",
       " '1e0efbb8979f',\n",
       " 'a8dcf7824e9b']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wsi_tile_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0a3431",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_dataset_tile_ids), len(wsi_tile_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde38677",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_imgs_dir, input_annots_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f098a03a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9c5cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_size = int(math.ceil(len(dataset_1_tile_ids) / num_folds))\n",
    "print(f'Fold size is {fold_size}')\n",
    "shifting_thresholds = [0.15,0.30,0.45,0.60,0.75,0.90]\n",
    "for i in range(1):\n",
    "  validation_imgs_dir = f'{base_path}/{base_data_dir}/{base_data_name}_imgs_merged_validation_{i}' if use_merged_dataset else f'{base_path}/{base_data_dir}/{base_data_name}_imgs_validation_{i}'\n",
    "  validation_annots_dir = f'{base_path}/{base_data_dir}/{base_data_name}_annotations_merged_validation_{i}' if use_merged_dataset else f'{base_path}/{base_data_dir}/{base_data_name}_annotations_validation_{i}'\n",
    "  train_imgs_dir = f'{base_path}/{base_data_dir}/{base_data_name}_imgs_merged_train_{i}' if use_merged_dataset else f'{base_path}/{base_data_dir}/{base_data_name}_imgs_train_{i}'\n",
    "  train_annots_dir = f'{base_path}/{base_data_dir}/{base_data_name}_annotations_merged_train_{i}' if use_merged_dataset else f'{base_path}/{base_data_dir}/{base_data_name}_annotations_train_{i}'\n",
    "  \n",
    "  if pick_validation_from_file:\n",
    "    with open(f'{base_path}/dataset1_files/validation_wsi_1_fold_img_ids.pkl', 'rb') as f:\n",
    "        wsi_1_validation_img_ids = pickle.load(f)\n",
    "    with open(f'{base_path}/dataset1_files/validation_wsi_2_fold_img_ids.pkl', 'rb') as f:\n",
    "        wsi_2_validation_img_ids = pickle.load(f)\n",
    "    wsi_validation_img_ids = wsi_1_validation_img_ids[i] + wsi_2_validation_img_ids[i]\n",
    "    cur_validaton_slice = wsi_validation_img_ids\n",
    "    cur_training_slice = list(set(dataset_1_tile_ids) - set(cur_validaton_slice))\n",
    "    print(len(cur_validaton_slice), len(cur_training_slice))\n",
    "    print(cur_validaton_slice, cur_training_slice)\n",
    "  else:\n",
    "    if get_whole_dataset_into_fold:\n",
    "        cur_validaton_slice = wsi_1_ids + wsi_2_ids\n",
    "        cur_training_slice = wsi_1_ids + wsi_2_ids\n",
    "    else:\n",
    "#         cur_validaton_slice = wsi_1_ids[int(i*(fold_size/2)):min(int((i+1)*(fold_size/2)), len(wsi_1_ids))] + wsi_2_ids[int(i*(fold_size/2)):min(int((i+1)*(fold_size/2)), len(wsi_2_ids))]\n",
    "#         cur_training_slice = list(set(dataset_1_tile_ids) - set(cur_validaton_slice)) if not generate_all_datset_annots else list(set(all_dataset_tile_ids) - set(cur_validaton_slice))\n",
    "        # Temporary changes below\n",
    "        cur_validaton_slice = []\n",
    "        num_wsis = float(len(wsi_tile_ids))\n",
    "        for wsi_tile_id in wsi_tile_ids:\n",
    "            cur_validaton_slice += wsi_tile_id[int(i*(fold_size/num_wsis)):min(int((i+1)*(fold_size/num_wsis)), len(wsi_tile_id))]\n",
    "        cur_training_slice = list(set(dataset_1_tile_ids) - set(cur_validaton_slice)) if not generate_all_datset_annots else list(set(all_dataset_tile_ids) - set(cur_validaton_slice))\n",
    "  \n",
    "  print('Saving validation images and annotations...')\n",
    "  for validation_img_id in cur_validaton_slice:\n",
    "      shutil.copy(f'{input_imgs_dir}/{validation_img_id}.png', f'{validation_imgs_dir}/{validation_img_id}.png')\n",
    "      shutil.copy(f'{input_annots_dir}/{validation_img_id}.pkl', f'{validation_annots_dir}/{validation_img_id}.pkl')\n",
    "    \n",
    "  print('Saving training images and annotations...')\n",
    "  for train_img_id in cur_training_slice:\n",
    "    shutil.copy(f'{input_imgs_dir}/{train_img_id}.png', f'{train_imgs_dir}/{train_img_id}.png')\n",
    "    shutil.copy(f'{input_annots_dir}/{train_img_id}.pkl', f'{train_annots_dir}/{train_img_id}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4cc98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "validation_annots_dir = f'{base_path}/{base_data_dir}/{base_data_name}_annotations_merged_validation_{i}' if use_merged_dataset else f'{base_path}/{base_data_dir}/{base_data_name}_annotations_validation_{i}'\n",
    "files = os.listdir(validation_annots_dir)\n",
    "wsi_counts = [0]*len(wsi_tile_ids)\n",
    "for f in files:\n",
    "    img_id = f.split('.pkl')[0]\n",
    "    for idx, wsi_tile_id in enumerate(wsi_tile_ids):\n",
    "        if img_id in wsi_tile_id:\n",
    "            wsi_counts[idx] += 1\n",
    "wsi_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89e5ce2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004d3ef5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164efbc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f36f86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3bd6c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89267e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_size = int(math.ceil(len(dataset_1_tile_ids) / num_folds))\n",
    "print(f'Fold size is {fold_size}')\n",
    "shifting_thresholds = [0.15,0.30,0.45,0.60,0.75,0.90]\n",
    "print(f'Fold size: {fold_size}')\n",
    "for i in range(1):\n",
    "  validation_imgs_dir = f'{base_path}/{base_data_dir}/{base_data_name}_imgs_merged_validation_{i}' if use_merged_dataset else f'{base_path}/{base_data_dir}/{base_data_name}_imgs_validation_{i}'\n",
    "  validation_annots_dir = f'{base_path}/{base_data_dir}/{base_data_name}_annotations_merged_validation_{i}' if use_merged_dataset else f'{base_path}/{base_data_dir}/{base_data_name}_annotations_validation_{i}'\n",
    "  train_imgs_dir = f'{base_path}/{base_data_dir}/{base_data_name}_imgs_merged_train_{i}' if use_merged_dataset else f'{base_path}/{base_data_dir}/{base_data_name}_imgs_train_{i}'\n",
    "  train_annots_dir = f'{base_path}/{base_data_dir}/{base_data_name}_annotations_merged_train_{i}' if use_merged_dataset else f'{base_path}/{base_data_dir}/{base_data_name}_annotations_train_{i}'\n",
    "  \n",
    "  if pick_validation_from_file:\n",
    "    with open(f'{base_path}/dataset1_files/validation_wsi_1_fold_img_ids.pkl', 'rb') as f:\n",
    "        wsi_1_validation_img_ids = pickle.load(f)\n",
    "    with open(f'{base_path}/dataset1_files/validation_wsi_2_fold_img_ids.pkl', 'rb') as f:\n",
    "        wsi_2_validation_img_ids = pickle.load(f)\n",
    "    wsi_validation_img_ids = wsi_1_validation_img_ids[i] + wsi_2_validation_img_ids[i]\n",
    "    cur_validaton_slice = wsi_validation_img_ids\n",
    "    cur_training_slice = list(set(dataset_1_tile_ids) - set(cur_validaton_slice))\n",
    "    print(len(cur_validaton_slice), len(cur_training_slice))\n",
    "    print(cur_validaton_slice, cur_training_slice)\n",
    "  else:\n",
    "#     cur_validaton_slice = dataset_1_tile_ids[int(i*fold_size):min(int((i+1)*fold_size), len(dataset_1_tile_ids))]\n",
    "#     cur_training_slice = dataset_1_tile_ids[:int(i*fold_size)] + dataset_1_tile_ids[min(int((i+1)*fold_size), len(dataset_1_tile_ids)):]\n",
    "    if get_whole_dataset_into_fold:\n",
    "        cur_validaton_slice = wsi_1_ids + wsi_2_ids\n",
    "        cur_training_slice = wsi_1_ids + wsi_2_ids\n",
    "    else:\n",
    "#         cur_validaton_slice = wsi_1_ids[int(i*(fold_size/2)):min(int((i+1)*(fold_size/2)), len(wsi_1_ids))] + wsi_2_ids[int(i*(fold_size/2)):min(int((i+1)*(fold_size/2)), len(wsi_2_ids))]\n",
    "#         cur_training_slice = list(set(dataset_1_tile_ids) - set(cur_validaton_slice)) if not generate_all_datset_annots else list(set(all_dataset_tile_ids) - set(cur_validaton_slice))\n",
    "        # Temporary changes below\n",
    "        cur_validaton_slice = []\n",
    "        num_wsis = float(len(wsi_tile_ids))\n",
    "        for wsi_tile_id in wsi_tile_ids:\n",
    "            cur_validaton_slice += wsi_tile_id[int(i*(fold_size/num_wsis)):min(int((i+1)*(fold_size/num_wsis)), len(wsi_tile_id))]\n",
    "        cur_training_slice = list(set(dataset_1_tile_ids) - set(cur_validaton_slice)) if not generate_all_datset_annots else list(set(all_dataset_tile_ids) - set(cur_validaton_slice))\n",
    "  \n",
    "  print('Saving validation images and annotations...')\n",
    "  for validation_img_id in cur_validaton_slice:\n",
    "      shutil.copy(f'{input_imgs_dir}/{validation_img_id}.png', f'{validation_imgs_dir}/{validation_img_id}.png')\n",
    "      shutil.copy(f'{input_annots_dir}/{validation_img_id}.pkl', f'{validation_annots_dir}/{validation_img_id}.pkl')\n",
    "  \n",
    "#   print('Creating adjacent tile dict for the current validation set...')\n",
    "#   dataset_1_wsi_1_pos_dict = {(x[0], x[1]): x[-1] for x in dataset_1.loc[(dataset_1['source_wsi']==1) & (dataset_1['id'].isin(cur_validaton_slice)), ['i', 'j', 'id']].values}\n",
    "#   dataset_1_wsi_2_pos_dict = {(x[0], x[1]): x[-1] for x in dataset_1.loc[(dataset_1['source_wsi']==2) & (dataset_1['id'].isin(cur_validaton_slice)), ['i', 'j', 'id']].values}\n",
    "#   reverse_dataset_1_wsi_1_pos_dict = {v:k for k,v in dataset_1_wsi_1_pos_dict.items()}\n",
    "#   reverse_dataset_1_wsi_2_pos_dict = {v:k for k,v in dataset_1_wsi_2_pos_dict.items()}\n",
    "#   adjacent_tile_dict = dict()\n",
    "#   for i,j in dataset_1_wsi_1_pos_dict.keys():\n",
    "#     adjacent_tile_dict = get_adjacent_tiles(i, j, dataset_1_wsi_1_pos_dict[(i,j)], adjacent_tile_dict, dataset_1_wsi_1_pos_dict)\n",
    "#   for i,j in dataset_1_wsi_2_pos_dict.keys():\n",
    "#     adjacent_tile_dict = get_adjacent_tiles(i, j, dataset_1_wsi_2_pos_dict[(i,j)], adjacent_tile_dict, dataset_1_wsi_2_pos_dict)\n",
    "  \n",
    "#   print('Saving validation images and annotations...')\n",
    "#   for validation_img_id in adjacent_tile_dict.keys():\n",
    "#     shutil.copy(f'{input_imgs_dir}/{validation_img_id}.png', f'{validation_imgs_dir}/{validation_img_id}.png')\n",
    "#     shutil.copy(f'{input_annots_dir}/{validation_img_id}.pkl', f'{validation_annots_dir}/{validation_img_id}.pkl')\n",
    "#     validation_adjacent_tiles = adjacent_tile_dict[validation_img_id]\n",
    "#     valid_shifts = []\n",
    "#     if validation_adjacent_tiles['R'] is not None:\n",
    "#       valid_shifts.append('R')\n",
    "#     if validation_adjacent_tiles['B'] is not None:\n",
    "#       valid_shifts.append('B')\n",
    "#     if validation_adjacent_tiles['B'] is not None and validation_adjacent_tiles['R'] is not None and validation_adjacent_tiles['BR'] is not None:\n",
    "#       valid_shifts.append('BR')\n",
    "#     if validation_adjacent_tiles['B'] is not None and validation_adjacent_tiles['L'] is not None and validation_adjacent_tiles['BL'] is not None:\n",
    "#       valid_shifts.append('BL')\n",
    "#     for valid_shift in valid_shifts:\n",
    "#       for shifting_threshold in shifting_thresholds:\n",
    "#         shutil.copy(f'{input_imgs_dir}/{validation_img_id}_{valid_shift}_{shifting_threshold}.png', f'{validation_imgs_dir}/{validation_img_id}_{valid_shift}_{shifting_threshold}.png')\n",
    "#         shutil.copy(f'{input_annots_dir}/{validation_img_id}_{valid_shift}_{shifting_threshold}.pkl', f'{validation_annots_dir}/{validation_img_id}_{valid_shift}_{shifting_threshold}.pkl')\n",
    "  \n",
    "  print('Creating adjacent tile dict for the current training set...')\n",
    "  dataset_1_wsi_1_pos_dict = {(x[0], x[1]): x[-1] for x in dataset_1.loc[(dataset_1['source_wsi']==1) & (dataset_1['id'].isin(cur_training_slice)), ['i', 'j', 'id']].values}\n",
    "  dataset_1_wsi_2_pos_dict = {(x[0], x[1]): x[-1] for x in dataset_1.loc[(dataset_1['source_wsi']==2) & (dataset_1['id'].isin(cur_training_slice)), ['i', 'j', 'id']].values}\n",
    "  reverse_dataset_1_wsi_1_pos_dict = {v:k for k,v in dataset_1_wsi_1_pos_dict.items()}\n",
    "  reverse_dataset_1_wsi_2_pos_dict = {v:k for k,v in dataset_1_wsi_2_pos_dict.items()}\n",
    "  adjacent_tile_dict = dict()\n",
    "  for i,j in dataset_1_wsi_1_pos_dict.keys():\n",
    "    adjacent_tile_dict = get_adjacent_tiles(i, j, dataset_1_wsi_1_pos_dict[(i,j)], adjacent_tile_dict, dataset_1_wsi_1_pos_dict)\n",
    "  for i,j in dataset_1_wsi_2_pos_dict.keys():\n",
    "    adjacent_tile_dict = get_adjacent_tiles(i, j, dataset_1_wsi_2_pos_dict[(i,j)], adjacent_tile_dict, dataset_1_wsi_2_pos_dict)\n",
    "  \n",
    "  print('Saving training images and annotations...')\n",
    "  for train_img_id in cur_training_slice:\n",
    "    shutil.copy(f'{input_imgs_dir}/{train_img_id}.png', f'{train_imgs_dir}/{train_img_id}.png')\n",
    "    if should_dilate:\n",
    "        with open(f'{input_annots_dir}/{train_img_id}.pkl', 'rb') as f:\n",
    "            annotations = pickle.load(f)\n",
    "        cur_tile_coco_annots = []\n",
    "        for annot in annotations:\n",
    "            coords = [[x,y] for x,y in zip(annot['segmentation'][0][::2], annot['segmentation'][0][1::2])]\n",
    "            mask = np.zeros((512,512), dtype=np.uint8)\n",
    "            cv2.fillPoly(mask, pts=[np.array(coords)], color=1)\n",
    "            kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "            dilated_mask = cv2.dilate(mask, kernel, iterations=2)\n",
    "            contours, _ = cv2.findContours(dilated_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            dilated_coords = contours[0].reshape(-1, 2)\n",
    "            coords = dilated_coords\n",
    "            segmentations = [[pt for pair in coords for pt in pair]]\n",
    "            segmentation_area = calculate_area(coords)\n",
    "            min_x = min(coords, key=lambda x: x[0])[0]\n",
    "            max_x = max(coords, key=lambda x: x[0])[0]\n",
    "            min_y = min(coords, key=lambda x: x[1])[1]\n",
    "            max_y = max(coords, key=lambda x: x[1])[1]\n",
    "            segmentation_bbox = [min_x, min_y, max_x-min_x, max_y-min_y]\n",
    "            category_id = annot['category_id']\n",
    "            cur_tile_coco_annots.append({\n",
    "              'segmentation': segmentations,\n",
    "              'area': segmentation_area,\n",
    "              'bbox': segmentation_bbox,\n",
    "              'category_id': category_id\n",
    "            })\n",
    "        with open(f'{train_annots_dir}/{train_img_id}.pkl', 'wb') as f:\n",
    "            pickle.dump(cur_tile_coco_annots, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    else:\n",
    "        shutil.copy(f'{input_annots_dir}/{train_img_id}.pkl', f'{train_annots_dir}/{train_img_id}.pkl')\n",
    "    if use_merged_dataset:\n",
    "        train_adjacent_tiles = adjacent_tile_dict[train_img_id]\n",
    "        valid_shifts = []\n",
    "        if train_adjacent_tiles['R'] is not None:\n",
    "          valid_shifts.append('R')\n",
    "        if train_adjacent_tiles['B'] is not None:\n",
    "          valid_shifts.append('B')\n",
    "        if train_adjacent_tiles['B'] is not None and train_adjacent_tiles['R'] is not None and train_adjacent_tiles['BR'] is not None:\n",
    "          valid_shifts.append('BR')\n",
    "        if train_adjacent_tiles['B'] is not None and train_adjacent_tiles['L'] is not None and train_adjacent_tiles['BL'] is not None:\n",
    "          valid_shifts.append('BL')\n",
    "        for valid_shift in valid_shifts:\n",
    "          for shifting_threshold in shifting_thresholds:\n",
    "            shutil.copy(f'{input_imgs_dir}/{train_img_id}_{valid_shift}_{shifting_threshold}.png', f'{train_imgs_dir}/{train_img_id}_{valid_shift}_{shifting_threshold}.png')\n",
    "            shutil.copy(f'{input_annots_dir}/{train_img_id}_{valid_shift}_{shifting_threshold}.pkl', f'{train_annots_dir}/{train_img_id}_{valid_shift}_{shifting_threshold}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32149e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_imgs = os.listdir(f'{base_path}/dataset1_files/all_dataset1_imgs_validation_0')\n",
    "wsi_1_validation_imgs = 0\n",
    "wsi_2_validation_imgs = 0\n",
    "for img in validation_imgs:\n",
    "    if img.split('.png')[0] in wsi_1_ids:\n",
    "        wsi_1_validation_imgs += 1\n",
    "    else:\n",
    "        wsi_2_validation_imgs += 1\n",
    "print(wsi_1_validation_imgs, wsi_2_validation_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5959ef85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(img_dir, img_id):\n",
    "  img = cv2.imread(f\"{img_dir}/{img_id}.png\")\n",
    "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "  return img\n",
    "def get_annotated_mask(imgs_dir, annots_dir, base_img_id):\n",
    "  orig_img = load_img(imgs_dir, base_img_id)\n",
    "  img_height, img_width, _ = orig_img.shape\n",
    "  with open(f'{annots_dir}/{base_img_id}.pkl', 'rb') as f:\n",
    "    tgt_annots = pickle.load(f)\n",
    "  img_mask = np.zeros((img_height, img_width))\n",
    "  for tgt_annot in tgt_annots:\n",
    "    coords = [[x, y] for x, y in zip(tgt_annot['segmentation'][0][::2], tgt_annot['segmentation'][0][1::2])]\n",
    "    cv2.fillPoly(img_mask, pts=[np.array(coords)], color=1)\n",
    "  return img_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6befc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "if generate_masks:\n",
    "  train_modes = ['train', 'validation']\n",
    "  for i in range(1):\n",
    "    start_time = time.time()\n",
    "    for train_mode in train_modes:\n",
    "      orig_img_dir = f'{base_path}/dataset1_files/all_dataset1_imgs_merged_{train_mode}_{i}'\n",
    "      orig_annots_dir = f'{base_path}/dataset1_files/all_dataset1_annotations_merged_{train_mode}_{i}'\n",
    "      output_mask_dir = f'{base_path}/dataset1_files/all_dataset1_masks_merged_{train_mode}_{i}'\n",
    "      all_orig_imgs = os.listdir(orig_img_dir)\n",
    "      for orig_img in all_orig_imgs:\n",
    "        orig_img_id = orig_img.split('.png')[0]\n",
    "        annotated_mask = get_annotated_mask(orig_img_dir, orig_annots_dir, orig_img_id)\n",
    "        cv2.imwrite(f'{output_mask_dir}/{orig_img_id}.png', annotated_mask)\n",
    "    print(f'Finished generating masks for fold {i} in {float(time.time()-start_time)/60} minutes')\n",
    "    start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9630da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_img_id = 'b8db704134ac_BR_0.75'\n",
    "train_mode = 'train'\n",
    "i = 0\n",
    "orig_img = get_annotated_img(f'{base_path}/dataset1_files/all_dataset1_imgs_merged_{train_mode}_{i}', f'{base_path}/dataset1_files/all_dataset1_annotations_merged_{train_mode}_{i}', tgt_img_id)\n",
    "# img_mask = cv2.imread(f'{base_path}/dataset1_files/all_dataset1_masks_merged_{train_mode}_{i}/{tgt_img_id}.png', cv2.IMREAD_GRAYSCALE)\n",
    "plt.imshow(orig_img)\n",
    "plt.show()\n",
    "# plt.imshow(img_mask)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60c5035",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/ec2-user/hubmap-hacking-the-human-vasculature/dataset1_files/all_dataset1_annotations_merged_train_0/bd090bb1b654.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5501a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_files = os.listdir('/home/ec2-user/hubmap-hacking-the-human-vasculature/dataset1_files/all_dataset1_annotations_merged_train_0')\n",
    "no_mask_annots = []\n",
    "for annot_file in annot_files:\n",
    "    with open(f'/home/ec2-user/hubmap-hacking-the-human-vasculature/dataset1_files/all_dataset1_annotations_merged_train_0/{annot_file}', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    if len(data)==0:\n",
    "        no_mask_annots.append(annot_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3a24cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for no_mask_annot in no_mask_annots:\n",
    "    img_id = no_mask_annot.split('.pkl')[0]\n",
    "    if os.path.exists(f'/home/ec2-user/hubmap-hacking-the-human-vasculature/dataset1_files/all_dataset1_annotations_merged_train_0/{img_id}.pkl'):\n",
    "        os.remove(f'/home/ec2-user/hubmap-hacking-the-human-vasculature/dataset1_files/all_dataset1_annotations_merged_train_0/{img_id}.pkl')\n",
    "    if os.path.exists(f'/home/ec2-user/hubmap-hacking-the-human-vasculature/dataset1_files/all_dataset1_imgs_merged_train_0/{img_id}.png'):\n",
    "        os.remove(f'/home/ec2-user/hubmap-hacking-the-human-vasculature/dataset1_files/all_dataset1_imgs_merged_train_0/{img_id}.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e395f04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HubMapEnv",
   "language": "python",
   "name": "hubmapenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
