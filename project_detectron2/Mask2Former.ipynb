{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "832a8981",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mask2former'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdetectron2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m model_zoo\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmask2former\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     44\u001b[0m     COCOInstanceNewBaselineDatasetMapper,\n\u001b[1;32m     45\u001b[0m     COCOPanopticNewBaselineDatasetMapper,\n\u001b[1;32m     46\u001b[0m     InstanceSegEvaluator,\n\u001b[1;32m     47\u001b[0m     MaskFormerInstanceDatasetMapper,\n\u001b[1;32m     48\u001b[0m     MaskFormerPanopticDatasetMapper,\n\u001b[1;32m     49\u001b[0m     MaskFormerSemanticDatasetMapper,\n\u001b[1;32m     50\u001b[0m     SemanticSegmentorWithTTA,\n\u001b[1;32m     51\u001b[0m     add_maskformer2_config,\n\u001b[1;32m     52\u001b[0m )\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# In[2]:\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mask2former'"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import logging\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import shutil\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from sklearn.model_selection import KFold\n",
    "import detectron2.utils.comm as comm\n",
    "from detectron2.checkpoint import DetectionCheckpointer, PeriodicCheckpointer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import default_argument_parser, default_setup, default_writers, launch\n",
    "from detectron2.evaluation import (\n",
    "    CityscapesInstanceEvaluator,\n",
    "    CityscapesSemSegEvaluator,\n",
    "    COCOEvaluator,\n",
    "    COCOPanopticEvaluator,\n",
    "    DatasetEvaluators,\n",
    "    LVISEvaluator,\n",
    "    PascalVOCDetectionEvaluator,\n",
    "    SemSegEvaluator,\n",
    "    inference_on_dataset,\n",
    "    print_csv_format,\n",
    ")\n",
    "from detectron2.data import (\n",
    "    DatasetCatalog,\n",
    "    MetadataCatalog,\n",
    "    build_detection_test_loader,\n",
    "    build_detection_train_loader,\n",
    ")\n",
    "from detectron2.modeling import build_model\n",
    "from detectron2.solver import build_lr_scheduler, build_optimizer\n",
    "from detectron2.utils.events import EventStorage\n",
    "import cv2\n",
    "import pickle\n",
    "from detectron2.structures import BoxMode\n",
    "from detectron2.data import DatasetMapper\n",
    "import detectron2.data.transforms as T\n",
    "from detectron2 import model_zoo\n",
    "import numpy as np\n",
    "\n",
    "from mask2former import (\n",
    "    COCOInstanceNewBaselineDatasetMapper,\n",
    "    COCOPanopticNewBaselineDatasetMapper,\n",
    "    InstanceSegEvaluator,\n",
    "    MaskFormerInstanceDatasetMapper,\n",
    "    MaskFormerPanopticDatasetMapper,\n",
    "    MaskFormerSemanticDatasetMapper,\n",
    "    SemanticSegmentorWithTTA,\n",
    "    add_maskformer2_config,\n",
    ")\n",
    "# In[2]:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3701f8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Custom HubMap Dataset\n",
    "class HubMapDataset:\n",
    "    def __init__(self, image_dir, annotation_dir):\n",
    "        self.image_dir = image_dir\n",
    "        self.annotation_dir = annotation_dir\n",
    "        self.image_file_list = sorted(os.listdir(self.image_dir))\n",
    "        self.annotation_file_list = os.listdir(self.annotation_dir)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_file_list)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_dir, self.image_file_list[idx])\n",
    "        img_id = self.image_file_list[idx].split('.png')[0]\n",
    "        annotation_path = os.path.join(self.annotation_dir, f'{img_id}.pkl')\n",
    "        \n",
    "        image = cv2.imread(image_path)\n",
    "        height, width = image.shape[:2]\n",
    "        \n",
    "        record = {\n",
    "            'file_name': image_path,\n",
    "            'image_id': idx,\n",
    "            'height': height,\n",
    "            'width': width,\n",
    "        }\n",
    "        \n",
    "        with open(annotation_path, 'rb') as f:\n",
    "            orig_annots = pickle.load(f)\n",
    "        \n",
    "        objs = []\n",
    "        for orig_annot in orig_annots:\n",
    "            bbox = orig_annot['bbox']\n",
    "            orig_annot['bbox'] = [bbox[0], bbox[1], bbox[0]+bbox[2], bbox[1]+bbox[3]]\n",
    "            orig_annot['bbox_mode'] = BoxMode.XYXY_ABS\n",
    "            objs.append(orig_annot)\n",
    "            \n",
    "        record['annotations'] = objs\n",
    "        \n",
    "        return record\n",
    "\n",
    "\n",
    "# Function to register a dataset\n",
    "CLASSES = ['blood_vessel']\n",
    "def register_custom_dataset(dataset_name, image_dir, annotation_dir):\n",
    "    DatasetCatalog.register(dataset_name, lambda: HubMapDataset(image_dir, annotation_dir))\n",
    "    MetadataCatalog.get(dataset_name).set(thing_classes=CLASSES, evaluator_type=\"coco\")\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "class CustomArguments:\n",
    "    def __init__(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "# arguments\n",
    "num_folds = 5\n",
    "config_file = '/home/ec2-user/hubmap-hacking-the-human-vasculature/Mask2Former/configs/coco/instance-segmentation/maskformer2_R50_bs16_50ep.yaml'\n",
    "base_dataset_path = '/home/ec2-user/hubmap-hacking-the-human-vasculature/dataset1_files'\n",
    "base_dataset_name = 'hubmap-dataset1'\n",
    "num_machines = 1\n",
    "num_gpus = 1\n",
    "machine_rank = 0\n",
    "port = 2**15 + 2**14 + hash(os.getuid() if sys.platform != \"win32\" else 1) % 2**14\n",
    "dist_url = \"tcp://127.0.0.1:{}\".format(port)\n",
    "opts = []\n",
    "# argument_dict = {'config_file':config_file, 'train_dataset_name': train_dataset_name, 'train_dir': train_dir, 'num_machines':num_machines, 'num_gpus':num_gpus, 'machine_rank': machine_rank, 'dist_url':dist_url, 'opts': opts}\n",
    "# args = CustomArguments(**argument_dict)\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "# Setup the config\n",
    "cfg = get_cfg()\n",
    "add_maskformer2_config(cfg)\n",
    "cfg.merge_from_file(config_file)\n",
    "cfg.DATASETS.TRAIN = ()\n",
    "cfg.DATASETS.TEST = ()\n",
    "# cfg.INPUT.MIN_SIZE_TRAIN = (256,350,480,512)  # Minimum input image size during training\n",
    "cfg.INPUT.MIN_SIZE_TRAIN = (350, 480, 512, 672, 736, 800)\n",
    "cfg.INPUT.MAX_SIZE_TRAIN = 800     # Maximum input image size during training\n",
    "cfg.INPUT.MIN_SIZE_TEST = (512,)      # Minimum input image size during testing\n",
    "cfg.INPUT.MAX_SIZE_TEST = 512      # Maximum input image size during testing\n",
    "cfg.DATALOADER.NUM_WORKERS = 4\n",
    "# cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url('COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml')\n",
    "# cfg.MODEL.WEIGHTS = '/home/ec2-user/hubmap-hacking-the-human-vasculature/project_detectron2/output/inference/best_model_fold_0_with_added_aug_lr_0.00025.pth'\n",
    "cfg.SOLVER.IMS_PER_BATCH = 16\n",
    "cfg.SOLVER.BASE_LR = 0.00025\n",
    "# cfg.SOLVER.BASE_LR = 0.002\n",
    "cfg.SOLVER.MAX_ITER = 6000\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(CLASSES)\n",
    "# cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "# compared to \"train_net.py\", we do not support accurate timing and\n",
    "# precise BN here, because they are not trivial to implement in a small training loop\n",
    "prob = 0.5\n",
    "data_transforms = [\n",
    "    T.RandomApply(T.RandomRotation([-90,90], expand=False), prob=prob),\n",
    "    T.RandomFlip(horizontal=True, vertical=False, prob=prob),\n",
    "    T.RandomFlip(horizontal=False, vertical=True, prob=prob),\n",
    "    T.RandomApply(T.RandomBrightness(0.8, 1.2), prob=prob),\n",
    "    T.RandomApply(T.RandomContrast(0.8, 1.2), prob=prob),\n",
    "    T.RandomApply(T.RandomSaturation(0.8,1.2), prob=prob),\n",
    "    T.RandomApply(T.RandomCrop('relative', (0.8, 0.8)), prob=prob)\n",
    "]\n",
    "\n",
    "# data_loader = build_detection_train_loader(cfg, mapper=DatasetMapper(cfg, is_train=True, augmentations=data_transforms))\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "def get_evaluator(cfg, dataset_name, output_folder=None):\n",
    "    \"\"\"\n",
    "    Create evaluator(s) for a given dataset.\n",
    "    This uses the special metadata \"evaluator_type\" associated with each builtin dataset.\n",
    "    For your own dataset, you can simply create an evaluator manually in your\n",
    "    script and do not have to worry about the hacky if-else logic here.\n",
    "    \"\"\"\n",
    "    if output_folder is None:\n",
    "        output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n",
    "    evaluator_list = []\n",
    "    evaluator_type = MetadataCatalog.get(dataset_name).evaluator_type\n",
    "    if evaluator_type in [\"sem_seg\", \"coco_panoptic_seg\"]:\n",
    "        evaluator_list.append(\n",
    "            SemSegEvaluator(\n",
    "                dataset_name,\n",
    "                distributed=True,\n",
    "                output_dir=output_folder,\n",
    "            )\n",
    "        )\n",
    "    if evaluator_type in [\"coco\", \"coco_panoptic_seg\"]:\n",
    "        evaluator_list.append(COCOEvaluator(dataset_name, output_dir=output_folder))\n",
    "    if evaluator_type == \"coco_panoptic_seg\":\n",
    "        evaluator_list.append(COCOPanopticEvaluator(dataset_name, output_folder))\n",
    "    if evaluator_type == \"cityscapes_instance\":\n",
    "        return CityscapesInstanceEvaluator(dataset_name)\n",
    "    if evaluator_type == \"cityscapes_sem_seg\":\n",
    "        return CityscapesSemSegEvaluator(dataset_name)\n",
    "    if evaluator_type == \"pascal_voc\":\n",
    "        return PascalVOCDetectionEvaluator(dataset_name)\n",
    "    if evaluator_type == \"lvis\":\n",
    "        return LVISEvaluator(dataset_name, cfg, True, output_folder)\n",
    "    if len(evaluator_list) == 0:\n",
    "        raise NotImplementedError(\n",
    "            \"no Evaluator for the dataset {} with the type {}\".format(dataset_name, evaluator_type)\n",
    "        )\n",
    "    if len(evaluator_list) == 1:\n",
    "        return evaluator_list[0]\n",
    "    return DatasetEvaluators(evaluator_list)\n",
    "\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "max_iter = cfg.SOLVER.MAX_ITER\n",
    "writers = default_writers(cfg.OUTPUT_DIR, max_iter) if comm.is_main_process() else []\n",
    "\n",
    "output_dir = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n",
    "\n",
    "for i in range(num_folds):\n",
    "    if os.path.exists(f'{output_dir}/model_stats_detectron_dataset1_fold_{i}.txt'):\n",
    "        os.remove(f'{output_dir}/model_stats_detectron_dataset1_fold_{i}.txt')\n",
    "\n",
    "for i in range(num_folds):\n",
    "    if os.path.exists(os.path.join(cfg.OUTPUT_DIR, \"inference\", f'{base_dataset_name}-train-fold-{i}')):\n",
    "        shutil.rmtree(os.path.join(cfg.OUTPUT_DIR, \"inference\", f'{base_dataset_name}-train-fold-{i}'))\n",
    "    if os.path.exists(os.path.join(cfg.OUTPUT_DIR, \"inference\", f'{base_dataset_name}-validation-fold-{i}')):\n",
    "        shutil.rmtree(os.path.join(cfg.OUTPUT_DIR, \"inference\", f'{base_dataset_name}-validation-fold-{i}'))\n",
    "\n",
    "for i in range(num_folds):\n",
    "    register_custom_dataset(f'{base_dataset_name}-train-fold-{i}', f'{base_dataset_path}/all_dataset1_imgs_merged_train_{i}', f'{base_dataset_path}/all_dataset1_annotations_merged_train_{i}')\n",
    "    register_custom_dataset(f'{base_dataset_name}-validation-fold-{i}', f'{base_dataset_path}/all_dataset1_imgs_merged_validation_{i}', f'{base_dataset_path}/all_dataset1_annotations_merged_validation_{i}')\n",
    "\n",
    "for i in range(num_folds):\n",
    "    train_dataset = DatasetCatalog.get(f'{base_dataset_name}-train-fold-{i}')\n",
    "    train_data_loader = build_detection_train_loader(cfg, mapper=DatasetMapper(cfg, is_train=True, augmentations=data_transforms), dataset=train_dataset)\n",
    "    validation_data_loader = build_detection_test_loader(cfg, f'{base_dataset_name}-validation-fold-{i}')\n",
    "    evaluator = get_evaluator(\n",
    "        cfg, f'{base_dataset_name}-validation-fold-{i}', os.path.join(cfg.OUTPUT_DIR, \"inference\", f'{base_dataset_name}-validation-fold-{i}')\n",
    "    )\n",
    "    \n",
    "    model = build_model(cfg)\n",
    "    model.train()\n",
    "    resume = False\n",
    "    optimizer = build_optimizer(cfg, model)\n",
    "    scheduler = build_lr_scheduler(cfg, optimizer)\n",
    "    checkpointer = DetectionCheckpointer(\n",
    "        model, cfg.OUTPUT_DIR, optimizer=optimizer, scheduler=scheduler\n",
    "    )\n",
    "    start_iter = (\n",
    "        checkpointer.resume_or_load(cfg.MODEL.WEIGHTS, resume=resume).get(\"iteration\", -1) + 1\n",
    "    )\n",
    "    periodic_checkpointer = PeriodicCheckpointer(\n",
    "        checkpointer, cfg.SOLVER.CHECKPOINT_PERIOD, max_iter=max_iter\n",
    "    )\n",
    "    iterations_per_epoch = len(train_dataset) // cfg.SOLVER.IMS_PER_BATCH\n",
    "    num_epochs = max_iter // iterations_per_epoch\n",
    "    num_iterations_to_show_stats = 50\n",
    "    max_ap = 0\n",
    "    loss_stats = {'total_loss': [], 'loss_cls': [], 'loss_box_reg': [], 'loss_mask': [], 'loss_rpn_cls': [], 'loss_rpn_loc': []}\n",
    "    with open(f'{output_dir}/model_stats_detectron_dataset1_fold_{i}.txt', 'a') as f:\n",
    "        f.write(f'Epoch info is - num_epochs: {num_epochs}, max_iter: {max_iter}, train_dataset_len: {len(train_dataset)}, iterations_per_epoch: {iterations_per_epoch}, num_iterations_to_show_stats: {num_iterations_to_show_stats}\\n')\n",
    "    # Training Loop\n",
    "    with EventStorage(start_iter) as storage:\n",
    "        start_time = time.time()\n",
    "        for data, iteration in zip(train_data_loader, range(start_iter, max_iter)):\n",
    "            storage.iter = iteration\n",
    "\n",
    "            loss_dict = model(data)\n",
    "            for loss_key in loss_dict:\n",
    "                if loss_key in loss_stats:\n",
    "                    loss_stats[loss_key].append(loss_dict[loss_key].item())\n",
    "            losses = sum(loss_dict.values())\n",
    "            loss_stats['total_loss'].append(losses.item())\n",
    "            assert torch.isfinite(losses).all(), loss_dict\n",
    "\n",
    "            loss_dict_reduced = {k: v.item() for k, v in comm.reduce_dict(loss_dict).items()}\n",
    "            losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
    "            if comm.is_main_process():\n",
    "                storage.put_scalars(total_loss=losses_reduced, **loss_dict_reduced)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "            storage.put_scalar(\"lr\", optimizer.param_groups[0][\"lr\"], smoothing_hint=False)\n",
    "            scheduler.step()\n",
    "\n",
    "            if iteration - start_iter > 5 and (\n",
    "                (iteration + 1) % 20 == 0 or iteration == max_iter - 1\n",
    "            ):\n",
    "                for writer in writers:\n",
    "                    writer.write()\n",
    "            periodic_checkpointer.step(iteration)\n",
    "        \n",
    "            if (iteration+1) % num_iterations_to_show_stats == 0:\n",
    "                metrics = inference_on_dataset(model, validation_data_loader, evaluator)\n",
    "                print('===========')\n",
    "                print(metrics)\n",
    "                print('===========')\n",
    "                metrics_str = ''\n",
    "                for task, task_metrics in metrics.items():\n",
    "                    task_str = f'{task}: '\n",
    "                    for metric, value in task_metrics.items():\n",
    "                        task_str += f'{metric}={value:.4f}, '\n",
    "                    metrics_str += task_str.rstrip(', ') + '\\n'\n",
    "                metrics_str = f'Iteration: {iteration}, time_taken: {float(time.time()-start_time)/60} minutes --> {metrics_str}'\n",
    "                loss_str = ''\n",
    "                for loss_key in loss_stats.keys():\n",
    "                    loss_str += f'{loss_key} - {np.mean(loss_stats[loss_key])}, '\n",
    "                    loss_stats[loss_key] = []\n",
    "                if 'segm' in metrics and metrics['segm']['AP'] > max_ap and metrics['segm']['AP']-max_ap >= 1:\n",
    "                    max_ap = metrics['segm']['AP']\n",
    "                    torch.save(model.state_dict(), f'{output_dir}/best_model_fold_{i}.pth')\n",
    "                    with open(f'{output_dir}/best_model_stats_detectron_dataset1_fold_{i}.txt', 'w') as f:\n",
    "                        f.write(f'{metrics_str}\\n{loss_str}\\n')\n",
    "                with open(f'{output_dir}/model_stats_detectron_dataset1_fold_{i}.txt', 'a') as f:\n",
    "                    f.write(f'{metrics_str}\\n{loss_str}\\n')\n",
    "                start_time = time.time()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detectron_env",
   "language": "python",
   "name": "detectron_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
