{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "832a8981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "sys.path.insert(0, '/home/ec2-user/hubmap-hacking-the-human-vasculature/Mask2Former')\n",
    "\n",
    "import logging\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import shutil\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from sklearn.model_selection import KFold\n",
    "import detectron2.utils.comm as comm\n",
    "from detectron2.checkpoint import DetectionCheckpointer, PeriodicCheckpointer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import default_argument_parser, default_setup, default_writers, launch\n",
    "from detectron2.evaluation import (\n",
    "    CityscapesInstanceEvaluator,\n",
    "    CityscapesSemSegEvaluator,\n",
    "    COCOEvaluator,\n",
    "    COCOPanopticEvaluator,\n",
    "    DatasetEvaluators,\n",
    "    LVISEvaluator,\n",
    "    PascalVOCDetectionEvaluator,\n",
    "    SemSegEvaluator,\n",
    "    inference_on_dataset,\n",
    "    print_csv_format,\n",
    ")\n",
    "from detectron2.data import (\n",
    "    DatasetCatalog,\n",
    "    MetadataCatalog,\n",
    "    build_detection_test_loader,\n",
    "    build_detection_train_loader,\n",
    ")\n",
    "from detectron2.modeling import build_model\n",
    "from detectron2.solver import build_lr_scheduler, build_optimizer\n",
    "from detectron2.utils.events import EventStorage\n",
    "import cv2\n",
    "import pickle\n",
    "from detectron2.structures import BoxMode\n",
    "from detectron2.data import DatasetMapper\n",
    "import detectron2.data.transforms as T\n",
    "from detectron2 import model_zoo\n",
    "import numpy as np\n",
    "\n",
    "from mask2former import (\n",
    "    COCOInstanceNewBaselineDatasetMapper,\n",
    "    COCOPanopticNewBaselineDatasetMapper,\n",
    "    InstanceSegEvaluator,\n",
    "    MaskFormerInstanceDatasetMapper,\n",
    "    MaskFormerPanopticDatasetMapper,\n",
    "    MaskFormerSemanticDatasetMapper,\n",
    "    SemanticSegmentorWithTTA,\n",
    "    add_maskformer2_config,\n",
    ")\n",
    "from train_net import Trainer\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "# Custom HubMap Dataset\n",
    "class HubMapDataset:\n",
    "    def __init__(self, image_dir, annotation_dir):\n",
    "        self.image_dir = image_dir\n",
    "        self.annotation_dir = annotation_dir\n",
    "        self.image_file_list = sorted(os.listdir(self.image_dir))\n",
    "        self.annotation_file_list = os.listdir(self.annotation_dir)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_file_list)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_dir, self.image_file_list[idx])\n",
    "        img_id = self.image_file_list[idx].split('.png')[0]\n",
    "        annotation_path = os.path.join(self.annotation_dir, f'{img_id}.pkl')\n",
    "        \n",
    "        image = cv2.imread(image_path)\n",
    "        height, width = image.shape[:2]\n",
    "        \n",
    "        record = {\n",
    "            'file_name': image_path,\n",
    "            'image_id': idx,\n",
    "            'height': height,\n",
    "            'width': width,\n",
    "        }\n",
    "        \n",
    "        with open(annotation_path, 'rb') as f:\n",
    "            orig_annots = pickle.load(f)\n",
    "        \n",
    "        objs = []\n",
    "        for orig_annot in orig_annots:\n",
    "            bbox = orig_annot['bbox']\n",
    "            orig_annot['bbox'] = [bbox[0], bbox[1], bbox[0]+bbox[2], bbox[1]+bbox[3]]\n",
    "            orig_annot['bbox_mode'] = BoxMode.XYXY_ABS\n",
    "            objs.append(orig_annot)\n",
    "            \n",
    "        record['annotations'] = objs\n",
    "        \n",
    "        return record\n",
    "\n",
    "\n",
    "model_prefix = \"mask2former\"\n",
    "# Function to register a dataset\n",
    "CLASSES = ['blood_vessel']\n",
    "def register_custom_dataset(dataset_name, image_dir, annotation_dir):\n",
    "    DatasetCatalog.register(dataset_name, lambda: HubMapDataset(image_dir, annotation_dir))\n",
    "    MetadataCatalog.get(dataset_name).set(thing_classes=CLASSES, evaluator_type=\"coco\")\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "class CustomArguments:\n",
    "    def __init__(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "# arguments\n",
    "num_folds = 5\n",
    "config_file = '/home/ec2-user/hubmap-hacking-the-human-vasculature/Mask2Former/configs/coco/instance-segmentation/maskformer2_R50_bs16_50ep.yaml'\n",
    "base_dataset_path = '/home/ec2-user/hubmap-hacking-the-human-vasculature/dataset1_files'\n",
    "base_dataset_name = 'hubmap-dataset1'\n",
    "num_machines = 1\n",
    "num_gpus = 1\n",
    "machine_rank = 0\n",
    "port = 2**15 + 2**14 + hash(os.getuid() if sys.platform != \"win32\" else 1) % 2**14\n",
    "dist_url = \"tcp://127.0.0.1:{}\".format(port)\n",
    "opts = []\n",
    "# argument_dict = {'config_file':config_file, 'train_dataset_name': train_dataset_name, 'train_dir': train_dir, 'num_machines':num_machines, 'num_gpus':num_gpus, 'machine_rank': machine_rank, 'dist_url':dist_url, 'opts': opts}\n",
    "# args = CustomArguments(**argument_dict)\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3701f8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDNN_BENCHMARK: False\n",
      "DATALOADER:\n",
      "  ASPECT_RATIO_GROUPING: True\n",
      "  FILTER_EMPTY_ANNOTATIONS: True\n",
      "  NUM_WORKERS: 4\n",
      "  REPEAT_THRESHOLD: 0.0\n",
      "  SAMPLER_TRAIN: TrainingSampler\n",
      "DATASETS:\n",
      "  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000\n",
      "  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000\n",
      "  PROPOSAL_FILES_TEST: ()\n",
      "  PROPOSAL_FILES_TRAIN: ()\n",
      "  TEST: ()\n",
      "  TRAIN: ()\n",
      "GLOBAL:\n",
      "  HACK: 1.0\n",
      "INPUT:\n",
      "  COLOR_AUG_SSD: False\n",
      "  CROP:\n",
      "    ENABLED: False\n",
      "    SINGLE_CATEGORY_MAX_AREA: 1.0\n",
      "    SIZE: [0.9, 0.9]\n",
      "    TYPE: relative_range\n",
      "  DATASET_MAPPER_NAME: mask_former_semantic\n",
      "  FORMAT: BGR\n",
      "  IMAGE_SIZE: 1024\n",
      "  MASK_FORMAT: polygon\n",
      "  MAX_SCALE: 2.0\n",
      "  MAX_SIZE_TEST: 1333\n",
      "  MAX_SIZE_TRAIN: 1333\n",
      "  MIN_SCALE: 0.1\n",
      "  MIN_SIZE_TEST: 800\n",
      "  MIN_SIZE_TRAIN: (800,)\n",
      "  MIN_SIZE_TRAIN_SAMPLING: choice\n",
      "  RANDOM_FLIP: horizontal\n",
      "  SIZE_DIVISIBILITY: -1\n",
      "MODEL:\n",
      "  ANCHOR_GENERATOR:\n",
      "    ANGLES: [[-90, 0, 90]]\n",
      "    ASPECT_RATIOS: [[0.5, 1.0, 2.0]]\n",
      "    NAME: DefaultAnchorGenerator\n",
      "    OFFSET: 0.0\n",
      "    SIZES: [[32, 64, 128, 256, 512]]\n",
      "  BACKBONE:\n",
      "    FREEZE_AT: 2\n",
      "    NAME: build_resnet_backbone\n",
      "  DEVICE: cuda\n",
      "  FPN:\n",
      "    FUSE_TYPE: sum\n",
      "    IN_FEATURES: []\n",
      "    NORM: \n",
      "    OUT_CHANNELS: 256\n",
      "  KEYPOINT_ON: False\n",
      "  LOAD_PROPOSALS: False\n",
      "  MASK_FORMER:\n",
      "    CLASS_WEIGHT: 1.0\n",
      "    DEC_LAYERS: 6\n",
      "    DEEP_SUPERVISION: True\n",
      "    DICE_WEIGHT: 1.0\n",
      "    DIM_FEEDFORWARD: 2048\n",
      "    DROPOUT: 0.1\n",
      "    ENC_LAYERS: 0\n",
      "    ENFORCE_INPUT_PROJ: False\n",
      "    HIDDEN_DIM: 256\n",
      "    IMPORTANCE_SAMPLE_RATIO: 0.75\n",
      "    MASK_WEIGHT: 20.0\n",
      "    NHEADS: 8\n",
      "    NO_OBJECT_WEIGHT: 0.1\n",
      "    NUM_OBJECT_QUERIES: 100\n",
      "    OVERSAMPLE_RATIO: 3.0\n",
      "    PRE_NORM: False\n",
      "    SIZE_DIVISIBILITY: 32\n",
      "    TEST:\n",
      "      INSTANCE_ON: False\n",
      "      OBJECT_MASK_THRESHOLD: 0.0\n",
      "      OVERLAP_THRESHOLD: 0.0\n",
      "      PANOPTIC_ON: False\n",
      "      SEMANTIC_ON: True\n",
      "      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: False\n",
      "    TRAIN_NUM_POINTS: 12544\n",
      "    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder\n",
      "    TRANSFORMER_IN_FEATURE: res5\n",
      "  MASK_ON: False\n",
      "  META_ARCHITECTURE: GeneralizedRCNN\n",
      "  PANOPTIC_FPN:\n",
      "    COMBINE:\n",
      "      ENABLED: True\n",
      "      INSTANCES_CONFIDENCE_THRESH: 0.5\n",
      "      OVERLAP_THRESH: 0.5\n",
      "      STUFF_AREA_LIMIT: 4096\n",
      "    INSTANCE_LOSS_WEIGHT: 1.0\n",
      "  PIXEL_MEAN: [103.53, 116.28, 123.675]\n",
      "  PIXEL_STD: [1.0, 1.0, 1.0]\n",
      "  PROPOSAL_GENERATOR:\n",
      "    MIN_SIZE: 0\n",
      "    NAME: RPN\n",
      "  RESNETS:\n",
      "    DEFORM_MODULATED: False\n",
      "    DEFORM_NUM_GROUPS: 1\n",
      "    DEFORM_ON_PER_STAGE: [False, False, False, False]\n",
      "    DEPTH: 50\n",
      "    NORM: FrozenBN\n",
      "    NUM_GROUPS: 1\n",
      "    OUT_FEATURES: ['res4']\n",
      "    RES2_OUT_CHANNELS: 256\n",
      "    RES5_DILATION: 1\n",
      "    STEM_OUT_CHANNELS: 64\n",
      "    STRIDE_IN_1X1: True\n",
      "    WIDTH_PER_GROUP: 64\n",
      "  RETINANET:\n",
      "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
      "    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)\n",
      "    FOCAL_LOSS_ALPHA: 0.25\n",
      "    FOCAL_LOSS_GAMMA: 2.0\n",
      "    IN_FEATURES: ['p3', 'p4', 'p5', 'p6', 'p7']\n",
      "    IOU_LABELS: [0, -1, 1]\n",
      "    IOU_THRESHOLDS: [0.4, 0.5]\n",
      "    NMS_THRESH_TEST: 0.5\n",
      "    NORM: \n",
      "    NUM_CLASSES: 80\n",
      "    NUM_CONVS: 4\n",
      "    PRIOR_PROB: 0.01\n",
      "    SCORE_THRESH_TEST: 0.05\n",
      "    SMOOTH_L1_LOSS_BETA: 0.1\n",
      "    TOPK_CANDIDATES_TEST: 1000\n",
      "  ROI_BOX_CASCADE_HEAD:\n",
      "    BBOX_REG_WEIGHTS: ((10.0, 10.0, 5.0, 5.0), (20.0, 20.0, 10.0, 10.0), (30.0, 30.0, 15.0, 15.0))\n",
      "    IOUS: (0.5, 0.6, 0.7)\n",
      "  ROI_BOX_HEAD:\n",
      "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
      "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
      "    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)\n",
      "    CLS_AGNOSTIC_BBOX_REG: False\n",
      "    CONV_DIM: 256\n",
      "    FC_DIM: 1024\n",
      "    FED_LOSS_FREQ_WEIGHT_POWER: 0.5\n",
      "    FED_LOSS_NUM_CLASSES: 50\n",
      "    NAME: \n",
      "    NORM: \n",
      "    NUM_CONV: 0\n",
      "    NUM_FC: 0\n",
      "    POOLER_RESOLUTION: 14\n",
      "    POOLER_SAMPLING_RATIO: 0\n",
      "    POOLER_TYPE: ROIAlignV2\n",
      "    SMOOTH_L1_BETA: 0.0\n",
      "    TRAIN_ON_PRED_BOXES: False\n",
      "    USE_FED_LOSS: False\n",
      "    USE_SIGMOID_CE: False\n",
      "  ROI_HEADS:\n",
      "    BATCH_SIZE_PER_IMAGE: 512\n",
      "    IN_FEATURES: ['res4']\n",
      "    IOU_LABELS: [0, 1]\n",
      "    IOU_THRESHOLDS: [0.5]\n",
      "    NAME: Res5ROIHeads\n",
      "    NMS_THRESH_TEST: 0.5\n",
      "    NUM_CLASSES: 80\n",
      "    POSITIVE_FRACTION: 0.25\n",
      "    PROPOSAL_APPEND_GT: True\n",
      "    SCORE_THRESH_TEST: 0.05\n",
      "  ROI_KEYPOINT_HEAD:\n",
      "    CONV_DIMS: (512, 512, 512, 512, 512, 512, 512, 512)\n",
      "    LOSS_WEIGHT: 1.0\n",
      "    MIN_KEYPOINTS_PER_IMAGE: 1\n",
      "    NAME: KRCNNConvDeconvUpsampleHead\n",
      "    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: True\n",
      "    NUM_KEYPOINTS: 17\n",
      "    POOLER_RESOLUTION: 14\n",
      "    POOLER_SAMPLING_RATIO: 0\n",
      "    POOLER_TYPE: ROIAlignV2\n",
      "  ROI_MASK_HEAD:\n",
      "    CLS_AGNOSTIC_MASK: False\n",
      "    CONV_DIM: 256\n",
      "    NAME: MaskRCNNConvUpsampleHead\n",
      "    NORM: \n",
      "    NUM_CONV: 0\n",
      "    POOLER_RESOLUTION: 14\n",
      "    POOLER_SAMPLING_RATIO: 0\n",
      "    POOLER_TYPE: ROIAlignV2\n",
      "  RPN:\n",
      "    BATCH_SIZE_PER_IMAGE: 256\n",
      "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
      "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
      "    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)\n",
      "    BOUNDARY_THRESH: -1\n",
      "    CONV_DIMS: [-1]\n",
      "    HEAD_NAME: StandardRPNHead\n",
      "    IN_FEATURES: ['res4']\n",
      "    IOU_LABELS: [0, -1, 1]\n",
      "    IOU_THRESHOLDS: [0.3, 0.7]\n",
      "    LOSS_WEIGHT: 1.0\n",
      "    NMS_THRESH: 0.7\n",
      "    POSITIVE_FRACTION: 0.5\n",
      "    POST_NMS_TOPK_TEST: 1000\n",
      "    POST_NMS_TOPK_TRAIN: 2000\n",
      "    PRE_NMS_TOPK_TEST: 6000\n",
      "    PRE_NMS_TOPK_TRAIN: 12000\n",
      "    SMOOTH_L1_BETA: 0.0\n",
      "  SEM_SEG_HEAD:\n",
      "    COMMON_STRIDE: 4\n",
      "    CONVS_DIM: 128\n",
      "    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES: ['res3', 'res4', 'res5']\n",
      "    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8\n",
      "    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4\n",
      "    IGNORE_VALUE: 255\n",
      "    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']\n",
      "    LOSS_WEIGHT: 1.0\n",
      "    MASK_DIM: 256\n",
      "    NAME: SemSegFPNHead\n",
      "    NORM: GN\n",
      "    NUM_CLASSES: 54\n",
      "    PIXEL_DECODER_NAME: BasePixelDecoder\n",
      "    TRANSFORMER_ENC_LAYERS: 0\n",
      "  SWIN:\n",
      "    APE: False\n",
      "    ATTN_DROP_RATE: 0.0\n",
      "    DEPTHS: [2, 2, 6, 2]\n",
      "    DROP_PATH_RATE: 0.3\n",
      "    DROP_RATE: 0.0\n",
      "    EMBED_DIM: 96\n",
      "    MLP_RATIO: 4.0\n",
      "    NUM_HEADS: [3, 6, 12, 24]\n",
      "    OUT_FEATURES: ['res2', 'res3', 'res4', 'res5']\n",
      "    PATCH_NORM: True\n",
      "    PATCH_SIZE: 4\n",
      "    PRETRAIN_IMG_SIZE: 224\n",
      "    QKV_BIAS: True\n",
      "    QK_SCALE: None\n",
      "    USE_CHECKPOINT: False\n",
      "    WINDOW_SIZE: 7\n",
      "  WEIGHTS: \n",
      "OUTPUT_DIR: ./output\n",
      "SEED: -1\n",
      "SOLVER:\n",
      "  AMP:\n",
      "    ENABLED: False\n",
      "  BACKBONE_MULTIPLIER: 0.1\n",
      "  BASE_LR: 0.001\n",
      "  BASE_LR_END: 0.0\n",
      "  BIAS_LR_FACTOR: 1.0\n",
      "  CHECKPOINT_PERIOD: 5000\n",
      "  CLIP_GRADIENTS:\n",
      "    CLIP_TYPE: value\n",
      "    CLIP_VALUE: 1.0\n",
      "    ENABLED: False\n",
      "    NORM_TYPE: 2.0\n",
      "  GAMMA: 0.1\n",
      "  IMS_PER_BATCH: 16\n",
      "  LR_SCHEDULER_NAME: WarmupMultiStepLR\n",
      "  MAX_ITER: 40000\n",
      "  MOMENTUM: 0.9\n",
      "  NESTEROV: False\n",
      "  NUM_DECAYS: 3\n",
      "  OPTIMIZER: ADAMW\n",
      "  REFERENCE_WORLD_SIZE: 0\n",
      "  RESCALE_INTERVAL: False\n",
      "  STEPS: (30000,)\n",
      "  WARMUP_FACTOR: 0.001\n",
      "  WARMUP_ITERS: 1000\n",
      "  WARMUP_METHOD: linear\n",
      "  WEIGHT_DECAY: 0.0001\n",
      "  WEIGHT_DECAY_BIAS: None\n",
      "  WEIGHT_DECAY_EMBED: 0.0\n",
      "  WEIGHT_DECAY_NORM: 0.0\n",
      "TEST:\n",
      "  AUG:\n",
      "    ENABLED: False\n",
      "    FLIP: True\n",
      "    MAX_SIZE: 4000\n",
      "    MIN_SIZES: (400, 500, 600, 700, 800, 900, 1000, 1100, 1200)\n",
      "  DETECTIONS_PER_IMAGE: 100\n",
      "  EVAL_PERIOD: 0\n",
      "  EXPECTED_RESULTS: []\n",
      "  KEYPOINT_OKS_SIGMAS: []\n",
      "  PRECISE_BN:\n",
      "    ENABLED: False\n",
      "    NUM_ITER: 200\n",
      "VERSION: 2\n",
      "VIS_PERIOD: 0\n",
      "\u001b[32m[06/17 17:53:49 d2.engine.defaults]: \u001b[0mModel:\n",
      "MaskFormer(\n",
      "  (backbone): ResNet(\n",
      "    (stem): BasicStem(\n",
      "      (conv1): Conv2d(\n",
      "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (res2): Sequential(\n",
      "      (0): BottleneckBlock(\n",
      "        (shortcut): Conv2d(\n",
      "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "        (conv1): Conv2d(\n",
      "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (1): BottleneckBlock(\n",
      "        (conv1): Conv2d(\n",
      "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (2): BottleneckBlock(\n",
      "        (conv1): Conv2d(\n",
      "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (res3): Sequential(\n",
      "      (0): BottleneckBlock(\n",
      "        (shortcut): Conv2d(\n",
      "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "        )\n",
      "        (conv1): Conv2d(\n",
      "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (1): BottleneckBlock(\n",
      "        (conv1): Conv2d(\n",
      "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (2): BottleneckBlock(\n",
      "        (conv1): Conv2d(\n",
      "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (3): BottleneckBlock(\n",
      "        (conv1): Conv2d(\n",
      "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (res4): Sequential(\n",
      "      (0): BottleneckBlock(\n",
      "        (shortcut): Conv2d(\n",
      "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "        )\n",
      "        (conv1): Conv2d(\n",
      "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (1): BottleneckBlock(\n",
      "        (conv1): Conv2d(\n",
      "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (2): BottleneckBlock(\n",
      "        (conv1): Conv2d(\n",
      "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (3): BottleneckBlock(\n",
      "        (conv1): Conv2d(\n",
      "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (4): BottleneckBlock(\n",
      "        (conv1): Conv2d(\n",
      "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (5): BottleneckBlock(\n",
      "        (conv1): Conv2d(\n",
      "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (res5): Sequential(\n",
      "      (0): BottleneckBlock(\n",
      "        (shortcut): Conv2d(\n",
      "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "        )\n",
      "        (conv1): Conv2d(\n",
      "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (1): BottleneckBlock(\n",
      "        (conv1): Conv2d(\n",
      "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (2): BottleneckBlock(\n",
      "        (conv1): Conv2d(\n",
      "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (sem_seg_head): MaskFormerHead(\n",
      "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
      "      (input_proj): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "        )\n",
      "      )\n",
      "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
      "        (encoder): MSDeformAttnTransformerEncoder(\n",
      "          (layers): ModuleList(\n",
      "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
      "              (self_attn): MSDeformAttn(\n",
      "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
      "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
      "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              )\n",
      "              (dropout1): Dropout(p=0.0, inplace=False)\n",
      "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.0, inplace=False)\n",
      "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "              (dropout3): Dropout(p=0.0, inplace=False)\n",
      "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
      "          num_pos_feats: 128\n",
      "          temperature: 10000\n",
      "          normalize: True\n",
      "          scale: 6.283185307179586\n",
      "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (adapter_1): Conv2d(\n",
      "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (layer_1): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "      )\n",
      "    )\n",
      "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
      "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
      "          num_pos_feats: 128\n",
      "          temperature: 10000\n",
      "          normalize: True\n",
      "          scale: 6.283185307179586\n",
      "      (transformer_self_attention_layers): ModuleList(\n",
      "        (0-8): 9 x SelfAttentionLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (transformer_cross_attention_layers): ModuleList(\n",
      "        (0-8): 9 x CrossAttentionLayer(\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (transformer_ffn_layers): ModuleList(\n",
      "        (0-8): 9 x FFNLayer(\n",
      "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (query_feat): Embedding(100, 256)\n",
      "      (query_embed): Embedding(100, 256)\n",
      "      (level_embed): Embedding(3, 256)\n",
      "      (input_proj): ModuleList(\n",
      "        (0-2): 3 x Sequential()\n",
      "      )\n",
      "      (class_embed): Linear(in_features=256, out_features=81, bias=True)\n",
      "      (mask_embed): MLP(\n",
      "        (layers): ModuleList(\n",
      "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (criterion): Criterion SetCriterion\n",
      "      matcher: Matcher HungarianMatcher\n",
      "          cost_class: 2.0\n",
      "          cost_mask: 5.0\n",
      "          cost_dice: 5.0\n",
      "      losses: ['labels', 'masks']\n",
      "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
      "      num_classes: 80\n",
      "      eos_coef: 0.1\n",
      "      num_points: 12544\n",
      "      oversample_ratio: 3.0\n",
      "      importance_sample_ratio: 0.75\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/17 17:54:19 d2.data.build]: \u001b[0mRemoved 43 images with no usable annotations. 4656 images left.\n",
      "\u001b[32m[06/17 17:54:19 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
      "\u001b[32m[06/17 17:54:19 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[06/17 17:54:19 d2.data.common]: \u001b[0mSerializing 4656 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[06/17 17:54:19 d2.data.common]: \u001b[0mSerialized dataset takes 18.31 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[06/17 17:54:19 d2.solver.build]: \u001b[0mSOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.\n",
      "\u001b[32m[06/17 17:54:20 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/detectron_env/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/home/ec2-user/anaconda3/envs/detectron_env/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Setup the config\n",
    "cfg = get_cfg()\n",
    "add_maskformer2_config(cfg)\n",
    "print(cfg)\n",
    "cfg.merge_from_file(config_file)\n",
    "cfg.DATASETS.TRAIN = ()\n",
    "cfg.DATASETS.TEST = ()\n",
    "# cfg.INPUT.MIN_SIZE_TRAIN = (256,350,480,512)  # Minimum input image size during training\n",
    "cfg.INPUT.MIN_SIZE_TRAIN = (512,)\n",
    "cfg.INPUT.MAX_SIZE_TRAIN = 512     # Maximum input image size during training\n",
    "cfg.INPUT.MIN_SIZE_TEST = (512,)      # Minimum input image size during testing\n",
    "cfg.INPUT.MAX_SIZE_TEST = 512      # Maximum input image size during testing\n",
    "cfg.DATALOADER.NUM_WORKERS = 4\n",
    "# cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url('COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml')\n",
    "# cfg.MODEL.WEIGHTS = '/home/ec2-user/hubmap-hacking-the-human-vasculature/project_detectron2/output/inference/best_model_fold_0_with_added_aug_lr_0.00025.pth'\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.SOLVER.BASE_LR = 0.00025\n",
    "# cfg.SOLVER.BASE_LR = 0.002\n",
    "cfg.SOLVER.MAX_ITER = 6000\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(CLASSES)\n",
    "# cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "# compared to \"train_net.py\", we do not support accurate timing and\n",
    "# precise BN here, because they are not trivial to implement in a small training loop\n",
    "prob = 0.5\n",
    "data_transforms = [\n",
    "    T.RandomApply(T.RandomRotation([-90,90], expand=False), prob=prob),\n",
    "    T.RandomFlip(horizontal=True, vertical=False, prob=prob),\n",
    "    T.RandomFlip(horizontal=False, vertical=True, prob=prob),\n",
    "    T.RandomApply(T.RandomBrightness(0.8, 1.2), prob=prob),\n",
    "    T.RandomApply(T.RandomContrast(0.8, 1.2), prob=prob),\n",
    "    T.RandomApply(T.RandomSaturation(0.8,1.2), prob=prob),\n",
    "    T.RandomApply(T.RandomCrop('relative', (0.8, 0.8)), prob=prob)\n",
    "]\n",
    "\n",
    "# data_loader = build_detection_train_loader(cfg, mapper=DatasetMapper(cfg, is_train=True, augmentations=data_transforms))\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "def get_evaluator(cfg, dataset_name, output_folder=None):\n",
    "    \"\"\"\n",
    "    Create evaluator(s) for a given dataset.\n",
    "    This uses the special metadata \"evaluator_type\" associated with each builtin dataset.\n",
    "    For your own dataset, you can simply create an evaluator manually in your\n",
    "    script and do not have to worry about the hacky if-else logic here.\n",
    "    \"\"\"\n",
    "    if output_folder is None:\n",
    "        output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n",
    "    evaluator_list = []\n",
    "    evaluator_type = MetadataCatalog.get(dataset_name).evaluator_type\n",
    "    if evaluator_type in [\"sem_seg\", \"coco_panoptic_seg\"]:\n",
    "        evaluator_list.append(\n",
    "            SemSegEvaluator(\n",
    "                dataset_name,\n",
    "                distributed=True,\n",
    "                output_dir=output_folder,\n",
    "            )\n",
    "        )\n",
    "    if evaluator_type in [\"coco\", \"coco_panoptic_seg\"]:\n",
    "        evaluator_list.append(COCOEvaluator(dataset_name, output_dir=output_folder))\n",
    "    if evaluator_type == \"coco_panoptic_seg\":\n",
    "        evaluator_list.append(COCOPanopticEvaluator(dataset_name, output_folder))\n",
    "    if evaluator_type == \"cityscapes_instance\":\n",
    "        return CityscapesInstanceEvaluator(dataset_name)\n",
    "    if evaluator_type == \"cityscapes_sem_seg\":\n",
    "        return CityscapesSemSegEvaluator(dataset_name)\n",
    "    if evaluator_type == \"pascal_voc\":\n",
    "        return PascalVOCDetectionEvaluator(dataset_name)\n",
    "    if evaluator_type == \"lvis\":\n",
    "        return LVISEvaluator(dataset_name, cfg, True, output_folder)\n",
    "    if len(evaluator_list) == 0:\n",
    "        raise NotImplementedError(\n",
    "            \"no Evaluator for the dataset {} with the type {}\".format(dataset_name, evaluator_type)\n",
    "        )\n",
    "    if len(evaluator_list) == 1:\n",
    "        return evaluator_list[0]\n",
    "    return DatasetEvaluators(evaluator_list)\n",
    "\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "max_iter = cfg.SOLVER.MAX_ITER\n",
    "writers = default_writers(cfg.OUTPUT_DIR, max_iter) if comm.is_main_process() else []\n",
    "\n",
    "output_dir = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n",
    "\n",
    "for i in range(num_folds):\n",
    "    if os.path.exists(f'{output_dir}/model_stats_detectron_dataset1_fold_{i}.txt'):\n",
    "        os.remove(f'{output_dir}/model_stats_detectron_dataset1_fold_{i}.txt')\n",
    "\n",
    "for i in range(num_folds):\n",
    "    if os.path.exists(os.path.join(cfg.OUTPUT_DIR, \"inference\", f'{base_dataset_name}-train-fold-{i}')):\n",
    "        shutil.rmtree(os.path.join(cfg.OUTPUT_DIR, \"inference\", f'{base_dataset_name}-train-fold-{i}'))\n",
    "    if os.path.exists(os.path.join(cfg.OUTPUT_DIR, \"inference\", f'{base_dataset_name}-validation-fold-{i}')):\n",
    "        shutil.rmtree(os.path.join(cfg.OUTPUT_DIR, \"inference\", f'{base_dataset_name}-validation-fold-{i}'))\n",
    "\n",
    "for i in range(num_folds):\n",
    "    register_custom_dataset(f'{base_dataset_name}-train-fold-{i}', f'{base_dataset_path}/all_dataset1_imgs_merged_train_{i}', f'{base_dataset_path}/all_dataset1_annotations_merged_train_{i}')\n",
    "    register_custom_dataset(f'{base_dataset_name}-validation-fold-{i}', f'{base_dataset_path}/all_dataset1_imgs_merged_validation_{i}', f'{base_dataset_path}/all_dataset1_annotations_merged_validation_{i}')\n",
    "\n",
    "for i in range(num_folds):\n",
    "    train_dataset = DatasetCatalog.get(f'{base_dataset_name}-train-fold-{i}')\n",
    "    train_data_loader = build_detection_train_loader(cfg, mapper=DatasetMapper(cfg, is_train=True, augmentations=data_transforms), dataset=train_dataset)\n",
    "    validation_data_loader = build_detection_test_loader(cfg, f'{base_dataset_name}-validation-fold-{i}')\n",
    "    evaluator = get_evaluator(\n",
    "        cfg, f'{base_dataset_name}-validation-fold-{i}', os.path.join(cfg.OUTPUT_DIR, \"inference\", f'{base_dataset_name}-validation-fold-{i}')\n",
    "    )\n",
    "    cfg.DATASETS.TRAIN = (f'{base_dataset_name}-train-fold-{i}',)\n",
    "    cfg.DATASETS.TEST = (f'{base_dataset_name}-validation-fold-{i}',)\n",
    "    trainer = Trainer(cfg)\n",
    "    model = build_model(cfg)\n",
    "    trainer.train()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb586fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_folds):\n",
    "    model.train()\n",
    "    resume = False\n",
    "    optimizer = Trainer.build_optimizer(cfg, model)\n",
    "    scheduler = Trainer.build_lr_scheduler(cfg, optimizer)\n",
    "    checkpointer = DetectionCheckpointer(\n",
    "        model, cfg.OUTPUT_DIR, optimizer=optimizer, scheduler=scheduler\n",
    "    )\n",
    "    start_iter = (\n",
    "        checkpointer.resume_or_load(cfg.MODEL.WEIGHTS, resume=resume).get(\"iteration\", -1) + 1\n",
    "    )\n",
    "    periodic_checkpointer = PeriodicCheckpointer(\n",
    "        checkpointer, cfg.SOLVER.CHECKPOINT_PERIOD, max_iter=max_iter\n",
    "    )\n",
    "    iterations_per_epoch = len(train_dataset) // cfg.SOLVER.IMS_PER_BATCH\n",
    "    num_epochs = max_iter // iterations_per_epoch\n",
    "    num_iterations_to_show_stats = 50\n",
    "    max_ap = 0\n",
    "    loss_stats = {'total_loss': [], 'loss_cls': [], 'loss_box_reg': [], 'loss_mask': [], 'loss_rpn_cls': [], 'loss_rpn_loc': []}\n",
    "    with open(f'{output_dir}/model_stats_detectron_dataset1_fold_{i}.txt', 'a') as f:\n",
    "        f.write(f'Epoch info is - num_epochs: {num_epochs}, max_iter: {max_iter}, train_dataset_len: {len(train_dataset)}, iterations_per_epoch: {iterations_per_epoch}, num_iterations_to_show_stats: {num_iterations_to_show_stats}\\n')\n",
    "    # Training Loop\n",
    "    with EventStorage(start_iter) as storage:\n",
    "        start_time = time.time()\n",
    "        for data, iteration in zip(train_data_loader, range(start_iter, max_iter)):\n",
    "            storage.iter = iteration\n",
    "\n",
    "            loss_dict = model(data)\n",
    "            for loss_key in loss_dict:\n",
    "                if loss_key in loss_stats:\n",
    "                    loss_stats[loss_key].append(loss_dict[loss_key].item())\n",
    "            losses = sum(loss_dict.values())\n",
    "            loss_stats['total_loss'].append(losses.item())\n",
    "            assert torch.isfinite(losses).all(), loss_dict\n",
    "\n",
    "            loss_dict_reduced = {k: v.item() for k, v in comm.reduce_dict(loss_dict).items()}\n",
    "            losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
    "            if comm.is_main_process():\n",
    "                storage.put_scalars(total_loss=losses_reduced, **loss_dict_reduced)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "            storage.put_scalar(\"lr\", optimizer.param_groups[0][\"lr\"], smoothing_hint=False)\n",
    "            scheduler.step()\n",
    "\n",
    "            if iteration - start_iter > 5 and (\n",
    "                (iteration + 1) % 20 == 0 or iteration == max_iter - 1\n",
    "            ):\n",
    "                for writer in writers:\n",
    "                    writer.write()\n",
    "            periodic_checkpointer.step(iteration)\n",
    "        \n",
    "            if (iteration+1) % num_iterations_to_show_stats == 0:\n",
    "                metrics = inference_on_dataset(model, validation_data_loader, evaluator)\n",
    "                print('===========')\n",
    "                print(metrics)\n",
    "                print('===========')\n",
    "                metrics_str = ''\n",
    "                for task, task_metrics in metrics.items():\n",
    "                    task_str = f'{task}: '\n",
    "                    for metric, value in task_metrics.items():\n",
    "                        task_str += f'{metric}={value:.4f}, '\n",
    "                    metrics_str += task_str.rstrip(', ') + '\\n'\n",
    "                metrics_str = f'Iteration: {iteration}, time_taken: {float(time.time()-start_time)/60} minutes --> {metrics_str}'\n",
    "                loss_str = ''\n",
    "                for loss_key in loss_stats.keys():\n",
    "                    loss_str += f'{loss_key} - {np.mean(loss_stats[loss_key])}, '\n",
    "                    loss_stats[loss_key] = []\n",
    "                if 'segm' in metrics and metrics['segm']['AP'] > max_ap and metrics['segm']['AP']-max_ap >= 1:\n",
    "                    max_ap = metrics['segm']['AP']\n",
    "                    torch.save(model.state_dict(), f'{output_dir}/best_model_fold_{i}.pth')\n",
    "                    with open(f'{output_dir}/best_model_stats_detectron_dataset1_fold_{i}.txt', 'w') as f:\n",
    "                        f.write(f'{metrics_str}\\n{loss_str}\\n')\n",
    "                with open(f'{output_dir}/model_stats_detectron_dataset1_fold_{i}.txt', 'a') as f:\n",
    "                    f.write(f'{metrics_str}\\n{loss_str}\\n')\n",
    "                start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d559f7d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
