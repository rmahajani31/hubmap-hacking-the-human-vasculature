{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "456a3a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import json\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import shutil\n",
    "from shapely.geometry import Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d99b5bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '..'\n",
    "generate_all_datset_annots = True\n",
    "keep_all_in_train = False\n",
    "tile_categories = ['blood_vessel']\n",
    "base_data_dir = 'dataset1_files' if not generate_all_datset_annots else 'all_dataset_files'\n",
    "base_data_name = 'all_dataset1' if not generate_all_datset_annots else 'all_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc99008c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 2\n",
    "input_imgs_path = f'{base_path}/train'\n",
    "\n",
    "for i in range(num_folds):\n",
    "    output_dir = f'{base_path}/{base_data_dir}/{base_data_name}_mmdet_fold_{i}'\n",
    "    if os.path.exists(output_dir):\n",
    "        shutil.rmtree(output_dir)\n",
    "    os.mkdir(output_dir)\n",
    "    os.mkdir(f'{output_dir}/train_images')\n",
    "    os.mkdir(f'{output_dir}/validation_images')\n",
    "    os.mkdir(f'{output_dir}/annotations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ee1232b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count_type(tiles_dict):\n",
    "  ids_with_info = []\n",
    "  for tile in tiles_dict:\n",
    "    cur_dict = {'id': tile['id'], 'blood_vessel': 0, 'glomerulus': 0, 'unsure': 0}\n",
    "    for annot in tile['annotations']:\n",
    "      cur_dict[annot['type']] += 1\n",
    "    ids_with_info.append(cur_dict)\n",
    "  return ids_with_info\n",
    "def calculate_area(coords):\n",
    "    p = Polygon(coords)\n",
    "    return p.area\n",
    "def visualize(**images):\n",
    "    \"\"\"PLot images in one row.\"\"\"\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(30, 30))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(' '.join(name.split('_')).title())\n",
    "        plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e65eac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_category_annots(tile_categories):\n",
    "    categories = []\n",
    "    for idx, tc in enumerate(tile_categories):\n",
    "        categories.append({'id': idx, 'name': tc})\n",
    "    return categories\n",
    "def create_image_annots(input_imgs_names):\n",
    "    image_annots = []\n",
    "    img_name_to_id = dict()\n",
    "    input_img_names = sorted(input_imgs_names)\n",
    "    for idx, img_name in enumerate(input_img_names):\n",
    "        img = cv2.imread(f'{input_imgs_path}/{img_name}.tif')\n",
    "        height, width = img.shape[:2]\n",
    "        image_annots.append({'file_name': f'{img_name}.tif', 'height': height, 'width': width, 'id': idx})\n",
    "        img_name_to_id[img_name] = idx\n",
    "    return image_annots, img_name_to_id\n",
    "def create_seg_annots(tgt_tile_dicts, img_name_to_id):\n",
    "    annotations = []\n",
    "    annot_id = 0\n",
    "    for tile_dict in tgt_tile_dicts:\n",
    "        cur_img_name = tile_dict['id']\n",
    "        cur_annots = tile_dict['annotations']\n",
    "        for annot in cur_annots:\n",
    "            if annot['type'] not in tile_categories:\n",
    "                continue\n",
    "            coords = annot['coordinates'][0]\n",
    "            segmentation = [[pt for pair in coords for pt in pair]]\n",
    "            area = calculate_area(coords)\n",
    "            image_id = img_name_to_id[cur_img_name]\n",
    "            min_x = min(coords, key=lambda x: x[0])[0]\n",
    "            max_x = max(coords, key=lambda x: x[0])[0]\n",
    "            min_y = min(coords, key=lambda x: x[1])[1]\n",
    "            max_y = max(coords, key=lambda x: x[1])[1]\n",
    "            bbox = [min_x, min_y, max_x-min_x, max_y-min_y]\n",
    "            category_id = tile_categories.index(annot['type'])\n",
    "            annotations.append({'segmentation': segmentation, 'area': area, 'iscrowd': 0, 'image_id': image_id, 'bbox': bbox, 'category_id': category_id, 'id': annot_id})\n",
    "#             annotations.append({'iscrowd': 0, 'image_id': image_id, 'bbox': bbox, 'category_id': category_id, 'id': annot_id})\n",
    "            annot_id += 1\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fceb6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{base_path}/cleaned_polygons.jsonl', 'r') as json_file:\n",
    "    json_list = list(json_file)\n",
    "    \n",
    "tiles_dicts = []\n",
    "for json_str in json_list:\n",
    "    tiles_dicts.append(json.loads(json_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cdb86c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source_wsi</th>\n",
       "      <th>dataset</th>\n",
       "      <th>i</th>\n",
       "      <th>j</th>\n",
       "      <th>blood_vessel</th>\n",
       "      <th>glomerulus</th>\n",
       "      <th>unsure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0006ff2aa7cd</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>16896</td>\n",
       "      <td>16420</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00168d1b7522</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>14848</td>\n",
       "      <td>14884</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0033bbc76b6b</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10240</td>\n",
       "      <td>43008</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>003504460b3a</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>8192</td>\n",
       "      <td>11776</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>004daf1cbe75</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>6144</td>\n",
       "      <td>11264</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  source_wsi  dataset      i      j  blood_vessel  glomerulus  \\\n",
       "0  0006ff2aa7cd           2        2  16896  16420             8           1   \n",
       "1  00168d1b7522           2        2  14848  14884             1           1   \n",
       "2  0033bbc76b6b           1        1  10240  43008             3           0   \n",
       "3  003504460b3a           3        2   8192  11776             7           0   \n",
       "4  004daf1cbe75           3        2   6144  11264            10           1   \n",
       "\n",
       "   unsure  \n",
       "0       0  \n",
       "1       0  \n",
       "2       1  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tile_df = pd.read_csv(f'{base_path}/tile_meta.csv')\n",
    "# Find the same stats as the above cell for annotated images\n",
    "annotated_ids_with_info = get_count_type(tiles_dicts)\n",
    "annotated_ids_with_info_df = pd.DataFrame.from_dict(annotated_ids_with_info)\n",
    "tile_df_annotated = pd.merge(tile_df, annotated_ids_with_info_df, on='id', how='inner')\n",
    "tile_df_annotated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89074215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1633\n"
     ]
    }
   ],
   "source": [
    "dataset_1_tile_ids = sorted(tile_df_annotated.loc[tile_df_annotated['dataset']==1, 'id'].values)\n",
    "all_dataset_tile_ids = sorted(tile_df_annotated.loc[:, 'id'].values)\n",
    "tgt_tile_dicts = [x for x in tiles_dicts if x['id'] in dataset_1_tile_ids] if not generate_all_datset_annots else tiles_dicts\n",
    "print(len(tgt_tile_dicts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04926a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "tgt_dataset_tile_ids = dataset_1_tile_ids if not generate_all_datset_annots else all_dataset_tile_ids\n",
    "random.shuffle(tgt_dataset_tile_ids)\n",
    "random.shuffle(dataset_1_tile_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6510253b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold size is 211\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "# fold_size = int(math.ceil(len(tgt_dataset_tile_ids) / num_folds))\n",
    "fold_size = int(math.ceil(len(dataset_1_tile_ids) / num_folds))\n",
    "print(f'Fold size is {fold_size}')\n",
    "categories = create_category_annots(tile_categories)\n",
    "for i in range(num_folds):\n",
    "    output_dir = f'{base_path}/{base_data_dir}/{base_data_name}_mmdet_fold_{i}'\n",
    "    \n",
    "#     cur_validation_slice = tgt_dataset_tile_ids[int(i*fold_size):min(int((i+1)*fold_size), len(tgt_dataset_tile_ids))]\n",
    "    cur_validation_slice = dataset_1_tile_ids[int(i*fold_size):min(int((i+1)*fold_size), len(dataset_1_tile_ids))]\n",
    "    cur_training_slice = list(set(tgt_dataset_tile_ids) - set(cur_validation_slice)) if not keep_all_in_train else tgt_dataset_tile_ids\n",
    "    \n",
    "    for img_name in cur_validation_slice:\n",
    "        shutil.copy(f'{input_imgs_path}/{img_name}.tif', f'{output_dir}/validation_images/{img_name}.tif')\n",
    "    for img_name in cur_training_slice:\n",
    "        shutil.copy(f'{input_imgs_path}/{img_name}.tif', f'{output_dir}/train_images/{img_name}.tif')\n",
    "    \n",
    "    val_images, val_img_name_to_id = create_image_annots(cur_validation_slice)\n",
    "    train_images, train_img_name_to_id = create_image_annots(cur_training_slice)\n",
    "    \n",
    "    val_segmentation_annots = create_seg_annots([x for x in tgt_tile_dicts if x['id'] in cur_validation_slice], val_img_name_to_id)\n",
    "    train_segmentation_annots = create_seg_annots([x for x in tgt_tile_dicts if x['id'] in cur_training_slice], train_img_name_to_id)\n",
    "    \n",
    "    val_final_annots = {'categories': categories, 'images': val_images, 'annotations': val_segmentation_annots}\n",
    "    train_final_annots = {'categories': categories, 'images': train_images, 'annotations': train_segmentation_annots}\n",
    "    \n",
    "    with open(f'{output_dir}/annotations/validation_annotations.json', 'w') as f:\n",
    "        json.dump(val_final_annots, f)\n",
    "    with open(f'{output_dir}/annotations/train_annotations.json', 'w') as f:\n",
    "        json.dump(train_final_annots, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3107d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "with open(f'{base_path}/{base_data_dir}/{base_data_name}_mmdet_fold_{i}/annotations/train_annotations.json', 'r') as f:\n",
    "    train_annots = json.load(f)\n",
    "with open(f'{base_path}/{base_data_dir}/{base_data_name}_mmdet_fold_{i}/annotations/validation_annotations.json', 'r') as f:\n",
    "    validation_annots = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fc9bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_annots.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643d00af",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_to_segs = dict()\n",
    "\n",
    "for annot in train_annots['annotations']:\n",
    "    image_id = annot['image_id']\n",
    "    segmentation = annot['segmentation'][0]\n",
    "    if image_id not in img_to_segs:\n",
    "        img_to_segs[image_id] = [segmentation]\n",
    "    else:\n",
    "        img_to_segs[image_id].append(segmentation)\n",
    "for image_id in img_to_segs.keys():\n",
    "    segmentations = img_to_segs[image_id]\n",
    "    if len(segmentations) > 0:\n",
    "        for i in range(len(segmentations)):\n",
    "            for j in range(i+1, len(segmentations)):\n",
    "                if segmentations[i]==segmentations[j]:\n",
    "                    print(f'Found duplicate annots for image {image_id}!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759694f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc27936",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = set()\n",
    "train_img_ids = set()\n",
    "train_seg_img_ids = set()\n",
    "for annot in train_annots['images']:\n",
    "    train_img_ids.add(annot['id'])\n",
    "for annot in train_annots['annotations']:\n",
    "    train_seg_img_ids.add(annot['image_id'])\n",
    "    categories.add(annot['category_id'])\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ec7275",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = set()\n",
    "validation_img_ids = set()\n",
    "validation_seg_img_ids = set()\n",
    "for annot in validation_annots['images']:\n",
    "    validation_img_ids.add(annot['id'])\n",
    "for annot in validation_annots['annotations']:\n",
    "    validation_seg_img_ids.add(annot['image_id'])\n",
    "    categories.add(annot['category_id'])\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ece8fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_img_ids), len(validation_img_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfc8972",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "train_imgs_dir = f'{base_path}/{base_data_dir}/{base_data_name}_train_mmdet_fold_{i}/images'\n",
    "train_img_files = os.listdir(train_imgs_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db119255",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmyolo_env",
   "language": "python",
   "name": "mmyolo_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
